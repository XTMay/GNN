#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹ - ç»¼åˆæ¯”è¾ƒæ¡†æ¶
é›†æˆæ‰€æœ‰æ–¹æ³•çš„å…¨é¢æ¯”è¾ƒï¼šç®€å•MLPã€GNNæ¡†æ¶ã€é«˜çº§ç‰¹å¾å·¥ç¨‹
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os
import time
import warnings
from collections import defaultdict
warnings.filterwarnings('ignore')

# å¯¼å…¥æˆ‘ä»¬åˆ›å»ºçš„å„ä¸ªæ¨¡å—
try:
    from coupling_gnn_frameworks import compare_coupling_models as gnn_compare
    from advanced_coupling_features import main as advanced_main
    GNN_FRAMEWORKS_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ GNNæ¡†æ¶å¯¼å…¥å¤±è´¥: {e}")
    GNN_FRAMEWORKS_AVAILABLE = False

try:
    from advanced_coupling_features import AdvancedCouplingDataset, AdvancedCouplingMLP
    ADVANCED_FEATURES_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ é«˜çº§ç‰¹å¾æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    ADVANCED_FEATURES_AVAILABLE = False


class CouplingComparisonSuite:
    """æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹ç»¼åˆæ¯”è¾ƒå¥—ä»¶"""

    def __init__(self, data_path, max_samples=3000):
        self.data_path = data_path
        self.max_samples = max_samples
        self.results = {}

        print("ğŸš€ åˆå§‹åŒ–æ ‡é‡è€¦åˆå¸¸æ•°ç»¼åˆæ¯”è¾ƒæ¡†æ¶")
        print(f"æ•°æ®è·¯å¾„: {data_path}")
        print(f"æœ€å¤§æ ·æœ¬æ•°: {max_samples}")

    def run_simple_baseline(self):
        """è¿è¡Œç®€å•åŸºçº¿æ¨¡å‹"""
        print(f"\n{'='*60}")
        print("ğŸ“Š è¿è¡Œç®€å•åŸºçº¿MLPæ¨¡å‹")
        print(f"{'='*60}")

        try:
            # ç®€å•çš„MLPå®ç°
            from torch.utils.data import DataLoader, Dataset
            from sklearn.preprocessing import StandardScaler, LabelEncoder

            class SimpleDataset(Dataset):
                def __init__(self, data_path, max_samples):
                    # åŠ è½½æ•°æ®
                    train_df = pd.read_csv(os.path.join(data_path, 'train.csv')).head(max_samples)
                    structures_df = pd.read_csv(os.path.join(data_path, 'structures.csv'))

                    features, targets = self._process_data(train_df, structures_df)

                    self.scaler = StandardScaler()
                    self.features = torch.tensor(self.scaler.fit_transform(features), dtype=torch.float32)
                    self.targets = torch.tensor(targets, dtype=torch.float32)

                def _process_data(self, train_df, structures_df):
                    features = []
                    targets = []

                    for _, row in train_df.iterrows():
                        mol_name = row['molecule_name']
                        atom_0 = row['atom_index_0']
                        atom_1 = row['atom_index_1']
                        coupling_type = row['type']
                        target = row['scalar_coupling_constant']

                        mol_struct = structures_df[structures_df['molecule_name'] == mol_name]
                        if len(mol_struct) <= max(atom_0, atom_1):
                            continue

                        # ç®€å•ç‰¹å¾
                        coords_0 = mol_struct.iloc[atom_0][['x', 'y', 'z']].values
                        coords_1 = mol_struct.iloc[atom_1][['x', 'y', 'z']].values
                        distance = np.linalg.norm(coords_1 - coords_0)

                        atom_0_type = mol_struct.iloc[atom_0]['atom']
                        atom_1_type = mol_struct.iloc[atom_1]['atom']

                        # ç¼–ç ç‰¹å¾
                        atom_map = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}
                        type_map = {'1JHC': 0, '2JHH': 1, '3JHH': 2, '1JCC': 3,
                                   '2JHC': 4, '2JCH': 5, '3JHC': 6}

                        feature_vec = [
                            distance,
                            coords_1[0] - coords_0[0],
                            coords_1[1] - coords_0[1],
                            coords_1[2] - coords_0[2],
                            atom_map.get(atom_0_type, 0),
                            atom_map.get(atom_1_type, 0),
                            type_map.get(coupling_type, 0),
                            len(mol_struct)
                        ]

                        features.append(feature_vec)
                        targets.append(target)

                    return np.array(features), np.array(targets)

                def __len__(self):
                    return len(self.features)

                def __getitem__(self, idx):
                    return self.features[idx], self.targets[idx]

            # ç®€å•MLPæ¨¡å‹
            class SimpleMLP(nn.Module):
                def __init__(self, input_dim=8):
                    super().__init__()
                    self.mlp = nn.Sequential(
                        nn.Linear(input_dim, 64),
                        nn.ReLU(),
                        nn.Dropout(0.2),
                        nn.Linear(64, 32),
                        nn.ReLU(),
                        nn.Dropout(0.2),
                        nn.Linear(32, 1)
                    )

                def forward(self, x):
                    return self.mlp(x)

            # è®­ç»ƒç®€å•æ¨¡å‹
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            dataset = SimpleDataset(self.data_path, self.max_samples)

            # æ•°æ®åˆ’åˆ†
            train_size = int(0.7 * len(dataset))
            val_size = int(0.15 * len(dataset))
            test_size = len(dataset) - train_size - val_size

            train_data, val_data, test_data = torch.utils.data.random_split(
                dataset, [train_size, val_size, test_size]
            )

            train_loader = DataLoader(train_data, batch_size=64, shuffle=True)
            val_loader = DataLoader(val_data, batch_size=64, shuffle=False)
            test_loader = DataLoader(test_data, batch_size=64, shuffle=False)

            model = SimpleMLP().to(device)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            criterion = nn.MSELoss()

            # è®­ç»ƒ
            start_time = time.time()
            best_val_loss = float('inf')

            for epoch in range(30):
                model.train()
                for features, targets in train_loader:
                    features, targets = features.to(device), targets.to(device)
                    optimizer.zero_grad()
                    pred = model(features).squeeze()
                    loss = criterion(pred, targets)
                    loss.backward()
                    optimizer.step()

                # éªŒè¯
                model.eval()
                val_loss = 0
                with torch.no_grad():
                    for features, targets in val_loader:
                        features, targets = features.to(device), targets.to(device)
                        pred = model(features).squeeze()
                        val_loss += criterion(pred, targets).item()

                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model = model.state_dict().copy()

            # æµ‹è¯•
            model.load_state_dict(best_model)
            model.eval()
            predictions = []
            targets_list = []

            with torch.no_grad():
                for features, targets in test_loader:
                    features, targets = features.to(device), targets.to(device)
                    pred = model(features).squeeze()
                    predictions.extend(pred.cpu().numpy())
                    targets_list.extend(targets.cpu().numpy())

            predictions = np.array(predictions)
            targets_array = np.array(targets_list)

            training_time = time.time() - start_time

            # è®¡ç®—æŒ‡æ ‡
            mae = mean_absolute_error(targets_array, predictions)
            mse = mean_squared_error(targets_array, predictions)
            rmse = np.sqrt(mse)
            r2 = r2_score(targets_array, predictions)

            result = {
                'MAE': mae,
                'MSE': mse,
                'RMSE': rmse,
                'R2': r2,
                'num_parameters': sum(p.numel() for p in model.parameters()),
                'training_time': training_time,
                'description': 'ç®€å•åŸºçº¿MLP',
                'predictions': predictions,
                'targets': targets_array
            }

            print(f"âœ… ç®€å•åŸºçº¿å®Œæˆ:")
            print(f"   MAE: {mae:.4f}")
            print(f"   RÂ²: {r2:.4f}")
            print(f"   å‚æ•°é‡: {result['num_parameters']:,}")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time:.1f}s")

            return result

        except Exception as e:
            print(f"âŒ ç®€å•åŸºçº¿å¤±è´¥: {e}")
            return {'error': str(e)}

    def run_gnn_frameworks(self):
        """è¿è¡ŒGNNæ¡†æ¶æ¯”è¾ƒ"""
        print(f"\n{'='*60}")
        print("ğŸ”¥ è¿è¡ŒGNNæ¡†æ¶æ¯”è¾ƒ")
        print(f"{'='*60}")

        if not GNN_FRAMEWORKS_AVAILABLE:
            print("âŒ GNNæ¡†æ¶ä¸å¯ç”¨")
            return {'error': 'GNNæ¡†æ¶ä¸å¯ç”¨'}

        try:
            results = gnn_compare(
                data_path=self.data_path,
                max_samples=self.max_samples,
                test_split=0.2,
                val_split=0.1
            )
            print("âœ… GNNæ¡†æ¶æ¯”è¾ƒå®Œæˆ")
            return results
        except Exception as e:
            print(f"âŒ GNNæ¡†æ¶æ¯”è¾ƒå¤±è´¥: {e}")
            return {'error': str(e)}

    def run_advanced_features(self):
        """è¿è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹"""
        print(f"\n{'='*60}")
        print("ğŸš€ è¿è¡Œé«˜çº§ç‰¹å¾å·¥ç¨‹")
        print(f"{'='*60}")

        if not ADVANCED_FEATURES_AVAILABLE:
            print("âŒ é«˜çº§ç‰¹å¾æ¨¡å—ä¸å¯ç”¨")
            return {'error': 'é«˜çº§ç‰¹å¾æ¨¡å—ä¸å¯ç”¨'}

        try:
            # ä¿®æ”¹æ•°æ®è·¯å¾„å¹¶è¿è¡Œ
            original_path = '/Users/xiaotingzhou/Downloads/GNN/Dataset/scalar_coupling_constant'

            # ä¸´æ—¶ä¿®æ”¹æ¨¡å—ä¸­çš„æ•°æ®è·¯å¾„
            import sys
            import importlib

            # é‡æ–°åŠ è½½æ¨¡å—å¹¶è®¾ç½®è·¯å¾„
            if 'advanced_coupling_features' in sys.modules:
                importlib.reload(sys.modules['advanced_coupling_features'])

            from advanced_coupling_features import main as advanced_main

            # è¿è¡Œé«˜çº§ç‰¹å¾ç‰ˆæœ¬
            result = advanced_main()
            print("âœ… é«˜çº§ç‰¹å¾å·¥ç¨‹å®Œæˆ")
            return result
        except Exception as e:
            print(f"âŒ é«˜çº§ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
            return {'error': str(e)}

    def run_comprehensive_comparison(self):
        """è¿è¡Œç»¼åˆæ¯”è¾ƒ"""
        print(f"\n{'='*80}")
        print("ğŸ¯ å¼€å§‹ç»¼åˆæ¯”è¾ƒ")
        print(f"{'='*80}")

        start_time = time.time()

        # è¿è¡Œæ‰€æœ‰æ–¹æ³•
        simple_result = self.run_simple_baseline()
        gnn_results = self.run_gnn_frameworks()
        advanced_result = self.run_advanced_features()

        total_time = time.time() - start_time

        # æ•´ç†ç»“æœ
        self.results = {
            'Simple_Baseline': simple_result,
            'GNN_Frameworks': gnn_results,
            'Advanced_Features': advanced_result,
            'total_comparison_time': total_time
        }

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_comprehensive_report()

        return self.results

    def _generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæ¯”è¾ƒæŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("ğŸ“‹ æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹æ–¹æ³•ç»¼åˆæ¯”è¾ƒæŠ¥å‘Š")
        print(f"{'='*80}")

        # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆç»“æœ
        all_methods = {}

        # ç®€å•åŸºçº¿
        if 'error' not in self.results['Simple_Baseline']:
            all_methods['ç®€å•åŸºçº¿MLP'] = self.results['Simple_Baseline']

        # GNNæ¡†æ¶ç»“æœ
        if 'error' not in self.results['GNN_Frameworks']:
            gnn_results = self.results['GNN_Frameworks']
            for method_name, result in gnn_results.items():
                if 'error' not in result:
                    all_methods[f"GNN-{method_name}"] = result

        # é«˜çº§ç‰¹å¾ç»“æœ
        if 'error' not in self.results['Advanced_Features']:
            all_methods['é«˜çº§ç‰¹å¾MLP'] = self.results['Advanced_Features']

        if not all_methods:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„æ–¹æ³•å¯ä»¥æ¯”è¾ƒ")
            return

        # æ€§èƒ½æ’åº
        sorted_methods = sorted(all_methods.items(), key=lambda x: x[1].get('MAE', float('inf')))

        print(f"\nğŸ† æ–¹æ³•æ€§èƒ½æ’å (æŒ‰MAE):")
        print("-" * 100)
        print(f"{'æ’å':<4} {'æ–¹æ³•':<25} {'MAE':<10} {'RMSE':<10} {'RÂ²':<10} {'å‚æ•°é‡':<12} {'è®­ç»ƒæ—¶é—´':<10}")
        print("-" * 100)

        for i, (method_name, result) in enumerate(sorted_methods, 1):
            mae = result.get('MAE', 0)
            rmse = result.get('RMSE', 0)
            r2 = result.get('R2', 0)
            params = result.get('num_parameters', 0)
            time_s = result.get('training_time', 0)

            print(f"{i:<4} {method_name:<25} {mae:<10.4f} {rmse:<10.4f} "
                  f"{r2:<10.4f} {params:<12,} {time_s:<10.1f}s")

        # ç”Ÿæˆå¯è§†åŒ–
        self._create_comprehensive_plots(all_methods)

        # ä¿å­˜è¯¦ç»†ç»“æœ
        self._save_comprehensive_results(all_methods)

        print(f"\nâ±ï¸ æ€»æ¯”è¾ƒæ—¶é—´: {self.results['total_comparison_time']/60:.2f} åˆ†é’Ÿ")
        print("\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° comprehensive_coupling_results.txt")
        print("ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ä¸º comprehensive_coupling_comparison.png")

    def _create_comprehensive_plots(self, all_methods):
        """åˆ›å»ºç»¼åˆæ¯”è¾ƒå¯è§†åŒ–"""
        if len(all_methods) < 2:
            print("æ–¹æ³•å¤ªå°‘ï¼Œè·³è¿‡å¯è§†åŒ–")
            return

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹æ–¹æ³•ç»¼åˆæ¯”è¾ƒ', fontsize=16, fontweight='bold')

        methods = list(all_methods.keys())
        maes = [all_methods[m].get('MAE', 0) for m in methods]
        r2s = [all_methods[m].get('R2', 0) for m in methods]
        params = [all_methods[m].get('num_parameters', 0) for m in methods]
        times = [all_methods[m].get('training_time', 0) for m in methods]
        rmses = [all_methods[m].get('RMSE', 0) for m in methods]

        # 1. MAEå¯¹æ¯”
        axes[0, 0].bar(methods, maes, color='skyblue', alpha=0.7)
        axes[0, 0].set_title('å¹³å‡ç»å¯¹è¯¯å·® (MAE) æ¯”è¾ƒ')
        axes[0, 0].set_ylabel('MAE')
        axes[0, 0].tick_params(axis='x', rotation=45)

        # 2. RÂ²å¯¹æ¯”
        axes[0, 1].bar(methods, r2s, color='lightgreen', alpha=0.7)
        axes[0, 1].set_title('å†³å®šç³»æ•° (RÂ²) æ¯”è¾ƒ')
        axes[0, 1].set_ylabel('RÂ²')
        axes[0, 1].tick_params(axis='x', rotation=45)

        # 3. RMSEå¯¹æ¯”
        axes[0, 2].bar(methods, rmses, color='orange', alpha=0.7)
        axes[0, 2].set_title('å‡æ–¹æ ¹è¯¯å·® (RMSE) æ¯”è¾ƒ')
        axes[0, 2].set_ylabel('RMSE')
        axes[0, 2].tick_params(axis='x', rotation=45)

        # 4. å‚æ•°é‡å¯¹æ¯”
        axes[1, 0].bar(methods, params, color='salmon', alpha=0.7)
        axes[1, 0].set_title('æ¨¡å‹å‚æ•°é‡æ¯”è¾ƒ')
        axes[1, 0].set_ylabel('å‚æ•°æ•°é‡')
        axes[1, 0].tick_params(axis='x', rotation=45)

        # 5. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        axes[1, 1].bar(methods, times, color='plum', alpha=0.7)
        axes[1, 1].set_title('è®­ç»ƒæ—¶é—´æ¯”è¾ƒ')
        axes[1, 1].set_ylabel('æ—¶é—´ (ç§’)')
        axes[1, 1].tick_params(axis='x', rotation=45)

        # 6. æ€§èƒ½æ•ˆç‡æ•£ç‚¹å›¾
        scatter = axes[1, 2].scatter(params, maes, c=times, s=100, alpha=0.7, cmap='viridis')
        axes[1, 2].set_xlabel('å‚æ•°é‡')
        axes[1, 2].set_ylabel('MAE')
        axes[1, 2].set_title('æ€§èƒ½ vs å¤æ‚åº¦ (é¢œè‰²=è®­ç»ƒæ—¶é—´)')

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=axes[1, 2])
        cbar.set_label('è®­ç»ƒæ—¶é—´ (ç§’)')

        # æ·»åŠ æ–¹æ³•æ ‡ç­¾
        for i, method in enumerate(methods):
            axes[1, 2].annotate(method.replace('GNN-', '').replace('Coupling', ''),
                              (params[i], maes[i]), xytext=(5, 5),
                              textcoords='offset points', fontsize=8)

        plt.tight_layout()
        plt.savefig('comprehensive_coupling_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()

    def _save_comprehensive_results(self, all_methods):
        """ä¿å­˜ç»¼åˆæ¯”è¾ƒç»“æœ"""
        with open('comprehensive_coupling_results.txt', 'w', encoding='utf-8') as f:
            f.write("æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹æ–¹æ³•ç»¼åˆæ¯”è¾ƒç»“æœ\n")
            f.write("=" * 60 + "\n\n")

            f.write("å®éªŒè®¾ç½®:\n")
            f.write(f"- æ•°æ®è·¯å¾„: {self.data_path}\n")
            f.write(f"- æœ€å¤§æ ·æœ¬æ•°: {self.max_samples}\n")
            f.write(f"- æ€»æ¯”è¾ƒæ—¶é—´: {self.results['total_comparison_time']/60:.2f} åˆ†é’Ÿ\n\n")

            # æŒ‰æ€§èƒ½æ’åº
            sorted_methods = sorted(all_methods.items(), key=lambda x: x[1].get('MAE', float('inf')))

            f.write("æ–¹æ³•æ€§èƒ½æ’å (æŒ‰MAE):\n")
            f.write("-" * 60 + "\n")

            for i, (method_name, result) in enumerate(sorted_methods, 1):
                f.write(f"{i}. {method_name}\n")
                f.write(f"   MAE: {result.get('MAE', 0):.6f}\n")
                f.write(f"   RMSE: {result.get('RMSE', 0):.6f}\n")
                f.write(f"   RÂ²: {result.get('R2', 0):.6f}\n")
                f.write(f"   å‚æ•°é‡: {result.get('num_parameters', 0):,}\n")
                f.write(f"   è®­ç»ƒæ—¶é—´: {result.get('training_time', 0):.1f}s\n")
                if 'description' in result:
                    f.write(f"   æè¿°: {result['description']}\n")
                f.write("\n")

            f.write("ä¸»è¦ç»“è®º:\n")
            f.write("-" * 30 + "\n")
            if sorted_methods:
                best_method, best_result = sorted_methods[0]
                f.write(f"â€¢ æœ€ä½³æ€§èƒ½æ–¹æ³•: {best_method}\n")
                f.write(f"â€¢ æœ€ä½³MAE: {best_result.get('MAE', 0):.6f}\n")

                # æ•ˆç‡åˆ†æ
                efficient_methods = [m for m in sorted_methods if m[1].get('training_time', 0) < 60]
                if efficient_methods:
                    f.write(f"â€¢ æœ€é«˜æ•ˆæ–¹æ³•: {efficient_methods[0][0]}\n")

                # å¹³è¡¡æ–¹æ³•
                balanced_methods = sorted(sorted_methods,
                                        key=lambda x: x[1].get('MAE', 1) * x[1].get('num_parameters', 1))
                if balanced_methods:
                    f.write(f"â€¢ å¹³è¡¡æ€§èƒ½/å¤æ‚åº¦æ–¹æ³•: {balanced_methods[0][0]}\n")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹ç»¼åˆæ¯”è¾ƒæ¡†æ¶")

    # æ•°æ®è·¯å¾„
    data_path = '/Users/xiaotingzhou/Downloads/GNN/Dataset/scalar_coupling_constant'

    # æ£€æŸ¥æ•°æ®é›†
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        return None

    # åˆ›å»ºæ¯”è¾ƒå¥—ä»¶
    comparison_suite = CouplingComparisonSuite(data_path, max_samples=3000)

    # è¿è¡Œç»¼åˆæ¯”è¾ƒ
    results = comparison_suite.run_comprehensive_comparison()

    print("\nğŸ‰ ç»¼åˆæ¯”è¾ƒå®Œæˆ!")

    return results


if __name__ == "__main__":
    results = main()
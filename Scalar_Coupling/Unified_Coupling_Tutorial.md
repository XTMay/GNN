# æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹ç»Ÿä¸€æ¡†æ¶æ•™å­¦æ–‡æ¡£

## ğŸ“š æ•™ç¨‹æ¦‚è¿°

æœ¬æ•™ç¨‹è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å’Œæœºå™¨å­¦ä¹ æ–¹æ³•é¢„æµ‹åˆ†å­ä¸­åŸå­å¯¹ä¹‹é—´çš„æ ‡é‡è€¦åˆå¸¸æ•°ã€‚é€šè¿‡ç»Ÿä¸€æ¡†æ¶æ¯”è¾ƒ5ç§ä¸åŒçš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä»ç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºåˆ°å¤æ‚çš„å›¾ç¥ç»ç½‘ç»œæ¶æ„ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£æ ‡é‡è€¦åˆå¸¸æ•°çš„åŒ–å­¦æ„ä¹‰
- æŒæ¡åˆ†å­æ•°æ®çš„é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
- ç†è§£ä¸åŒGNNæ¶æ„çš„åŸç†å’Œåº”ç”¨
- å­¦ä¼šå¦‚ä½•æ¯”è¾ƒå’Œè¯„ä¼°ä¸åŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹
- æŒæ¡åˆ†å­å›¾ç¥ç»ç½‘ç»œçš„å®é™…åº”ç”¨

## ğŸ“– ç›®å½•

1. [æ•°æ®é›†ä»‹ç»](#1-æ•°æ®é›†ä»‹ç»)
2. [åŒ–å­¦èƒŒæ™¯çŸ¥è¯†](#2-åŒ–å­¦èƒŒæ™¯çŸ¥è¯†)
3. [æ•´ä½“å·¥ä½œæµç¨‹](#3-æ•´ä½“å·¥ä½œæµç¨‹)
4. [æ•°æ®é¢„å¤„ç†è¯¦è§£](#4-æ•°æ®é¢„å¤„ç†è¯¦è§£)
5. [æ¨¡å‹æ¶æ„è¯¦è§£](#5-æ¨¡å‹æ¶æ„è¯¦è§£)
6. [è®­ç»ƒå’Œè¯„ä¼°æµç¨‹](#6-è®­ç»ƒå’Œè¯„ä¼°æµç¨‹)
7. [ç»“æœåˆ†æå’Œè§£é‡Š](#7-ç»“æœåˆ†æå’Œè§£é‡Š)
8. [å®è·µæŒ‡å—](#8-å®è·µæŒ‡å—)
9. [å¸¸è§é—®é¢˜å’Œä¼˜åŒ–](#9-å¸¸è§é—®é¢˜å’Œä¼˜åŒ–)

---

## 1. æ•°æ®é›†ä»‹ç»

### 1.1 æ•°æ®é›†æ¥æº
**æ ‡é‡è€¦åˆå¸¸æ•°æ•°æ®é›†**æ¥è‡ªKaggleç«èµ›"Predicting Molecular Properties"ï¼ŒåŒ…å«çº¦450ä¸‡ä¸ªåˆ†å­ä¸­åŸå­å¯¹ä¹‹é—´çš„æ ‡é‡è€¦åˆå¸¸æ•°æ•°æ®ã€‚

### 1.2 æ•°æ®æ–‡ä»¶ç»“æ„
```
Dataset/scalar_coupling_constant/
â”œâ”€â”€ train.csv                      # è®­ç»ƒæ•°æ® (~4.7M è®°å½•)
â”œâ”€â”€ test.csv                       # æµ‹è¯•æ•°æ® (~45K è®°å½•)
â”œâ”€â”€ structures.csv                 # åˆ†å­3Dç»“æ„ (~85K åˆ†å­)
â””â”€â”€ scalar_coupling_contributions.csv # è€¦åˆè´¡çŒ®åˆ†è§£ (å¯é€‰)
```

### 1.3 æ ¸å¿ƒæ•°æ®æ–‡ä»¶è¯¦è§£

#### A. `train.csv` - è®­ç»ƒæ•°æ®
```csv
id,molecule_name,atom_index_0,atom_index_1,type,scalar_coupling_constant
0,dsgdb9nsd_000001,1,0,1JHC,84.8076
1,dsgdb9nsd_000001,1,2,2JHH,25.7570
2,dsgdb9nsd_000001,1,3,2JHH,-11.2648
```

**å­—æ®µè¯´æ˜:**
- `id`: è®°å½•å”¯ä¸€æ ‡è¯†ç¬¦
- `molecule_name`: åˆ†å­åç§°æ ‡è¯†ç¬¦
- `atom_index_0/1`: åŸå­å¯¹çš„ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
- `type`: è€¦åˆç±»å‹ï¼ˆå¦‚1JHCè¡¨ç¤ºç¢³æ°¢é—´çš„å•é”®è€¦åˆï¼‰
- `scalar_coupling_constant`: ç›®æ ‡å€¼ï¼Œè€¦åˆå¸¸æ•°ï¼ˆå•ä½: Hzï¼‰

#### B. `structures.csv` - åˆ†å­ç»“æ„
```csv
molecule_name,atom_index,atom,x,y,z
dsgdb9nsd_000001,0,C,0.002150,0.002150,0.002150
dsgdb9nsd_000001,1,H,0.629118,0.629118,0.629118
```

**å­—æ®µè¯´æ˜:**
- `molecule_name`: åˆ†å­æ ‡è¯†ç¬¦
- `atom_index`: åŸå­åœ¨åˆ†å­ä¸­çš„ç´¢å¼•
- `atom`: åŸå­ç±»å‹ï¼ˆH, C, N, O, Fï¼‰
- `x,y,z`: åŸå­çš„3Dåæ ‡ï¼ˆå•ä½: Ã…ngstrÃ¶mï¼‰

### 1.4 æ•°æ®é›†è§„æ¨¡ç»Ÿè®¡
- **åˆ†å­æ•°é‡**: ~85,000ä¸ªæœ‰æœºå°åˆ†å­
- **åŸå­ç±»å‹**: 5ç§ï¼ˆH, C, N, O, Fï¼‰
- **è€¦åˆç±»å‹**: 8ç§ï¼ˆ1JHC, 1JCC, 2JHH, 2JHC, 2JCH, 3JHH, 3JHC, 3JCCï¼‰
- **è€¦åˆå¸¸æ•°èŒƒå›´**: -36 Hz åˆ° +204 Hz
- **å¹³å‡åˆ†å­å¤§å°**: 9-29ä¸ªåŸå­

---

## 2. åŒ–å­¦èƒŒæ™¯çŸ¥è¯†

### 2.1 ä»€ä¹ˆæ˜¯æ ‡é‡è€¦åˆå¸¸æ•°ï¼Ÿ

æ ‡é‡è€¦åˆå¸¸æ•°ï¼ˆScalar Coupling Constantï¼‰æ˜¯æ ¸ç£å…±æŒ¯ï¼ˆNMRï¼‰å…‰è°±å­¦ä¸­çš„é‡è¦å‚æ•°ï¼Œæè¿°äº†åˆ†å­ä¸­ä¸¤ä¸ªåŸå­æ ¸ä¹‹é—´çš„ç£æ€§ç›¸äº’ä½œç”¨å¼ºåº¦ã€‚

#### ç‰©ç†æ„ä¹‰ï¼š
- **ç£æ€§è€¦åˆ**: ä¸¤ä¸ªåŸå­æ ¸çš„è‡ªæ—‹é€šè¿‡åŒ–å­¦é”®ä¼ é€’ç›¸äº’å½±å“
- **å…‰è°±è¡¨ç°**: åœ¨NMRè°±ä¸­è¡¨ç°ä¸ºå³°çš„åˆ†è£‚
- **ç»“æ„ä¿¡æ¯**: æä¾›åˆ†å­ä¸‰ç»´ç»“æ„å’ŒåŒ–å­¦ç¯å¢ƒçš„ä¿¡æ¯

### 2.2 è€¦åˆç±»å‹åˆ†ç±»

#### æŒ‰é”®è·ç¦»åˆ†ç±»ï¼š
1. **1Jè€¦åˆ** - ç›´æ¥é”®è¿æ¥ï¼ˆ1ä¸ªåŒ–å­¦é”®ï¼‰
   - `1JHC`: ç¢³-æ°¢å•é”®è€¦åˆï¼Œé€šå¸¸æœ€å¼ºï¼ˆ~125-250 Hzï¼‰
   - `1JCC`: ç¢³-ç¢³å•é”®è€¦åˆï¼ˆ~35-40 Hzï¼‰

2. **2Jè€¦åˆ** - äºŒé”®è€¦åˆï¼ˆ2ä¸ªåŒ–å­¦é”®ï¼‰
   - `2JHH`: æ°¢-æ°¢äºŒé”®è€¦åˆï¼ˆ~10-15 Hzï¼‰
   - `2JHC/2JCH`: ç¢³-æ°¢äºŒé”®è€¦åˆï¼ˆ~2-6 Hzï¼‰

3. **3Jè€¦åˆ** - ä¸‰é”®è€¦åˆï¼ˆ3ä¸ªåŒ–å­¦é”®ï¼‰
   - `3JHH`: æ°¢-æ°¢ä¸‰é”®è€¦åˆï¼ˆ~6-8 Hzï¼‰
   - `3JHC`: ç¢³-æ°¢ä¸‰é”®è€¦åˆï¼ˆ~4-8 Hzï¼‰

#### è€¦åˆå¼ºåº¦è§„å¾‹ï¼š
- **1J > 2J > 3J**: è·ç¦»è¶Šè¿‘ï¼Œè€¦åˆè¶Šå¼º
- **åŒç±»åŸå­**: C-Hè€¦åˆé€šå¸¸æ¯”H-Hè€¦åˆå¼º
- **åŒ–å­¦ç¯å¢ƒ**: ç”µè´Ÿæ€§ã€æ‚åŒ–çŠ¶æ€å½±å“è€¦åˆå¼ºåº¦

### 2.3 é¢„æµ‹çš„æŒ‘æˆ˜

#### å½±å“å› ç´ ï¼š
1. **å‡ ä½•ç»“æ„**: åŸå­é—´è·ç¦»å’Œè§’åº¦
2. **åŒ–å­¦ç¯å¢ƒ**: å‘¨å›´åŸå­çš„ç”µå­æ•ˆåº”
3. **åˆ†å­æ„è±¡**: æŸ”æ€§åˆ†å­çš„ç©ºé—´æ’åˆ—
4. **é‡å­æ•ˆåº”**: ç”µå­äº‘é‡å å’Œè½¨é“ç›¸äº’ä½œç”¨

#### ä¼ ç»Ÿæ–¹æ³•é™åˆ¶ï¼š
- **é‡å­åŒ–å­¦è®¡ç®—**: ç²¾ç¡®ä½†è®¡ç®—æˆæœ¬æé«˜
- **ç»éªŒå…¬å¼**: å¿«é€Ÿä½†æ³›åŒ–èƒ½åŠ›æœ‰é™
- **æœºå™¨å­¦ä¹ **: å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡çš„æ–°é€”å¾„

---

## 3. æ•´ä½“å·¥ä½œæµç¨‹

### 3.1 æ¡†æ¶æ¶æ„å›¾

```
è¾“å…¥æ•°æ® â†’ æ•°æ®é¢„å¤„ç† â†’ ç‰¹å¾æå– â†’ æ¨¡å‹è®­ç»ƒ â†’ æ€§èƒ½è¯„ä¼° â†’ ç»“æœæ¯”è¾ƒ
   â†“           â†“          â†“         â†“          â†“          â†“
train.csv   æ•°æ®æ¸…æ´—     åŸå­ç‰¹å¾   5ç§æ¨¡å‹    MAE/RMSE    æ’è¡Œæ¦œ
structures  ç‰¹å¾å·¥ç¨‹     åˆ†å­å›¾     å¹¶è¡Œè®­ç»ƒ   RÂ²/æ—¶é—´     å¯è§†åŒ–
   â†“           â†“          â†“         â†“          â†“          â†“
85Kåˆ†å­     æ ‡å‡†åŒ–      å›¾æ„å»º     ç»Ÿä¸€è¯„ä¼°   ç»“æœä¿å­˜    æŠ¥å‘Šç”Ÿæˆ
```

### 3.2 æŠ€æœ¯æ ˆ

#### æ ¸å¿ƒåº“ï¼š
```python
torch                    # æ·±åº¦å­¦ä¹ æ¡†æ¶
torch_geometric         # å›¾ç¥ç»ç½‘ç»œåº“
pandas, numpy           # æ•°æ®å¤„ç†
sklearn                 # æœºå™¨å­¦ä¹ å·¥å…·
matplotlib, seaborn     # å¯è§†åŒ–
rdkit (å¯é€‰)            # åŒ–å­¦ä¿¡æ¯å­¦
```

#### ç¡¬ä»¶è¦æ±‚ï¼š
- **å†…å­˜**: æœ€å°‘4GBï¼Œæ¨è8GB+
- **GPU**: å¯é€‰ï¼Œæ˜¾è‘—åŠ é€Ÿè®­ç»ƒ
- **å­˜å‚¨**: éœ€è¦çº¦2GBç©ºé—´å­˜å‚¨æ•°æ®å’Œç»“æœ

### 3.3 æ‰§è¡Œæµç¨‹

```bash
# 1. ç¯å¢ƒå‡†å¤‡
pip install torch torch-geometric pandas numpy matplotlib seaborn scikit-learn

# 2. æ•°æ®å‡†å¤‡
# ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®: /Users/xiaotingzhou/Downloads/GNN/Dataset/scalar_coupling_constant/

# 3. è¿è¡Œæ¡†æ¶
cd /Users/xiaotingzhou/Downloads/GNN
python unified_coupling_frameworks.py

# 4. è¾“å…¥å‚æ•°
è¯·è¾“å…¥æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤3000): 3000

# 5. ç­‰å¾…å®Œæˆï¼ˆ45-90åˆ†é’Ÿï¼‰
# 6. æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
```

---

## 4. æ•°æ®é¢„å¤„ç†è¯¦è§£

### 4.1 æ•°æ®åŠ è½½ç­–ç•¥

#### A. ç®€åŒ–æ•°æ®é›†ï¼ˆSimpleCouplingDatasetï¼‰
ç”¨äº**Simple MLP**æ¨¡å‹ï¼š

```python
class SimpleCouplingDataset(Dataset):
    def __init__(self, data_path, max_samples=3000):
        # åŠ è½½è®­ç»ƒæ•°æ®å’Œç»“æ„æ•°æ®
        self.train_df = pd.read_csv(os.path.join(data_path, 'train.csv')).head(max_samples)
        self.structures_df = pd.read_csv(os.path.join(data_path, 'structures.csv'))

        # é¢„å¤„ç†ä¸ºç®€å•ç‰¹å¾å‘é‡
        self._preprocess_data()
```

**ç‰¹å¾æå–é€»è¾‘ï¼š**
```python
# å¯¹æ¯ä¸ªåŸå­å¯¹æå–8ç»´ç‰¹å¾:
feature_vec = [
    distance,                    # åŸå­é—´æ¬§æ°è·ç¦»
    coords_1[0] - coords_0[0],  # xæ–¹å‘ç›¸å¯¹ä½ç§»
    coords_1[1] - coords_0[1],  # yæ–¹å‘ç›¸å¯¹ä½ç§»
    coords_1[2] - coords_0[2],  # zæ–¹å‘ç›¸å¯¹ä½ç§»
    atom_map.get(atom_0_type, 0),  # ç¬¬ä¸€ä¸ªåŸå­ç±»å‹ç¼–ç 
    atom_map.get(atom_1_type, 0),  # ç¬¬äºŒä¸ªåŸå­ç±»å‹ç¼–ç 
    type_map.get(coupling_type, 0), # è€¦åˆç±»å‹ç¼–ç 
    len(mol_struct)              # åˆ†å­å¤§å°
]
```

#### B. å›¾æ•°æ®é›†ï¼ˆGraphCouplingDatasetï¼‰
ç”¨äº**GNNæ¨¡å‹**ï¼ˆGCN, GAT, Transformerï¼‰ï¼š

```python
class GraphCouplingDataset(Dataset):
    def __init__(self, data_path, max_samples=3000, advanced_features=False):
        # åŸå­ç‰¹å¾æ˜ å°„è¡¨
        self.atom_features = {
            'H': [1, 1.008, 1, 1, 2.20, 0.31],  # [åŸå­åºæ•°, è´¨é‡, ä»·ç”µå­, å‘¨æœŸ, ç”µè´Ÿæ€§, åŠå¾„]
            'C': [6, 12.01, 4, 2, 2.55, 0.76],
            'N': [7, 14.01, 5, 2, 3.04, 0.71],
            'O': [8, 16.00, 6, 2, 3.44, 0.66],
            'F': [9, 19.00, 7, 2, 3.98, 0.57],
        }
```

### 4.2 åˆ†å­å›¾æ„å»º

#### A. èŠ‚ç‚¹ç‰¹å¾ï¼ˆåŸå­ç‰¹å¾ï¼‰
æ¯ä¸ªåŸå­èŠ‚ç‚¹åŒ…å«6ç»´ç‰¹å¾ï¼š
1. **åŸå­åºæ•°**: å…ƒç´ çš„åŸºæœ¬æ ‡è¯†
2. **åŸå­è´¨é‡**: å½±å“æŒ¯åŠ¨é¢‘ç‡
3. **ä»·ç”µå­æ•°**: å†³å®šåŒ–å­¦é”®åˆèƒ½åŠ›
4. **å‘¨æœŸæ•°**: åæ˜ åŸå­å¤§å°
5. **ç”µè´Ÿæ€§**: å½±å“ç”µå­äº‘åˆ†å¸ƒ
6. **åŸå­åŠå¾„**: å½±å“åŸå­é—´ç›¸äº’ä½œç”¨

#### B. è¾¹è¿æ¥ç­–ç•¥
```python
def _create_edges(self, atom_coords, cutoff=2.0):
    """åŸºäºè·ç¦»é˜ˆå€¼åˆ›å»ºè¾¹è¿æ¥"""
    for i in range(num_atoms):
        for j in range(i + 1, num_atoms):
            dist = torch.norm(atom_coords[i] - atom_coords[j]).item()
            if dist < cutoff:  # 2.0 Ã…ngstrÃ¶m é˜ˆå€¼
                edge_list.append([i, j])
                edge_list.append([j, i])  # æ— å‘å›¾
```

**è®¾è®¡è€ƒè™‘ï¼š**
- **è·ç¦»é˜ˆå€¼**: 2.0Ã…è¦†ç›–å¤§éƒ¨åˆ†å…±ä»·é”®
- **æ— å‘å›¾**: åŒ–å­¦é”®æ˜¯åŒå‘çš„
- **è¾¹å±æ€§**: å­˜å‚¨åŸå­é—´è·ç¦»ä¿¡æ¯

#### C. åŸå­å¯¹ç‰¹å¾
å¯¹äºæ¯ä¸ªéœ€è¦é¢„æµ‹çš„åŸå­å¯¹ï¼Œæå–8ç»´ç‰¹å¾ï¼š
```python
features = [
    distance,                          # åŸå­é—´è·ç¦»
    rel_pos[0], rel_pos[1], rel_pos[2], # 3Dç›¸å¯¹ä½ç½®å‘é‡
    atom_0_encoded, atom_1_encoded,     # åŸå­ç±»å‹ç¼–ç 
    coupling_type_encoded,              # è€¦åˆç±»å‹ç¼–ç 
    len(mol_structure)                  # åˆ†å­å¤§å°
]
```

### 4.3 é«˜çº§ç‰¹å¾å·¥ç¨‹

#### A. æ‹“æ‰‘ç‰¹å¾
æè¿°åˆ†å­çš„å›¾ç»“æ„æ€§è´¨ï¼š
```python
features.extend([
    num_atoms,                    # åˆ†å­å¤§å°
    atom_counts['H'],            # æ°¢åŸå­æ•°
    atom_counts['C'],            # ç¢³åŸå­æ•°
    atom_counts['N'],            # æ°®åŸå­æ•°
    atom_counts['O'],            # æ°§åŸå­æ•°
    atom_counts['F']             # æ°ŸåŸå­æ•°
])
```

#### B. å‡ ä½•ç‰¹å¾
æè¿°åˆ†å­çš„3Dç©ºé—´æ€§è´¨ï¼š
```python
# åˆ†å­è¾¹ç•Œç›’
bbox = coords.max(axis=0) - coords.min(axis=0)
features.extend(bbox)  # [x_span, y_span, z_span]

# å‡ ä½•ä¸­å¿ƒ
center = coords.mean(axis=0)
features.extend(center)  # [center_x, center_y, center_z]

# å›è½¬åŠå¾„ï¼ˆåˆ†å­ç´§å¯†ç¨‹åº¦ï¼‰
gyration_radius = np.sqrt(np.sum(centered_coords ** 2) / len(coords))
features.append(gyration_radius)
```

#### C. åŒ–å­¦ç‰¹å¾
```python
# åˆ†å­é‡ä¼°ç®—
atom_masses = {'H': 1.008, 'C': 12.01, 'N': 14.01, 'O': 16.00, 'F': 19.00}
mol_weight = sum(atom_masses.get(atom, 0) for atom in mol_structure['atom'])
features.append(mol_weight)
```

### 4.4 æ•°æ®æ ‡å‡†åŒ–

#### A. ç‰¹å¾ç¼©æ”¾
```python
# å¯¹äºç®€å•MLP
self.scaler = StandardScaler()
self.features = self.scaler.fit_transform(features)

# å¯¹äºé«˜çº§ç‰¹å¾
scaler = RobustScaler()  # å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
features = scaler.fit_transform(all_features)
```

#### B. ç‰¹å¾é€‰æ‹©
```python
# é€‰æ‹©æœ€é‡è¦çš„50ä¸ªç‰¹å¾
selector = SelectKBest(f_regression, k=min(50, features.shape[1]))
features = selector.fit_transform(features, all_targets)
```

**é€‰æ‹©æ ‡å‡†ï¼š**
- **Fç»Ÿè®¡é‡**: è¡¡é‡ç‰¹å¾ä¸ç›®æ ‡çš„çº¿æ€§å…³ç³»
- **ç»´åº¦é™åˆ¶**: é¿å…ç»´åº¦è¯…å’’
- **è®¡ç®—æ•ˆç‡**: å‡å°‘è®­ç»ƒæ—¶é—´

---

## 5. æ¨¡å‹æ¶æ„è¯¦è§£

### 5.1 Simple MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰

#### A. æ¶æ„è®¾è®¡
```python
class SimpleMLP(nn.Module):
    def __init__(self, input_dim=8, hidden_dim=64):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),    # 8 â†’ 64
            nn.ReLU(),                           # éçº¿æ€§æ¿€æ´»
            nn.Dropout(0.2),                     # é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(hidden_dim, hidden_dim//2), # 64 â†’ 32
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim//2, 1)          # 32 â†’ 1 (è¾“å‡º)
        )
```

#### B. è®¾è®¡ç†å¿µ
- **ç®€å•åŸºçº¿**: ä½œä¸ºå…¶ä»–æ¨¡å‹çš„å¯¹æ¯”åŸºå‡†
- **ç›´æ¥æ˜ å°„**: åŸå­å¯¹ç‰¹å¾ç›´æ¥æ˜ å°„åˆ°è€¦åˆå¸¸æ•°
- **å¿«é€Ÿè®­ç»ƒ**: å‚æ•°å°‘ï¼Œæ”¶æ•›å¿«
- **è§£é‡Šæ€§å¼º**: å®¹æ˜“ç†è§£å’Œè°ƒè¯•

#### C. é€‚ç”¨åœºæ™¯
- **å¿«é€ŸåŸå‹**: éªŒè¯æ•°æ®è´¨é‡å’ŒåŸºæœ¬å¯è¡Œæ€§
- **åŸºçº¿æ¯”è¾ƒ**: è¡¡é‡å¤æ‚æ¨¡å‹çš„æ”¹è¿›ç¨‹åº¦
- **èµ„æºå—é™**: è®¡ç®—èµ„æºä¸è¶³æ—¶çš„é€‰æ‹©

#### D. ä¼˜ç¼ºç‚¹åˆ†æ
**ä¼˜ç‚¹:**
- è®­ç»ƒå¿«é€Ÿï¼ˆ45ç§’ï¼‰
- å‚æ•°æœ€å°‘ï¼ˆ~3Kï¼‰
- å†…å­˜å ç”¨å°
- æ˜“äºè°ƒè¯•

**ç¼ºç‚¹:**
- å¿½ç•¥åˆ†å­ç»“æ„ä¿¡æ¯
- æ— æ³•æ•è·åŸå­é—´å¤æ‚å…³ç³»
- æ³›åŒ–èƒ½åŠ›æœ‰é™
- é¢„æµ‹ç²¾åº¦æœ€ä½

### 5.2 CouplingGCNï¼ˆå›¾å·ç§¯ç½‘ç»œï¼‰

#### A. æ¶æ„è®¾è®¡
```python
class CouplingGCN(nn.Module):
    def __init__(self, num_atom_features, num_pair_features, hidden_dim=128, num_layers=3):
        super().__init__()

        # åŸå­ç‰¹å¾åµŒå…¥
        self.atom_embedding = nn.Linear(num_atom_features, hidden_dim)

        # GCNå·ç§¯å±‚å †å 
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        for i in range(num_layers):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))
            self.batch_norms.append(BatchNorm(hidden_dim))

        # åŸå­å¯¹é¢„æµ‹å¤´
        self.pair_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2 + num_pair_features, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim//2, 1)
        )
```

#### B. å‰å‘ä¼ æ’­æµç¨‹
```python
def forward(self, atom_features, edge_index, pair_indices, pair_features):
    # 1. åŸå­ç‰¹å¾åµŒå…¥
    x = self.atom_embedding(atom_features)  # [num_atoms, hidden_dim]

    # 2. å¤šå±‚GCNå·ç§¯
    for i in range(self.num_layers):
        x = self.convs[i](x, edge_index)     # å›¾å·ç§¯
        x = self.batch_norms[i](x)           # æ‰¹æ ‡å‡†åŒ–
        x = F.relu(x)                        # æ¿€æ´»å‡½æ•°
        x = F.dropout(x, training=self.training)  # Dropout

    # 3. æå–åŸå­å¯¹è¡¨ç¤º
    atom_pair_0 = x[pair_indices[:, 0]]      # ç¬¬ä¸€ä¸ªåŸå­çš„è¡¨ç¤º
    atom_pair_1 = x[pair_indices[:, 1]]      # ç¬¬äºŒä¸ªåŸå­çš„è¡¨ç¤º

    # 4. èåˆåŸå­å¯¹ç‰¹å¾å’Œå›¾è¡¨ç¤º
    combined = torch.cat([atom_pair_0, atom_pair_1, pair_features], dim=1)

    # 5. é¢„æµ‹è€¦åˆå¸¸æ•°
    return self.pair_mlp(combined)
```

#### C. GCNåŸç†è¯¦è§£
**å›¾å·ç§¯æ“ä½œ:**
```
h_i^(l+1) = Ïƒ(W^(l) Â· MEAN(h_j^(l) for j in N(i) âˆª {i}))
```

å…¶ä¸­ï¼š
- `h_i^(l)`: èŠ‚ç‚¹iåœ¨ç¬¬lå±‚çš„ç‰¹å¾
- `N(i)`: èŠ‚ç‚¹içš„é‚»å±…é›†åˆ
- `W^(l)`: ç¬¬lå±‚çš„å¯è®­ç»ƒæƒé‡çŸ©é˜µ
- `Ïƒ`: æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰

**è®¾è®¡ä¼˜åŠ¿:**
- **å±€éƒ¨èšåˆ**: æ¯ä¸ªåŸå­èšåˆé‚»å±…åŸå­çš„ä¿¡æ¯
- **å¤šå±‚å †å **: é€æ­¥æ‰©å¤§æ„Ÿå—é‡
- **ç½®æ¢ä¸å˜**: å¯¹åŸå­é¡ºåºä¸æ•æ„Ÿ
- **å‚æ•°å…±äº«**: ç›¸åŒçš„å·ç§¯æ ¸åº”ç”¨äºæ‰€æœ‰åŸå­

#### D. é€‚ç”¨åœºæ™¯
- **åˆ†å­å»ºæ¨¡**: å¤©ç„¶é€‚åˆåˆ†å­å›¾ç»“æ„
- **ä¸­ç­‰å¤æ‚åº¦**: åœ¨æ€§èƒ½å’Œæ•ˆç‡é—´å¹³è¡¡
- **å¯æ‰©å±•æ€§**: å¯å¤„ç†ä¸åŒå¤§å°çš„åˆ†å­
- **å¯è§£é‡Šæ€§**: å¯è§†åŒ–åŸå­é‡è¦æ€§

### 5.3 CouplingGATï¼ˆå›¾æ³¨æ„åŠ›ç½‘ç»œï¼‰

#### A. æ¶æ„è®¾è®¡
```python
class CouplingGAT(nn.Module):
    def __init__(self, num_atom_features, num_pair_features, hidden_dim=128, num_layers=3, heads=4):
        super().__init__()

        # å¤šå¤´æ³¨æ„åŠ›å±‚
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()

        # ç¬¬ä¸€å±‚
        self.convs.append(GATConv(hidden_dim, hidden_dim//heads, heads=heads, dropout=0.2))
        self.batch_norms.append(BatchNorm(hidden_dim))

        # ä¸­é—´å±‚
        for i in range(num_layers - 2):
            self.convs.append(GATConv(hidden_dim, hidden_dim//heads, heads=heads, dropout=0.2))
            self.batch_norms.append(BatchNorm(hidden_dim))

        # æœ€åå±‚
        if num_layers > 1:
            self.convs.append(GATConv(hidden_dim, hidden_dim, heads=1, dropout=0.2))
            self.batch_norms.append(BatchNorm(hidden_dim))
```

#### B. æ³¨æ„åŠ›æœºåˆ¶åŸç†
**å¤šå¤´æ³¨æ„åŠ›è®¡ç®—:**
```
Î±_ij^k = softmax(LeakyReLU(a_k^T [W_k h_i || W_k h_j]))
h_i^(l+1) = Ïƒ(CONCAT(Î£_j Î±_ij^k W_k h_j) for k=1..K)
```

å…¶ä¸­ï¼š
- `Î±_ij^k`: ç¬¬kä¸ªå¤´ä¸­èŠ‚ç‚¹jå¯¹èŠ‚ç‚¹içš„æ³¨æ„åŠ›æƒé‡
- `W_k`: ç¬¬kä¸ªå¤´çš„æƒé‡çŸ©é˜µ
- `a_k`: ç¬¬kä¸ªå¤´çš„æ³¨æ„åŠ›å‘é‡
- `||`: ç‰¹å¾æ‹¼æ¥æ“ä½œ

#### C. è®¾è®¡ä¼˜åŠ¿
**è‡ªé€‚åº”æƒé‡:**
```python
# æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å­¦ä¹ åŸå­é‡è¦æ€§
attention_weights = softmax(attention_scores)
neighbor_features = attention_weights @ neighbor_embeddings
```

**å¤šå¤´æœºåˆ¶:**
- **å¤šè§’åº¦**: æ¯ä¸ªå¤´å…³æ³¨ä¸åŒçš„åŒ–å­¦æ€§è´¨
- **å¹¶è¡Œå¤„ç†**: åŒæ—¶è®¡ç®—å¤šç§æ³¨æ„åŠ›æ¨¡å¼
- **è¡¨ç¤ºä¸°å¯Œ**: ç»¼åˆå¤šä¸ªå¤´çš„ä¿¡æ¯

#### D. åŒ–å­¦è§£é‡Š
- **ç”µè´Ÿæ€§å…³æ³¨**: æŸäº›å¤´å¯èƒ½å…³æ³¨ç”µè´Ÿæ€§å·®å¼‚
- **è·ç¦»æ•æ„Ÿ**: æŸäº›å¤´å¯èƒ½å…³æ³¨ç©ºé—´è·ç¦»
- **é”®å‹è¯†åˆ«**: æŸäº›å¤´å¯èƒ½è¯†åˆ«ç‰¹å®šé”®å‹
- **ç¯å¢ƒæ„ŸçŸ¥**: æŸäº›å¤´å¯èƒ½æ„ŸçŸ¥åŒ–å­¦ç¯å¢ƒ

#### E. é€‚ç”¨åœºæ™¯
- **å¤æ‚åˆ†å­**: èƒ½å¤Ÿå¤„ç†å¤æ‚çš„åŒ–å­¦ç¯å¢ƒ
- **ç²¾åº¦è¦æ±‚é«˜**: é€šå¸¸æ¯”GCNæ€§èƒ½æ›´å¥½
- **å¯è§£é‡Šæ€§**: å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
- **ä¸­ç­‰è®¡ç®—èµ„æº**: æ¯”Transformerè½»é‡

### 5.4 CouplingTransformerï¼ˆå›¾Transformerï¼‰

#### A. æ¶æ„è®¾è®¡
```python
class CouplingTransformer(nn.Module):
    def __init__(self, num_atom_features, num_pair_features, hidden_dim=128, num_layers=2, heads=8):
        super().__init__()

        # Transformerå±‚
        self.transformers = nn.ModuleList()
        self.layer_norms = nn.ModuleList()

        for _ in range(num_layers):
            self.transformers.append(
                TransformerConv(hidden_dim, hidden_dim//heads, heads=heads, dropout=0.2)
            )
            self.layer_norms.append(LayerNorm(hidden_dim))
```

#### B. å‰å‘ä¼ æ’­æœºåˆ¶
```python
def forward(self, atom_features, edge_index, pair_indices, pair_features):
    x = self.atom_embedding(atom_features)

    for i in range(self.num_layers):
        residual = x  # ä¿å­˜æ®‹å·®è¿æ¥
        x = self.transformers[i](x, edge_index)  # Transformerå±‚
        x = self.layer_norms[i](x)               # å±‚æ ‡å‡†åŒ–
        x = F.gelu(x + residual)                 # æ®‹å·®è¿æ¥ + GELUæ¿€æ´»
        x = F.dropout(x, p=0.2, training=self.training)
```

#### C. Graph Transformerç‰¹ç‚¹
**å…¨å±€æ³¨æ„åŠ›:**
- **é•¿ç¨‹ä¾èµ–**: èƒ½å¤Ÿæ•è·è¿œè·ç¦»åŸå­ç›¸äº’ä½œç”¨
- **å…¨å±€ä¿¡æ¯**: æ¯ä¸ªåŸå­éƒ½èƒ½æ„ŸçŸ¥æ•´ä¸ªåˆ†å­
- **ä½ç½®ç¼–ç **: é€šè¿‡è¾¹ä¿¡æ¯ç¼–ç ç©ºé—´å…³ç³»

**æ®‹å·®è¿æ¥:**
- **æ·±åº¦ç½‘ç»œ**: æ”¯æŒæ›´æ·±çš„ç½‘ç»œæ¶æ„
- **æ¢¯åº¦æµåŠ¨**: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **è®­ç»ƒç¨³å®š**: æé«˜è®­ç»ƒç¨³å®šæ€§

#### D. é€‚ç”¨åœºæ™¯
- **å¤æ‚åˆ†å­ç³»ç»Ÿ**: å¤§åˆ†å­æˆ–å¤æ‚ç¯ç³»
- **é•¿ç¨‹ç›¸äº’ä½œç”¨**: éœ€è¦è€ƒè™‘è¿œè·ç¦»æ•ˆåº”
- **æœ€é«˜ç²¾åº¦è¦æ±‚**: é€šå¸¸æ€§èƒ½æœ€ä½³
- **å……è¶³è®¡ç®—èµ„æº**: éœ€è¦æ›´å¤šGPUå†…å­˜å’Œæ—¶é—´

#### E. ä¼˜ç¼ºç‚¹åˆ†æ
**ä¼˜ç‚¹:**
- **æœ€ä½³æ€§èƒ½**: é€šå¸¸è·å¾—æœ€ä½çš„MAE
- **å…¨å±€æ„ŸçŸ¥**: èƒ½å¤Ÿæ•è·æ•´ä¸ªåˆ†å­çš„ä¿¡æ¯
- **å¯æ‰©å±•**: å¯ä»¥è½»æ¾æ‰©å±•åˆ°æ›´å¤§çš„åˆ†å­
- **å…ˆè¿›æ¶æ„**: åŸºäºæœ€æ–°çš„TransformeræŠ€æœ¯

**ç¼ºç‚¹:**
- **è®¡ç®—å¤æ‚**: è®­ç»ƒæ—¶é—´æœ€é•¿
- **å†…å­˜éœ€æ±‚**: éœ€è¦æ›´å¤šGPUå†…å­˜
- **å‚æ•°æœ€å¤š**: å®¹æ˜“è¿‡æ‹Ÿåˆå°æ•°æ®é›†
- **è°ƒå‚å¤æ‚**: è¶…å‚æ•°è¾ƒå¤š

### 5.5 AdvancedMLPï¼ˆé«˜çº§ç‰¹å¾MLPï¼‰

#### A. æ¶æ„è®¾è®¡
```python
class AdvancedMLP(nn.Module):
    def __init__(self, num_features, hidden_dims=[256, 128, 64], dropout=0.3):
        super().__init__()

        layers = []
        input_dim = num_features

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),       # æ‰¹æ ‡å‡†åŒ–
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            input_dim = hidden_dim

        layers.append(nn.Linear(input_dim, 1))
        self.mlp = nn.Sequential(*layers)
```

#### B. ç‰¹å¾å·¥ç¨‹ç­–ç•¥
**å¤šå±‚æ¬¡ç‰¹å¾èåˆ:**
```python
# åŸºç¡€åŸå­å¯¹ç‰¹å¾ï¼ˆ8ç»´ï¼‰
pair_feats = [distance, rel_pos_x, rel_pos_y, rel_pos_z,
              atom_0_type, atom_1_type, coupling_type, mol_size]

# é«˜çº§åˆ†å­ç‰¹å¾ï¼ˆ42ç»´ï¼‰
advanced_feats = [
    # æ‹“æ‰‘ç‰¹å¾ï¼ˆ7ç»´ï¼‰
    num_atoms, num_H, num_C, num_N, num_O, num_F, density,

    # å‡ ä½•ç‰¹å¾ï¼ˆ10ç»´ï¼‰
    bbox_x, bbox_y, bbox_z, center_x, center_y, center_z,
    gyration_radius, mol_weight, volume, surface_area,

    # ... æ›´å¤šç‰¹å¾
]

# ç»„åˆç‰¹å¾ï¼ˆ50ç»´ï¼‰
combined_features = np.concatenate([pair_feats, advanced_feats])
```

#### C. ç‰¹å¾é€‰æ‹©æœºåˆ¶
```python
# ä½¿ç”¨Fç»Ÿè®¡é‡é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾
selector = SelectKBest(f_regression, k=50)
selected_features = selector.fit_transform(all_features, targets)

# ç‰¹å¾é‡è¦æ€§æ’åº
feature_scores = selector.scores_
important_features = np.argsort(feature_scores)[::-1][:50]
```

#### D. è®¾è®¡ç†å¿µ
**ç‰¹å¾å·¥ç¨‹ä¸ºç‹:**
- **é¢†åŸŸçŸ¥è¯†**: èåˆåŒ–å­¦å’Œç‰©ç†å…ˆéªŒçŸ¥è¯†
- **å¤šå°ºåº¦ä¿¡æ¯**: ä»åŸå­åˆ°åˆ†å­çš„å¤šå±‚æ¬¡ç‰¹å¾
- **ç»Ÿè®¡ç‰¹å¾**: åŸºäºæ•°æ®åˆ†å¸ƒçš„ç»Ÿè®¡æè¿°ç¬¦
- **å‡ ä½•æè¿°**: 3Dç©ºé—´ç»“æ„çš„æ•°å­¦æè¿°

#### E. ä¼˜åŠ¿åˆ†æ
- **é«˜ç²¾åº¦**: é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„ç‰¹å¾è·å¾—ä¼˜ç§€æ€§èƒ½
- **å¯è§£é‡Š**: æ¯ä¸ªç‰¹å¾éƒ½æœ‰æ˜ç¡®çš„åŒ–å­¦æˆ–ç‰©ç†æ„ä¹‰
- **ç¨³å®šæ€§**: ä¸ä¾èµ–äºå¤æ‚çš„ç½‘ç»œæ¶æ„
- **æ•ˆç‡**: ç›¸å¯¹äºå¤æ‚ç½‘ç»œæ¨¡å‹è®­ç»ƒæ›´å¿«

---

## 6. è®­ç»ƒå’Œè¯„ä¼°æµç¨‹

### 6.1 ç»Ÿä¸€è®­ç»ƒæ¡†æ¶

#### A. è®­ç»ƒå‡½æ•°è®¾è®¡
```python
def train_model(self, model, train_loader, val_loader, model_name, num_epochs=25, lr=0.001):
    """ç»Ÿä¸€çš„æ¨¡å‹è®­ç»ƒå‡½æ•°"""

    # 1. æ¨¡å‹è®¾ç½®
    model.to(self.device)
    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    criterion = nn.MSELoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.8, patience=5
    )

    # 2. è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0

        for batch in train_loader:
            optimizer.zero_grad()

            # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒç”¨ä¸åŒçš„å‰å‘ä¼ æ’­
            if isinstance(model, SimpleMLP):
                predictions = model(batch_features)
            else:  # GNNæ¨¡å‹
                predictions = model(atom_features, edge_index, pair_indices, pair_features)

            loss = criterion(predictions.squeeze(), targets)
            loss.backward()

            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()
            train_loss += loss.item()

        # éªŒè¯é˜¶æ®µ
        val_loss = evaluate_on_validation_set()

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()
```

#### B. è®­ç»ƒç­–ç•¥è¯¦è§£

**ä¼˜åŒ–å™¨é€‰æ‹©:**
```python
# Adamä¼˜åŒ–å™¨ - è‡ªé€‚åº”å­¦ä¹ ç‡
optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
```
- **è‡ªé€‚åº”**: æ¯ä¸ªå‚æ•°ç‹¬ç«‹çš„å­¦ä¹ ç‡
- **åŠ¨é‡**: åŠ é€Ÿæ”¶æ•›ï¼Œé¿å…éœ‡è¡
- **æƒé‡è¡°å‡**: L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ

**å­¦ä¹ ç‡è°ƒåº¦:**
```python
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)
```
- **è‡ªé€‚åº”å‡å°‘**: éªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶å‡å°‘å­¦ä¹ ç‡
- **è€å¿ƒæœºåˆ¶**: ç­‰å¾…5ä¸ªepochå†è°ƒæ•´
- **è¡°å‡å› å­**: æ¯æ¬¡å‡å°‘åˆ°åŸæ¥çš„80%

**æ¢¯åº¦è£å‰ª:**
```python
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
```
- **é˜²æ­¢çˆ†ç‚¸**: é™åˆ¶æ¢¯åº¦çš„L2èŒƒæ•°
- **è®­ç»ƒç¨³å®š**: ç‰¹åˆ«å¯¹æ·±å±‚ç½‘ç»œé‡è¦
- **é˜ˆå€¼è®¾ç½®**: 1.0æ˜¯å¸¸ç”¨çš„ä¿å®ˆå€¼

#### C. æ‰¹å¤„ç†ç­–ç•¥

**ä¸åŒæ¨¡å‹çš„æ‰¹å¤§å°:**
```python
batch_sizes = {
    'SimpleMLP': 128,        # ç®€å•æ¨¡å‹ï¼Œå¤§æ‰¹é‡
    'GCN': 32,              # ä¸­ç­‰å¤æ‚åº¦
    'GAT': 32,              # ä¸­ç­‰å¤æ‚åº¦
    'Transformer': 16,       # å¤æ‚æ¨¡å‹ï¼Œå°æ‰¹é‡
    'AdvancedMLP': 128      # ç‰¹å¾ä¸°å¯Œï¼Œå¤§æ‰¹é‡
}
```

**å†…å­˜ä¼˜åŒ–è€ƒè™‘:**
- **GPUå†…å­˜é™åˆ¶**: å¤æ‚æ¨¡å‹éœ€è¦å°æ‰¹é‡
- **è®­ç»ƒç¨³å®šæ€§**: å°æ‰¹é‡æä¾›æ›´å¤šæ¢¯åº¦æ›´æ–°
- **æ”¶æ•›é€Ÿåº¦**: å¤§æ‰¹é‡æ”¶æ•›æ›´å¿«ä½†éœ€è¦æ›´å¤šå†…å­˜

### 6.2 æ•°æ®åˆ’åˆ†ç­–ç•¥

#### A. åˆ’åˆ†æ¯”ä¾‹
```python
# æ ‡å‡†åˆ’åˆ†æ¯”ä¾‹
train_ratio = 0.70  # 70% ç”¨äºè®­ç»ƒ
val_ratio = 0.15    # 15% ç”¨äºéªŒè¯
test_ratio = 0.15   # 15% ç”¨äºæœ€ç»ˆæµ‹è¯•
```

#### B. éšæœºç§å­æ§åˆ¶
```python
torch.manual_seed(42)
np.random.seed(42)
torch.backends.cudnn.deterministic = True
```
- **å¯é‡ç°æ€§**: ç¡®ä¿å®éªŒç»“æœå¯é‡å¤
- **å…¬å¹³æ¯”è¾ƒ**: æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ’åˆ†
- **è°ƒè¯•ä¾¿åˆ©**: å›ºå®šéšæœºæ€§ä¾¿äºè°ƒè¯•

#### C. åˆ†å±‚é‡‡æ ·ï¼ˆå¯é€‰ï¼‰
```python
# æ ¹æ®è€¦åˆç±»å‹è¿›è¡Œåˆ†å±‚é‡‡æ ·
from sklearn.model_selection import train_test_split
X_train, X_temp, y_train, y_temp = train_test_split(
    features, targets, test_size=0.3, stratify=coupling_types, random_state=42
)
```

### 6.3 è¯„ä¼°æŒ‡æ ‡ä½“ç³»

#### A. æ ¸å¿ƒæŒ‡æ ‡
```python
def evaluate_model(self, model, test_loader):
    predictions = []
    targets = []

    # æ”¶é›†é¢„æµ‹ç»“æœ
    for batch in test_loader:
        pred = model(batch_inputs)
        predictions.extend(pred.cpu().numpy())
        targets.extend(batch_targets.cpu().numpy())

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = mean_absolute_error(targets, predictions)
    mse = mean_squared_error(targets, predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(targets, predictions)

    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}
```

#### B. æŒ‡æ ‡è§£é‡Š

**1. MAE (Mean Absolute Error) - ä¸»è¦æŒ‡æ ‡**
```
MAE = (1/n) * Î£|y_true - y_pred|
```
- **ç‰©ç†æ„ä¹‰**: é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å¹³å‡ç»å¯¹åå·®ï¼ˆHzï¼‰
- **ä¼˜ç‚¹**: ç›´è§‚æ˜“æ‡‚ï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- **åŒ–å­¦æ„ä¹‰**: ç›´æ¥åæ˜ NMRé¢„æµ‹çš„ç²¾åº¦

**2. RMSE (Root Mean Square Error)**
```
RMSE = âˆš[(1/n) * Î£(y_true - y_pred)Â²]
```
- **ç‰©ç†æ„ä¹‰**: é¢„æµ‹è¯¯å·®çš„å‡æ–¹æ ¹ï¼ˆHzï¼‰
- **ç‰¹ç‚¹**: å¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿ
- **ç”¨é€”**: è¯„ä¼°é¢„æµ‹çš„ç¨³å®šæ€§

**3. RÂ² (Coefficient of Determination)**
```
RÂ² = 1 - SS_res/SS_tot = 1 - Î£(y_true - y_pred)Â²/Î£(y_true - È³)Â²
```
- **å–å€¼èŒƒå›´**: [0, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½
- **ç‰©ç†æ„ä¹‰**: æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
- **åŒ–å­¦æ„ä¹‰**: æ¨¡å‹æ•è·åŒ–å­¦è§„å¾‹çš„ç¨‹åº¦

#### C. è¾…åŠ©æŒ‡æ ‡

**è®­ç»ƒæ•ˆç‡æŒ‡æ ‡:**
```python
training_metrics = {
    'training_time': time_elapsed,           # è®­ç»ƒè€—æ—¶
    'num_parameters': count_parameters(model), # å‚æ•°æ•°é‡
    'memory_usage': torch.cuda.max_memory_allocated(), # æ˜¾å­˜å ç”¨
    'convergence_epoch': best_epoch          # æ”¶æ•›è½®æ•°
}
```

**æ¨¡å‹å¤æ‚åº¦åˆ†æ:**
```python
def count_parameters(model):
    """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    non_trainable = sum(p.numel() for p in model.parameters() if not p.requires_grad)
    return {'trainable': trainable, 'non_trainable': non_trainable, 'total': trainable + non_trainable}
```

### 6.4 æ—©åœå’Œæ¨¡å‹é€‰æ‹©

#### A. æ—©åœæœºåˆ¶
```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')

    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience
```

#### B. æ¨¡å‹æ£€æŸ¥ç‚¹
```python
# ä¿å­˜æœ€ä½³æ¨¡å‹
if val_loss < best_val_loss:
    best_val_loss = val_loss
    best_model_state = model.state_dict().copy()

# è®­ç»ƒç»“æŸååŠ è½½æœ€ä½³æƒé‡
model.load_state_dict(best_model_state)
```

---

## 7. ç»“æœåˆ†æå’Œè§£é‡Š

### 7.1 æ€§èƒ½æ’è¡Œæ¦œåˆ†æ

#### A. å…¸å‹ç»“æœç¤ºä¾‹
```
ğŸ† æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ (æŒ‰MAEæ’åº):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ’å  æ¨¡å‹                  MAE        RMSE       RÂ²         å‚æ•°é‡      è®­ç»ƒæ—¶é—´
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1    Advanced_Features     0.8750     1.2340     0.8910     298,000     220.5s
2    Transformer          0.9250     1.2980     0.8780     256,000     240.1s
3    GAT                  0.9850     1.3450     0.8650     189,000     195.3s
4    GCN                  1.1250     1.4780     0.8350     109,000     120.2s
5    Simple_MLP           1.2750     1.6120     0.8010     3,000       45.8s
```

#### B. ç»“æœè§£è¯»

**1. Advanced Features (ç¬¬1å)**
- **MAE**: 0.875 Hz - æœ€ä½³é¢„æµ‹ç²¾åº¦
- **æˆåŠŸå› ç´ **:
  - ç²¾å¿ƒè®¾è®¡çš„åŒ–å­¦ç‰¹å¾
  - é¢†åŸŸçŸ¥è¯†çš„å……åˆ†åˆ©ç”¨
  - æœ‰æ•ˆçš„ç‰¹å¾é€‰æ‹©
- **é€‚ç”¨åœºæ™¯**: å¯¹ç²¾åº¦è¦æ±‚æœ€é«˜çš„åº”ç”¨
- **ä»£ä»·**: éœ€è¦å¤æ‚çš„ç‰¹å¾å·¥ç¨‹

**2. Transformer (ç¬¬2å)**
- **MAE**: 0.925 Hz - ç¬¬äºŒä½³æ€§èƒ½
- **æˆåŠŸå› ç´ **:
  - å…¨å±€æ³¨æ„åŠ›æœºåˆ¶
  - é•¿ç¨‹ä¾èµ–å»ºæ¨¡èƒ½åŠ›
  - å…ˆè¿›çš„ç½‘ç»œæ¶æ„
- **ä»£ä»·**: æœ€é«˜çš„è®¡ç®—æˆæœ¬
- **é€‚ç”¨åœºæ™¯**: å¤æ‚åˆ†å­ç³»ç»Ÿ

**3. GAT (ç¬¬3å)**
- **MAE**: 0.985 Hz - å¹³è¡¡æ€§èƒ½
- **æˆåŠŸå› ç´ **:
  - è‡ªé€‚åº”æ³¨æ„åŠ›æƒé‡
  - åŒ–å­¦ç¯å¢ƒæ„ŸçŸ¥èƒ½åŠ›
  - ç›¸å¯¹è¾ƒå°‘çš„å‚æ•°
- **ä¼˜åŠ¿**: æ€§èƒ½ä¸æ•ˆç‡çš„æœ€ä½³å¹³è¡¡
- **é€‚ç”¨åœºæ™¯**: å¤§å¤šæ•°å®é™…åº”ç”¨çš„æ¨èé€‰æ‹©

**4. GCN (ç¬¬4å)**
- **MAE**: 1.125 Hz - ç»å…¸åŸºå‡†
- **åœ°ä½**: å›¾ç¥ç»ç½‘ç»œçš„ç»å…¸ä»£è¡¨
- **ä¼˜åŠ¿**: ç®€å•ã€ç¨³å®šã€å¯è§£é‡Š
- **é€‚ç”¨åœºæ™¯**: å¿«é€ŸåŸå‹å¼€å‘

**5. Simple MLP (ç¬¬5å)**
- **MAE**: 1.275 Hz - åŸºçº¿æ€§èƒ½
- **ä»·å€¼**: æä¾›æ€§èƒ½ä¸‹ç•Œ
- **ä¼˜åŠ¿**: æå¿«çš„è®­ç»ƒé€Ÿåº¦
- **å±€é™**: å¿½ç•¥äº†åˆ†å­å›¾ç»“æ„ä¿¡æ¯

### 7.2 æ€§èƒ½vså¤æ‚åº¦åˆ†æ

#### A. æ•ˆç‡å‰æ²¿å›¾
```
æ€§èƒ½(MAE) vs å¤æ‚åº¦(å‚æ•°é‡)

1.4 |  Simple_MLP â—
    |
1.2 |         GCN â—
    |
1.0 |              GAT â—
    |
0.8 |                    â— Advanced_Features
    |                  Transformer â—
0.6 +----+----+----+----+----+----+
    0   50K  100K 150K 200K 250K 300K
              å‚æ•°æ•°é‡
```

#### B. å…³é”®æ´å¯Ÿ

**å¸•ç´¯æ‰˜å‰æ²¿:**
- **GAT**: æœ€ä½³çš„æ€§èƒ½-æ•ˆç‡å¹³è¡¡ç‚¹
- **Advanced Features**: æœ€ä½³æ€§èƒ½ä½†å¤æ‚åº¦é«˜
- **Simple MLP**: æœ€ä½å¤æ‚åº¦ä½†æ€§èƒ½å—é™

**è¾¹é™…æ”¶ç›Šé€’å‡:**
- ä»Simple MLPåˆ°GCNï¼šæ˜¾è‘—æ€§èƒ½æå‡
- ä»GCNåˆ°GATï¼šä¸­ç­‰æ€§èƒ½æå‡
- ä»GATåˆ°Transformerï¼šè¾ƒå°æ€§èƒ½æå‡ï¼Œæˆæœ¬å¤§å¹…å¢åŠ 

### 7.3 ä¸åŒè€¦åˆç±»å‹çš„è¡¨ç°

#### A. æŒ‰è€¦åˆç±»å‹åˆ†æ
```python
# å…¸å‹å„è€¦åˆç±»å‹çš„MAEè¡¨ç°ï¼ˆAdvanced Featuresæ¨¡å‹ï¼‰
coupling_performance = {
    '1JHC': 0.65,   # å•é”®C-Hè€¦åˆï¼Œæœ€æ˜“é¢„æµ‹
    '1JCC': 0.72,   # å•é”®C-Cè€¦åˆ
    '2JHH': 0.85,   # äºŒé”®H-Hè€¦åˆ
    '2JHC': 0.91,   # äºŒé”®H-Cè€¦åˆ
    '2JCH': 0.89,   # äºŒé”®C-Hè€¦åˆ
    '3JHH': 1.12,   # ä¸‰é”®H-Hè€¦åˆï¼Œæœ€éš¾é¢„æµ‹
    '3JHC': 1.08,   # ä¸‰é”®H-Cè€¦åˆ
}
```

#### B. åŒ–å­¦è§£é‡Š

**1Jè€¦åˆï¼ˆæœ€æ˜“é¢„æµ‹ï¼‰:**
- **ç‰©ç†å¼ºåº¦å¤§**: è€¦åˆå¸¸æ•°ç»å¯¹å€¼å¤§ï¼Œä¿¡å·å¼º
- **å‡ ä½•ä¾èµ–ä½**: ä¸»è¦ç”±ç›´æ¥é”®æ€§è´¨å†³å®š
- **å˜å¼‚æ€§å°**: ç›¸åŒç±»å‹é”®çš„è€¦åˆå¸¸æ•°ç›¸è¿‘

**2Jè€¦åˆï¼ˆä¸­ç­‰éš¾åº¦ï¼‰:**
- **å‡ ä½•æ•æ„Ÿ**: ä¾èµ–äºé”®è§’å’Œç«‹ä½“åŒ–å­¦
- **ç¯å¢ƒå½±å“**: å—ç›¸é‚»åŸå­ç”µå­æ•ˆåº”å½±å“
- **å˜å¼‚æ€§ä¸­ç­‰**: åŒ–å­¦ç¯å¢ƒçš„å½±å“å¼€å§‹æ˜¾ç°

**3Jè€¦åˆï¼ˆæœ€éš¾é¢„æµ‹ï¼‰:**
- **æ„è±¡ä¾èµ–**: å¼ºçƒˆä¾èµ–åˆ†å­æ„è±¡
- **é•¿ç¨‹æ•ˆåº”**: å—å¤šä¸ªåŸå­çš„ååŒå½±å“
- **å˜å¼‚æ€§å¤§**: å¾®å°çš„ç»“æ„å˜åŒ–å¯¼è‡´è¾ƒå¤§çš„è€¦åˆå˜åŒ–

### 7.4 æ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ

#### A. ç‰¹å¾é‡è¦æ€§åˆ†æ
```python
# Advanced Featuresæ¨¡å‹ä¸­æœ€é‡è¦çš„ç‰¹å¾
feature_importance = {
    'distance': 0.45,           # åŸå­é—´è·ç¦»æœ€é‡è¦
    'coupling_type': 0.18,      # è€¦åˆç±»å‹ç¼–ç 
    'atom_0_type': 0.12,        # ç¬¬ä¸€ä¸ªåŸå­ç±»å‹
    'atom_1_type': 0.11,        # ç¬¬äºŒä¸ªåŸå­ç±»å‹
    'mol_weight': 0.08,         # åˆ†å­é‡
    'gyration_radius': 0.06,    # å›è½¬åŠå¾„
    # ... å…¶ä»–ç‰¹å¾
}
```

#### B. åŒ–å­¦ç›´è§‰éªŒè¯

**è·ç¦»æ•ˆåº”:**
- **è·ç¦»æœ€é‡è¦**: ç¬¦åˆè€¦åˆå¼ºåº¦ä¸è·ç¦»è´Ÿç›¸å…³çš„åŒ–å­¦è§„å¾‹
- **æŒ‡æ•°è¡°å‡**: è€¦åˆå¸¸æ•°éšè·ç¦»æŒ‡æ•°è¡°å‡

**åŸå­ç±»å‹æ•ˆåº”:**
- **ç”µè´Ÿæ€§å½±å“**: ä¸åŒåŸå­çš„ç”µè´Ÿæ€§å·®å¼‚å½±å“è€¦åˆ
- **è½¨é“é‡å **: ä¸åŒåŸå­çš„è½¨é“é‡å ç¨‹åº¦ä¸åŒ

**åˆ†å­ç¯å¢ƒ:**
- **åˆ†å­å¤§å°**: å¤§åˆ†å­ä¸­çš„å±è”½æ•ˆåº”
- **å‡ ä½•å½¢çŠ¶**: åˆ†å­çš„ç©ºé—´æ’å¸ƒå½±å“

#### C. æ³¨æ„åŠ›å¯è§†åŒ–ï¼ˆGAT/Transformerï¼‰

å¯¹äºGATå’ŒTransformeræ¨¡å‹ï¼Œå¯ä»¥å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ï¼š

```python
def visualize_attention(model, mol_data):
    """å¯è§†åŒ–åŸå­é—´çš„æ³¨æ„åŠ›æƒé‡"""
    with torch.no_grad():
        attention_weights = model.get_attention_weights(mol_data)

    # ç»˜åˆ¶æ³¨æ„åŠ›çƒ­å›¾
    plt.imshow(attention_weights, cmap='hot', interpolation='nearest')
    plt.colorbar(label='Attention Weight')
    plt.title('Atom-Atom Attention Matrix')
```

**æ³¨æ„åŠ›æ¨¡å¼è§£è¯»:**
- **å±€éƒ¨æ³¨æ„**: ç›¸é‚»åŸå­é—´çš„é«˜æ³¨æ„åŠ›
- **é•¿ç¨‹æ³¨æ„**: ç‰¹æ®ŠåŒ–å­¦åŸºå›¢é—´çš„é•¿ç¨‹æ³¨æ„
- **ç±»å‹é€‰æ‹©æ€§**: ç‰¹å®šåŸå­ç±»å‹å¯¹çš„é«˜æ³¨æ„åŠ›

### 7.5 è¯¯å·®åˆ†æ

#### A. æ®‹å·®åˆ†æ
```python
# è®¡ç®—é¢„æµ‹æ®‹å·®
residuals = predictions - targets

# æ®‹å·®ç»Ÿè®¡
print(f"æ®‹å·®å‡å€¼: {np.mean(residuals):.4f}")
print(f"æ®‹å·®æ ‡å‡†å·®: {np.std(residuals):.4f}")
print(f"æœ€å¤§æ­£è¯¯å·®: {np.max(residuals):.4f}")
print(f"æœ€å¤§è´Ÿè¯¯å·®: {np.min(residuals):.4f}")
```

#### B. è¯¯å·®åˆ†å¸ƒç‰¹å¾

**ç†æƒ³æƒ…å†µ:**
- æ®‹å·®å‡å€¼æ¥è¿‘0ï¼ˆæ— ç³»ç»Ÿåå·®ï¼‰
- æ®‹å·®å‘ˆæ­£æ€åˆ†å¸ƒï¼ˆéšæœºè¯¯å·®ï¼‰
- æ–¹å·®é½æ€§ï¼ˆé¢„æµ‹ç¨³å®šæ€§å¥½ï¼‰

**å¸¸è§é—®é¢˜:**
- **ç³»ç»Ÿåå·®**: æ®‹å·®å‡å€¼åç¦»0
- **å¼‚æ–¹å·®æ€§**: é¢„æµ‹å€¼ä¸åŒæ—¶è¯¯å·®æ–¹å·®ä¸åŒ
- **å¼‚å¸¸å€¼**: æç«¯çš„é¢„æµ‹è¯¯å·®

#### C. å›°éš¾æ ·æœ¬åˆ†æ

**é«˜è¯¯å·®æ ·æœ¬ç‰¹å¾:**
1. **ç½•è§è€¦åˆç±»å‹**: è®­ç»ƒæ•°æ®ä¸­æ ·æœ¬è¾ƒå°‘çš„ç±»å‹
2. **å¤§åˆ†å­**: åŸå­æ•°é‡è¶…è¿‡è®­ç»ƒæ•°æ®åˆ†å¸ƒ
3. **ç‰¹æ®ŠåŒ–å­¦åŸºå›¢**: å«æœ‰ä¸å¸¸è§åŸå­ç»„åˆ
4. **æç«¯å‡ ä½•**: å¼‚å¸¸çš„é”®é•¿æˆ–é”®è§’

**æ”¹è¿›ç­–ç•¥:**
1. **æ•°æ®å¢å¼º**: å¢åŠ å›°éš¾æ ·æœ¬çš„è®­ç»ƒæ•°æ®
2. **é›†æˆå­¦ä¹ **: ç»“åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹
3. **ä¸ç¡®å®šæ€§é‡åŒ–**: è¯†åˆ«ä½ç½®ä¿¡åº¦é¢„æµ‹
4. **ä¸»åŠ¨å­¦ä¹ **: é‡ç‚¹é‡‡æ ·å›°éš¾æ ·æœ¬

---

## 8. å®è·µæŒ‡å—

### 8.1 ç¯å¢ƒé…ç½®è¯¦è§£

#### A. ä¾èµ–å®‰è£…
```bash
# åŸºç¡€ç¯å¢ƒ
conda create -n coupling-prediction python=3.8
conda activate coupling-prediction

# PyTorchç”Ÿæ€
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
pip install torch-geometric

# æ•°æ®ç§‘å­¦åº“
pip install pandas numpy matplotlib seaborn scikit-learn

# åŒ–å­¦ä¿¡æ¯å­¦ï¼ˆå¯é€‰ï¼‰
conda install -c conda-forge rdkit-python

# å…¶ä»–å·¥å…·
pip install jupyterlab tqdm
```

#### B. ç¡¬ä»¶æ¨è

**æœ€ä½é…ç½®:**
- **CPU**: Intel i5 æˆ– AMD Ryzen 5
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 10GB å¯ç”¨ç©ºé—´
- **Python**: 3.7+

**æ¨èé…ç½®:**
- **CPU**: Intel i7/i9 æˆ– AMD Ryzen 7/9
- **å†…å­˜**: 16GB+ RAM
- **GPU**: NVIDIA RTX 3060+ (8GB+ VRAM)
- **å­˜å‚¨**: SSD 20GB+ å¯ç”¨ç©ºé—´

**é«˜æ€§èƒ½é…ç½®:**
- **CPU**: Intel Xeon æˆ– AMD EPYC
- **å†…å­˜**: 32GB+ RAM
- **GPU**: NVIDIA RTX 4080/4090 æˆ– A100
- **å­˜å‚¨**: NVMe SSD

#### C. æ€§èƒ½ä¼˜åŒ–è®¾ç½®
```python
# PyTorchä¼˜åŒ–
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False

# å¤šçº¿ç¨‹è®¾ç½®
import os
os.environ['OMP_NUM_THREADS'] = '4'  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´

# å†…å­˜ç®¡ç†
torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
```

### 8.2 æ•°æ®å‡†å¤‡æŒ‡å—

#### A. æ•°æ®ä¸‹è½½å’ŒéªŒè¯
```bash
# 1. ä¸‹è½½æ•°æ®é›†ï¼ˆéœ€è¦Kaggleè´¦å·ï¼‰
kaggle competitions download -c champs-scalar-coupling

# 2. è§£å‹æ•°æ®
unzip champs-scalar-coupling.zip -d Dataset/scalar_coupling_constant/

# 3. éªŒè¯æ•°æ®å®Œæ•´æ€§
ls Dataset/scalar_coupling_constant/
# åº”è¯¥çœ‹åˆ°: train.csv, test.csv, structures.csv, scalar_coupling_contributions.csv
```

#### B. æ•°æ®è´¨é‡æ£€æŸ¥
```python
import pandas as pd

# æ£€æŸ¥æ•°æ®åŸºæœ¬ä¿¡æ¯
train_df = pd.read_csv('Dataset/scalar_coupling_constant/train.csv')
structures_df = pd.read_csv('Dataset/scalar_coupling_constant/structures.csv')

print("è®­ç»ƒæ•°æ®å½¢çŠ¶:", train_df.shape)
print("ç»“æ„æ•°æ®å½¢çŠ¶:", structures_df.shape)
print("è€¦åˆç±»å‹åˆ†å¸ƒ:\n", train_df['type'].value_counts())
print("åŸå­ç±»å‹åˆ†å¸ƒ:\n", structures_df['atom'].value_counts())

# æ£€æŸ¥ç¼ºå¤±å€¼
print("ç¼ºå¤±å€¼æ£€æŸ¥:")
print("è®­ç»ƒæ•°æ®:", train_df.isnull().sum().sum())
print("ç»“æ„æ•°æ®:", structures_df.isnull().sum().sum())

# æ£€æŸ¥æ•°æ®èŒƒå›´
print("è€¦åˆå¸¸æ•°ç»Ÿè®¡:")
print(train_df['scalar_coupling_constant'].describe())
```

#### C. æ•°æ®å­é›†é€‰æ‹©ç­–ç•¥
```python
def create_balanced_subset(train_df, max_samples=3000):
    """åˆ›å»ºå¹³è¡¡çš„æ•°æ®å­é›†"""

    # æŒ‰è€¦åˆç±»å‹åˆ†å±‚é‡‡æ ·
    type_counts = train_df['type'].value_counts()
    samples_per_type = max_samples // len(type_counts)

    subset_dfs = []
    for coupling_type in type_counts.index:
        type_data = train_df[train_df['type'] == coupling_type]
        sampled = type_data.sample(n=min(samples_per_type, len(type_data)),
                                 random_state=42)
        subset_dfs.append(sampled)

    balanced_subset = pd.concat(subset_dfs, ignore_index=True)
    return balanced_subset.sample(frac=1, random_state=42)  # æ‰“ä¹±é¡ºåº
```

### 8.3 è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

#### A. å­¦ä¹ ç‡è°ƒä¼˜
```python
# å­¦ä¹ ç‡æœç´¢ç½‘æ ¼
learning_rates = [0.0001, 0.0005, 0.001, 0.005, 0.01]

best_lr = None
best_val_loss = float('inf')

for lr in learning_rates:
    model = create_model()
    optimizer = Adam(model.parameters(), lr=lr)
    val_loss = train_and_validate(model, optimizer, epochs=10)

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_lr = lr

print(f"æœ€ä½³å­¦ä¹ ç‡: {best_lr}")
```

#### B. ç½‘ç»œæ¶æ„è°ƒä¼˜
```python
# éšè—å±‚ç»´åº¦æœç´¢
hidden_dims = [64, 128, 256, 512]
num_layers = [2, 3, 4, 5]

best_config = None
best_performance = float('inf')

for dim in hidden_dims:
    for layers in num_layers:
        config = {'hidden_dim': dim, 'num_layers': layers}
        performance = evaluate_config(config)

        if performance < best_performance:
            best_performance = performance
            best_config = config
```

#### C. æ­£åˆ™åŒ–è°ƒä¼˜
```python
# Dropoutæ¯”ä¾‹æœç´¢
dropout_rates = [0.1, 0.2, 0.3, 0.4, 0.5]
weight_decays = [1e-5, 1e-4, 1e-3, 1e-2]

# ç½‘æ ¼æœç´¢
for dropout in dropout_rates:
    for weight_decay in weight_decays:
        model = create_model(dropout=dropout)
        optimizer = Adam(model.parameters(), weight_decay=weight_decay)
        # ... è®­ç»ƒå’Œè¯„ä¼°
```

### 8.4 æ¨¡å‹éƒ¨ç½²æŒ‡å—

#### A. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
```python
# ä¿å­˜å®Œæ•´æ¨¡å‹ä¿¡æ¯
def save_model_complete(model, filepath, metadata):
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'model_class': model.__class__.__name__,
        'model_config': model.get_config(),
        'metadata': metadata,
        'timestamp': datetime.now().isoformat()
    }
    torch.save(checkpoint, filepath)

# åŠ è½½æ¨¡å‹
def load_model_complete(filepath):
    checkpoint = torch.load(filepath, map_location='cpu')

    # æ ¹æ®æ¨¡å‹ç±»é‡å»ºæ¨¡å‹
    model_class = globals()[checkpoint['model_class']]
    model = model_class(**checkpoint['model_config'])
    model.load_state_dict(checkpoint['model_state_dict'])

    return model, checkpoint['metadata']
```

#### B. æ¨ç†ä¼˜åŒ–
```python
# æ¨¡å‹é‡åŒ–
model_quantized = torch.quantization.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)

# TorchScriptå¯¼å‡º
model_scripted = torch.jit.script(model)
model_scripted.save('model_scripted.pt')

# ONNXå¯¼å‡º
torch.onnx.export(model, dummy_input, 'model.onnx')
```

#### C. æ‰¹é‡é¢„æµ‹æ¥å£
```python
class CouplingPredictor:
    def __init__(self, model_path, device='cpu'):
        self.device = device
        self.model, self.metadata = load_model_complete(model_path)
        self.model.eval()
        self.model.to(device)

    def predict_single(self, molecule_structure, atom_pair, coupling_type):
        """é¢„æµ‹å•ä¸ªåŸå­å¯¹çš„è€¦åˆå¸¸æ•°"""
        # ç‰¹å¾æå–
        features = self.extract_features(molecule_structure, atom_pair, coupling_type)

        # é¢„æµ‹
        with torch.no_grad():
            prediction = self.model(features)

        return prediction.item()

    def predict_batch(self, data_batch):
        """æ‰¹é‡é¢„æµ‹"""
        predictions = []
        with torch.no_grad():
            for batch_data in data_batch:
                pred = self.model(batch_data)
                predictions.extend(pred.cpu().numpy())
        return predictions
```

### 8.5 ç›‘æ§å’Œè°ƒè¯•

#### A. è®­ç»ƒç›‘æ§
```python
import wandb  # Weights & Biases ç›‘æ§

# åˆå§‹åŒ–ç›‘æ§
wandb.init(project="coupling-prediction", name="experiment-1")

# è®°å½•è¶…å‚æ•°
wandb.config.update({
    "learning_rate": 0.001,
    "hidden_dim": 128,
    "num_layers": 3,
    "batch_size": 32
})

# è®­ç»ƒå¾ªç¯ä¸­è®°å½•æŒ‡æ ‡
for epoch in range(num_epochs):
    train_loss = train_one_epoch()
    val_loss = validate()

    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "val_loss": val_loss,
        "learning_rate": optimizer.param_groups[0]['lr']
    })
```

#### B. è°ƒè¯•æŠ€å·§
```python
# 1. æ¢¯åº¦æ£€æŸ¥
def check_gradients(model):
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            print(f"{name}: grad_norm = {grad_norm:.6f}")
            if grad_norm > 10.0:
                print(f"WARNING: Large gradient in {name}")

# 2. æƒé‡åˆ†å¸ƒæ£€æŸ¥
def check_weights(model):
    for name, param in model.named_parameters():
        print(f"{name}: mean={param.mean():.6f}, std={param.std():.6f}")

# 3. è¾“å‡ºåˆ†å¸ƒæ£€æŸ¥
def check_predictions(predictions, targets):
    pred_mean, pred_std = predictions.mean(), predictions.std()
    targ_mean, targ_std = targets.mean(), targets.std()

    print(f"Predictions: mean={pred_mean:.4f}, std={pred_std:.4f}")
    print(f"Targets: mean={targ_mean:.4f}, std={targ_std:.4f}")
    print(f"Correlation: {np.corrcoef(predictions, targets)[0,1]:.4f}")
```

#### C. æ€§èƒ½åˆ†æ
```python
import cProfile
import torch.profiler

# CPUæ€§èƒ½åˆ†æ
def profile_training():
    profiler = cProfile.Profile()
    profiler.enable()

    # è®­ç»ƒä»£ç 
    train_one_epoch()

    profiler.disable()
    profiler.dump_stats('training_profile.prof')

# GPUæ€§èƒ½åˆ†æ
def profile_gpu():
    with torch.profiler.profile(
        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),
        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/profiler'),
        record_shapes=True,
        profile_memory=True,
        with_stack=True
    ) as prof:
        for step, batch in enumerate(train_loader):
            train_step(batch)
            prof.step()
```

### 8.6 ç»“æœéªŒè¯å’Œæµ‹è¯•

#### A. äº¤å‰éªŒè¯
```python
from sklearn.model_selection import KFold

def cross_validate(data, n_splits=5):
    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    cv_scores = []

    for fold, (train_idx, val_idx) in enumerate(kfold.split(data)):
        print(f"Fold {fold + 1}/{n_splits}")

        train_data = data[train_idx]
        val_data = data[val_idx]

        model = create_model()
        score = train_and_evaluate(model, train_data, val_data)
        cv_scores.append(score)

        print(f"Fold {fold + 1} Score: {score:.4f}")

    print(f"CV Mean: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
    return cv_scores
```

#### B. ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
```python
from scipy import stats

def statistical_test(scores_a, scores_b):
    """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹æ€§èƒ½çš„ç»Ÿè®¡æ˜¾è‘—æ€§"""

    # é…å¯¹tæ£€éªŒ
    statistic, p_value = stats.ttest_rel(scores_a, scores_b)

    print(f"é…å¯¹tæ£€éªŒç»“æœ:")
    print(f"tç»Ÿè®¡é‡: {statistic:.4f}")
    print(f"på€¼: {p_value:.4f}")

    if p_value < 0.05:
        print("ç»“æœå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ (p < 0.05)")
    else:
        print("ç»“æœä¸å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ (p >= 0.05)")

    return statistic, p_value
```

#### C. é²æ£’æ€§æµ‹è¯•
```python
def robustness_test(model, test_data, noise_levels=[0.1, 0.2, 0.5]):
    """æµ‹è¯•æ¨¡å‹å¯¹è¾“å…¥å™ªå£°çš„é²æ£’æ€§"""

    baseline_mae = evaluate(model, test_data)
    print(f"åŸºçº¿MAE: {baseline_mae:.4f}")

    for noise_level in noise_levels:
        # æ·»åŠ é«˜æ–¯å™ªå£°
        noisy_data = test_data + np.random.normal(0, noise_level, test_data.shape)
        noisy_mae = evaluate(model, noisy_data)

        degradation = (noisy_mae - baseline_mae) / baseline_mae * 100
        print(f"å™ªå£°æ°´å¹³ {noise_level}: MAE = {noisy_mae:.4f} (+{degradation:.1f}%)")
```

---

## 9. å¸¸è§é—®é¢˜å’Œä¼˜åŒ–

### 9.1 è®­ç»ƒå¸¸è§é—®é¢˜

#### A. æ”¶æ•›é—®é¢˜

**é—®é¢˜1: æŸå¤±ä¸ä¸‹é™**
```python
# å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆ
debugging_checklist = {
    "å­¦ä¹ ç‡è¿‡å¤§": "å°è¯•æ›´å°çš„å­¦ä¹ ç‡ (1e-4, 1e-5)",
    "å­¦ä¹ ç‡è¿‡å°": "å°è¯•æ›´å¤§çš„å­¦ä¹ ç‡ (1e-2, 1e-3)",
    "æ¢¯åº¦æ¶ˆå¤±": "æ£€æŸ¥ç½‘ç»œæ·±åº¦ï¼Œä½¿ç”¨æ®‹å·®è¿æ¥",
    "æ¢¯åº¦çˆ†ç‚¸": "ä½¿ç”¨æ¢¯åº¦è£å‰ª clip_grad_norm",
    "æ•°æ®æ ‡å‡†åŒ–": "ç¡®ä¿è¾“å…¥ç‰¹å¾å·²ç»æ ‡å‡†åŒ–",
    "æƒé‡åˆå§‹åŒ–": "ä½¿ç”¨Xavieræˆ–Heåˆå§‹åŒ–"
}

# å­¦ä¹ ç‡æŸ¥æ‰¾å™¨
def find_learning_rate(model, train_loader, start_lr=1e-7, end_lr=10, num_iter=100):
    lr_finder = []
    losses = []

    optimizer = Adam(model.parameters(), lr=start_lr)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=(end_lr/start_lr)**(1/num_iter))

    for i, batch in enumerate(train_loader):
        if i >= num_iter:
            break

        optimizer.zero_grad()
        loss = compute_loss(model, batch)
        loss.backward()
        optimizer.step()
        scheduler.step()

        current_lr = optimizer.param_groups[0]['lr']
        lr_finder.append(current_lr)
        losses.append(loss.item())

    # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
    plt.semilogx(lr_finder, losses)
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('Learning Rate Finder')
    plt.show()
```

**é—®é¢˜2: è¿‡æ‹Ÿåˆ**
```python
# è¿‡æ‹Ÿåˆæ£€æµ‹å’Œç¼“è§£
def detect_overfitting(train_losses, val_losses, patience=5):
    """æ£€æµ‹è¿‡æ‹Ÿåˆ"""
    if len(val_losses) < patience:
        return False

    # æ£€æŸ¥éªŒè¯æŸå¤±æ˜¯å¦æŒç»­ä¸Šå‡
    recent_val = val_losses[-patience:]
    return all(recent_val[i] >= recent_val[i-1] for i in range(1, len(recent_val)))

# è¿‡æ‹Ÿåˆç¼“è§£ç­–ç•¥
overfitting_solutions = [
    "å¢åŠ Dropoutæ¯”ä¾‹ (0.3 â†’ 0.5)",
    "å¢åŠ æƒé‡è¡°å‡ (1e-4 â†’ 1e-3)",
    "å‡å°‘æ¨¡å‹å¤æ‚åº¦ (æ›´å°‘çš„å‚æ•°)",
    "æ•°æ®å¢å¼º",
    "æ—©åœæœºåˆ¶",
    "L1/L2æ­£åˆ™åŒ–",
    "æ‰¹æ ‡å‡†åŒ–"
]
```

#### B. å†…å­˜é—®é¢˜

**é—®é¢˜: GPUå†…å­˜ä¸è¶³**
```python
# å†…å­˜ä¼˜åŒ–ç­–ç•¥
def optimize_memory():
    # 1. å‡å°‘æ‰¹å¤§å°
    batch_size = 16  # ä»32å‡å°‘åˆ°16

    # 2. æ¢¯åº¦ç´¯ç§¯
    accumulation_steps = 4
    effective_batch_size = batch_size * accumulation_steps

    optimizer.zero_grad()
    for i, batch in enumerate(train_loader):
        loss = model(batch) / accumulation_steps
        loss.backward()

        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

    # 3. æ··åˆç²¾åº¦è®­ç»ƒ
    from torch.cuda.amp import autocast, GradScaler

    scaler = GradScaler()

    for batch in train_loader:
        optimizer.zero_grad()

        with autocast():
            loss = model(batch)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

    # 4. æ£€æŸ¥ç‚¹æœºåˆ¶
    def save_checkpoint(model, optimizer, epoch, loss):
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
        }, f'checkpoint_epoch_{epoch}.pth')
```

#### C. æ•°æ®ç›¸å…³é—®é¢˜

**é—®é¢˜: æ•°æ®ä¸å¹³è¡¡**
```python
# å¤„ç†è€¦åˆç±»å‹ä¸å¹³è¡¡
def handle_imbalanced_data(train_df):
    type_counts = train_df['type'].value_counts()
    print("è€¦åˆç±»å‹åˆ†å¸ƒ:")
    print(type_counts)

    # ç­–ç•¥1: é‡é‡‡æ ·
    from sklearn.utils import resample

    balanced_dfs = []
    target_count = type_counts.median()

    for coupling_type in type_counts.index:
        type_data = train_df[train_df['type'] == coupling_type]

        if len(type_data) < target_count:
            # ä¸Šé‡‡æ ·
            upsampled = resample(type_data,
                               replace=True,
                               n_samples=int(target_count),
                               random_state=42)
            balanced_dfs.append(upsampled)
        else:
            # ä¸‹é‡‡æ ·
            downsampled = resample(type_data,
                                 replace=False,
                                 n_samples=int(target_count),
                                 random_state=42)
            balanced_dfs.append(downsampled)

    balanced_df = pd.concat(balanced_dfs, ignore_index=True)
    return balanced_df

# ç­–ç•¥2: åŠ æƒæŸå¤±å‡½æ•°
def create_weighted_loss(train_df):
    type_counts = train_df['type'].value_counts()
    total_samples = len(train_df)

    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼ˆinversely proportional to frequencyï¼‰
    weights = {}
    for coupling_type, count in type_counts.items():
        weights[coupling_type] = total_samples / (len(type_counts) * count)

    def weighted_mse_loss(predictions, targets, coupling_types):
        losses = []
        for i, coupling_type in enumerate(coupling_types):
            weight = weights[coupling_type]
            loss = weight * (predictions[i] - targets[i]) ** 2
            losses.append(loss)
        return torch.mean(torch.stack(losses))

    return weighted_mse_loss
```

### 9.2 æ¨¡å‹ç‰¹å®šä¼˜åŒ–

#### A. GNNæ¨¡å‹ä¼˜åŒ–

**å›¾é‡‡æ ·ç­–ç•¥:**
```python
# å¯¹äºå¤§åˆ†å­ï¼Œä½¿ç”¨å›¾é‡‡æ ·å‡å°‘è®¡ç®—é‡
class GraphSampler:
    def __init__(self, num_neighbors=10):
        self.num_neighbors = num_neighbors

    def sample_subgraph(self, data, target_atoms):
        """é‡‡æ ·åŒ…å«ç›®æ ‡åŸå­å¯¹çš„å­å›¾"""
        # æ‰¾åˆ°ç›®æ ‡åŸå­çš„kè·³é‚»å±…
        edge_index = data.edge_index

        # BFSæœç´¢é‚»å±…
        visited = set(target_atoms)
        queue = list(target_atoms)

        for _ in range(2):  # 2-hop neighbors
            next_queue = []
            for atom in queue:
                neighbors = edge_index[1][edge_index[0] == atom]
                for neighbor in neighbors:
                    if neighbor.item() not in visited:
                        visited.add(neighbor.item())
                        next_queue.append(neighbor.item())
            queue = next_queue

        # æ„å»ºå­å›¾
        subgraph_nodes = list(visited)
        node_mapping = {old_id: new_id for new_id, old_id in enumerate(subgraph_nodes)}

        return create_subgraph(data, subgraph_nodes, node_mapping)

# è¾¹dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
def edge_dropout(edge_index, p=0.1, training=True):
    if not training:
        return edge_index

    num_edges = edge_index.size(1)
    mask = torch.rand(num_edges) > p
    return edge_index[:, mask]
```

**æ¶ˆæ¯ä¼ é€’ä¼˜åŒ–:**
```python
# è‡ªå®šä¹‰æ¶ˆæ¯ä¼ é€’å‡½æ•°
class OptimizedMessagePassing(MessagePassing):
    def __init__(self, hidden_dim):
        super().__init__(aggr='add')
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def message(self, x_i, x_j, edge_attr=None):
        # ä¼˜åŒ–çš„æ¶ˆæ¯å‡½æ•°
        if edge_attr is not None:
            # åŒ…å«è¾¹ç‰¹å¾çš„æ¶ˆæ¯
            message = torch.cat([x_i, x_j, edge_attr], dim=-1)
        else:
            message = torch.cat([x_i, x_j], dim=-1)

        return self.mlp(message)

    def update(self, aggr_out, x):
        # æ®‹å·®è¿æ¥
        return x + aggr_out
```

#### B. ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–

**è‡ªåŠ¨ç‰¹å¾é€‰æ‹©:**
```python
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestRegressor

def automatic_feature_selection(X, y, cv=5):
    """é€’å½’ç‰¹å¾æ¶ˆé™¤ä¸äº¤å‰éªŒè¯"""

    # ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºåŸºä¼°è®¡å™¨
    estimator = RandomForestRegressor(n_estimators=100, random_state=42)

    # é€’å½’ç‰¹å¾æ¶ˆé™¤
    selector = RFECV(estimator, step=1, cv=cv, scoring='neg_mean_absolute_error')
    selector.fit(X, y)

    print(f"æœ€ä¼˜ç‰¹å¾æ•°é‡: {selector.n_features_}")
    print(f"ç‰¹å¾é€‰æ‹©å¾—åˆ†: {selector.grid_scores_}")

    # è¿”å›é€‰æ‹©çš„ç‰¹å¾
    selected_features = X[:, selector.support_]
    feature_importance = selector.ranking_

    return selected_features, feature_importance

# ç‰¹å¾äº¤äº’
def create_feature_interactions(features):
    """åˆ›å»ºç‰¹å¾äº¤äº’é¡¹"""
    n_features = features.shape[1]
    interactions = []

    # äºŒé˜¶äº¤äº’
    for i in range(n_features):
        for j in range(i+1, n_features):
            interaction = features[:, i] * features[:, j]
            interactions.append(interaction)

    # å¤šé¡¹å¼ç‰¹å¾
    squared_features = features ** 2

    # ç»„åˆæ‰€æœ‰ç‰¹å¾
    all_features = np.column_stack([
        features,                    # åŸå§‹ç‰¹å¾
        np.column_stack(interactions), # äº¤äº’ç‰¹å¾
        squared_features            # å¹³æ–¹ç‰¹å¾
    ])

    return all_features
```

#### C. è®­ç»ƒç­–ç•¥ä¼˜åŒ–

**å­¦ä¹ ç‡è°ƒåº¦ä¼˜åŒ–:**
```python
# Cosineé€€ç«è°ƒåº¦å™¨
def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return 0.5 * (1.0 + math.cos(math.pi * progress))

    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

# å¾ªç¯å­¦ä¹ ç‡
def get_cyclic_lr_schedule(optimizer, base_lr, max_lr, step_size):
    return torch.optim.lr_scheduler.CyclicLR(
        optimizer,
        base_lr=base_lr,
        max_lr=max_lr,
        step_size_up=step_size,
        mode='triangular'
    )

# è‡ªé€‚åº”å­¦ä¹ ç‡
class AdaptiveLRScheduler:
    def __init__(self, optimizer, patience=5, factor=0.5, min_lr=1e-6):
        self.optimizer = optimizer
        self.patience = patience
        self.factor = factor
        self.min_lr = min_lr
        self.best_loss = float('inf')
        self.wait = 0

    def step(self, val_loss):
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                for param_group in self.optimizer.param_groups:
                    old_lr = param_group['lr']
                    new_lr = max(old_lr * self.factor, self.min_lr)
                    param_group['lr'] = new_lr
                    print(f"å­¦ä¹ ç‡è°ƒæ•´: {old_lr:.6f} â†’ {new_lr:.6f}")
                self.wait = 0
```

### 9.3 æ€§èƒ½ç›‘æ§å’Œè¯Šæ–­

#### A. è®­ç»ƒè¿‡ç¨‹ç›‘æ§
```python
class TrainingMonitor:
    def __init__(self, log_dir='./logs'):
        self.log_dir = log_dir
        self.metrics = defaultdict(list)
        os.makedirs(log_dir, exist_ok=True)

    def log_scalar(self, tag, value, step):
        self.metrics[tag].append((step, value))

        # å®æ—¶ä¿å­˜åˆ°æ–‡ä»¶
        with open(f"{self.log_dir}/{tag}.csv", 'a') as f:
            f.write(f"{step},{value}\n")

    def log_histogram(self, tag, values, step):
        # è®°å½•æƒé‡åˆ†å¸ƒ
        plt.figure(figsize=(10, 6))
        plt.hist(values, bins=50, alpha=0.7)
        plt.title(f'{tag} - Step {step}')
        plt.savefig(f"{self.log_dir}/{tag}_step_{step}.png")
        plt.close()

    def plot_metrics(self):
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        if 'train_loss' in self.metrics and 'val_loss' in self.metrics:
            train_steps, train_losses = zip(*self.metrics['train_loss'])
            val_steps, val_losses = zip(*self.metrics['val_loss'])

            axes[0,0].plot(train_steps, train_losses, label='Train')
            axes[0,0].plot(val_steps, val_losses, label='Validation')
            axes[0,0].set_title('Loss Curves')
            axes[0,0].legend()

        # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
        if 'learning_rate' in self.metrics:
            lr_steps, lrs = zip(*self.metrics['learning_rate'])
            axes[0,1].plot(lr_steps, lrs)
            axes[0,1].set_title('Learning Rate')
            axes[0,1].set_yscale('log')

        plt.tight_layout()
        plt.savefig(f"{self.log_dir}/training_summary.png")
        plt.show()

# ä½¿ç”¨ç›‘æ§å™¨
monitor = TrainingMonitor()

for epoch in range(num_epochs):
    train_loss = train_epoch()
    val_loss = validate()
    lr = optimizer.param_groups[0]['lr']

    monitor.log_scalar('train_loss', train_loss, epoch)
    monitor.log_scalar('val_loss', val_loss, epoch)
    monitor.log_scalar('learning_rate', lr, epoch)

    # è®°å½•æƒé‡åˆ†å¸ƒ
    if epoch % 10 == 0:
        for name, param in model.named_parameters():
            monitor.log_histogram(name, param.data.cpu().numpy(), epoch)
```

#### B. æ¨¡å‹è¯Šæ–­å·¥å…·
```python
def diagnose_model(model, data_loader, device):
    """å…¨é¢çš„æ¨¡å‹è¯Šæ–­"""
    model.eval()

    diagnostics = {}

    # 1. è¾“å‡ºç»Ÿè®¡
    all_outputs = []
    with torch.no_grad():
        for batch in data_loader:
            outputs = model(batch)
            all_outputs.extend(outputs.cpu().numpy())

    all_outputs = np.array(all_outputs)
    diagnostics['output_stats'] = {
        'mean': np.mean(all_outputs),
        'std': np.std(all_outputs),
        'min': np.min(all_outputs),
        'max': np.max(all_outputs),
        'has_nan': np.isnan(all_outputs).any(),
        'has_inf': np.isinf(all_outputs).any()
    }

    # 2. æƒé‡åˆ†æ
    weight_stats = {}
    for name, param in model.named_parameters():
        weight_stats[name] = {
            'mean': param.data.mean().item(),
            'std': param.data.std().item(),
            'norm': param.data.norm().item(),
            'grad_norm': param.grad.norm().item() if param.grad is not None else 0
        }
    diagnostics['weight_stats'] = weight_stats

    # 3. æ¿€æ´»å€¼åˆ†æ
    activations = {}

    def hook_fn(name):
        def hook(module, input, output):
            activations[name] = output.detach()
        return hook

    # æ³¨å†Œé’©å­
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, (nn.ReLU, nn.ELU, nn.GELU)):
            hook = module.register_forward_hook(hook_fn(name))
            hooks.append(hook)

    # å‰å‘ä¼ æ’­æ”¶é›†æ¿€æ´»
    with torch.no_grad():
        sample_batch = next(iter(data_loader))
        model(sample_batch)

    # åˆ†ææ¿€æ´»å€¼
    activation_stats = {}
    for name, activation in activations.items():
        activation_stats[name] = {
            'mean': activation.mean().item(),
            'std': activation.std().item(),
            'zero_fraction': (activation == 0).float().mean().item()
        }
    diagnostics['activation_stats'] = activation_stats

    # æ¸…ç†é’©å­
    for hook in hooks:
        hook.remove()

    return diagnostics

# ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
def generate_diagnostic_report(diagnostics):
    print("=== æ¨¡å‹è¯Šæ–­æŠ¥å‘Š ===")

    # è¾“å‡ºç»Ÿè®¡
    output_stats = diagnostics['output_stats']
    print(f"\nè¾“å‡ºç»Ÿè®¡:")
    print(f"  å‡å€¼: {output_stats['mean']:.4f}")
    print(f"  æ ‡å‡†å·®: {output_stats['std']:.4f}")
    print(f"  èŒƒå›´: [{output_stats['min']:.4f}, {output_stats['max']:.4f}]")
    print(f"  åŒ…å«NaN: {output_stats['has_nan']}")
    print(f"  åŒ…å«Inf: {output_stats['has_inf']}")

    # æƒé‡åˆ†æ
    print(f"\næƒé‡åˆ†æ:")
    for name, stats in diagnostics['weight_stats'].items():
        print(f"  {name}:")
        print(f"    æƒé‡èŒƒæ•°: {stats['norm']:.4f}")
        print(f"    æ¢¯åº¦èŒƒæ•°: {stats['grad_norm']:.4f}")

    # æ¿€æ´»å€¼åˆ†æ
    print(f"\næ¿€æ´»å€¼åˆ†æ:")
    for name, stats in diagnostics['activation_stats'].items():
        print(f"  {name}:")
        print(f"    å‡å€¼: {stats['mean']:.4f}")
        print(f"    æ­»ç¥ç»å…ƒæ¯”ä¾‹: {stats['zero_fraction']:.2%}")
```

### 9.4 éƒ¨ç½²å’Œç”Ÿäº§ä¼˜åŒ–

#### A. æ¨¡å‹å‹ç¼©
```python
# çŸ¥è¯†è’¸é¦
class DistillationTrainer:
    def __init__(self, teacher_model, student_model, temperature=3.0, alpha=0.7):
        self.teacher = teacher_model
        self.student = student_model
        self.temperature = temperature
        self.alpha = alpha

        self.teacher.eval()

    def distillation_loss(self, student_outputs, teacher_outputs, targets):
        # è½¯ç›®æ ‡æŸå¤±
        soft_targets = F.softmax(teacher_outputs / self.temperature, dim=-1)
        soft_prob = F.log_softmax(student_outputs / self.temperature, dim=-1)
        soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean')
        soft_loss *= (self.temperature ** 2)

        # ç¡¬ç›®æ ‡æŸå¤±
        hard_loss = F.mse_loss(student_outputs, targets)

        # ç»„åˆæŸå¤±
        return self.alpha * soft_loss + (1.0 - self.alpha) * hard_loss

    def train_step(self, batch, optimizer):
        inputs, targets = batch

        # æ•™å¸ˆæ¨¡å‹é¢„æµ‹ï¼ˆä¸æ›´æ–°æ¢¯åº¦ï¼‰
        with torch.no_grad():
            teacher_outputs = self.teacher(inputs)

        # å­¦ç”Ÿæ¨¡å‹é¢„æµ‹
        student_outputs = self.student(inputs)

        # è®¡ç®—è’¸é¦æŸå¤±
        loss = self.distillation_loss(student_outputs, teacher_outputs, targets)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        return loss.item()

# æ¨¡å‹å‰ªæ
def prune_model(model, amount=0.2):
    """ç»“æ„åŒ–å‰ªæ"""
    import torch.nn.utils.prune as prune

    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            prune.l1_unstructured(module, name='weight', amount=amount)
            prune.remove(module, 'weight')

    return model

# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
def quantization_aware_training(model, train_loader, num_epochs=10):
    """é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ"""
    # å‡†å¤‡é‡åŒ–
    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
    model_prepared = torch.quantization.prepare_qat(model, inplace=False)

    # è®­ç»ƒ
    optimizer = torch.optim.Adam(model_prepared.parameters(), lr=0.0001)
    criterion = nn.MSELoss()

    for epoch in range(num_epochs):
        for batch in train_loader:
            optimizer.zero_grad()
            outputs = model_prepared(batch)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    model_quantized = torch.quantization.convert(model_prepared, inplace=False)
    return model_quantized
```

#### B. æ¨ç†åŠ é€Ÿ
```python
# æ‰¹é‡é¢„æµ‹ä¼˜åŒ–
class FastPredictor:
    def __init__(self, model, device='cuda', batch_size=64):
        self.model = model.eval()
        self.device = device
        self.batch_size = batch_size

        # é¢„ç¼–è¯‘æ¨¡å‹
        if hasattr(torch, 'compile'):
            self.model = torch.compile(self.model)

    def predict_batch(self, features):
        """ä¼˜åŒ–çš„æ‰¹é‡é¢„æµ‹"""
        predictions = []

        with torch.no_grad():
            for i in range(0, len(features), self.batch_size):
                batch = features[i:i+self.batch_size]
                batch_tensor = torch.tensor(batch, device=self.device)

                # é¢„æµ‹
                pred = self.model(batch_tensor)
                predictions.extend(pred.cpu().numpy())

        return np.array(predictions)

    def predict_single_optimized(self, features):
        """å•æ ·æœ¬é¢„æµ‹ä¼˜åŒ–"""
        # ç¼“å­˜å¸¸ç”¨è®¡ç®—
        if not hasattr(self, '_cached_tensors'):
            self._cached_tensors = {}

        # ç‰¹å¾å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
        feature_key = hash(features.tobytes())
        if feature_key in self._cached_tensors:
            return self._cached_tensors[feature_key]

        with torch.no_grad():
            tensor = torch.tensor(features, device=self.device).unsqueeze(0)
            pred = self.model(tensor).item()

            # ç¼“å­˜ç»“æœï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
            if len(self._cached_tensors) < 1000:
                self._cached_tensors[feature_key] = pred

            return pred

# æ¨¡å‹æœåŠ¡åŒ–
from flask import Flask, request, jsonify
import pickle

class CouplingPredictionService:
    def __init__(self, model_path, device='cpu'):
        self.predictor = FastPredictor(
            torch.load(model_path, map_location=device),
            device=device
        )

        # åˆ›å»ºFlaskåº”ç”¨
        self.app = Flask(__name__)
        self._register_routes()

    def _register_routes(self):
        @self.app.route('/predict', methods=['POST'])
        def predict():
            try:
                data = request.json
                features = np.array(data['features'])

                if features.ndim == 1:
                    # å•ä¸ªé¢„æµ‹
                    prediction = self.predictor.predict_single_optimized(features)
                    return jsonify({'prediction': float(prediction)})
                else:
                    # æ‰¹é‡é¢„æµ‹
                    predictions = self.predictor.predict_batch(features)
                    return jsonify({'predictions': predictions.tolist()})

            except Exception as e:
                return jsonify({'error': str(e)}), 400

        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({'status': 'healthy'})

    def run(self, host='0.0.0.0', port=5000):
        self.app.run(host=host, port=port)
```

---

## ğŸ“ æ€»ç»“ä¸å±•æœ›

### ğŸ¯ å­¦ä¹ æˆæœæ€»ç»“

é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°å­¦ä¹ äº†ï¼š

1. **åŒ–å­¦èƒŒæ™¯**: æ ‡é‡è€¦åˆå¸¸æ•°çš„ç‰©ç†æ„ä¹‰å’Œé¢„æµ‹æŒ‘æˆ˜
2. **æ•°æ®å¤„ç†**: åˆ†å­å›¾æ„å»ºã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®æ ‡å‡†åŒ–
3. **æ¨¡å‹æ¶æ„**: ä»ç®€å•MLPåˆ°å¤æ‚çš„å›¾ç¥ç»ç½‘ç»œ
4. **è®­ç»ƒæŠ€å·§**: ä¼˜åŒ–ç­–ç•¥ã€æ­£åˆ™åŒ–ã€è¶…å‚æ•°è°ƒä¼˜
5. **è¯„ä¼°æ–¹æ³•**: å¤šç§æŒ‡æ ‡ä½“ç³»å’Œç»Ÿè®¡åˆ†æ
6. **å·¥ç¨‹å®è·µ**: éƒ¨ç½²ä¼˜åŒ–ã€ç›‘æ§è°ƒè¯•ã€æ€§èƒ½è°ƒä¼˜

### ğŸ“ˆ å®é™…åº”ç”¨ä»·å€¼

**å­¦æœ¯ç ”ç©¶:**
- ä¸ºåŒ–å­¦ä¿¡æ¯å­¦æä¾›åŸºå‡†æ–¹æ³•
- æ¢ç´¢å›¾ç¥ç»ç½‘ç»œåœ¨åˆ†å­å»ºæ¨¡ä¸­çš„åº”ç”¨
- å‘å±•æ–°çš„ç‰¹å¾å·¥ç¨‹æŠ€æœ¯

**å·¥ä¸šåº”ç”¨:**
- è¯ç‰©å‘ç°ä¸­çš„åˆ†å­æ€§è´¨é¢„æµ‹
- åŒ–å­¦ååº”è·¯å¾„è§„åˆ’
- ææ–™ç§‘å­¦ä¸­çš„æ€§è´¨é¢„æµ‹
- NMRè°±å›¾è§£æçš„è¾…åŠ©å·¥å…·

**æ•™è‚²ä»·å€¼:**
- å›¾ç¥ç»ç½‘ç»œçš„å®Œæ•´å­¦ä¹ æ¡ˆä¾‹
- æœºå™¨å­¦ä¹ åœ¨ç§‘å­¦è®¡ç®—ä¸­çš„åº”ç”¨
- ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹æ¯”è¾ƒçš„æœ€ä½³å®è·µ

### ğŸš€ è¿›ä¸€æ­¥å‘å±•æ–¹å‘

**æ¨¡å‹æ”¹è¿›:**
1. **æ›´å¤æ‚çš„GNNæ¶æ„**: GraphSAINT, FastGCN, å›¾Transformerå˜ä½“
2. **å¤šä»»åŠ¡å­¦ä¹ **: åŒæ—¶é¢„æµ‹å¤šç§åˆ†å­æ€§è´¨
3. **ä¸ç¡®å®šæ€§é‡åŒ–**: è´å¶æ–¯ç¥ç»ç½‘ç»œã€é›†æˆæ–¹æ³•
4. **å…ƒå­¦ä¹ **: å¿«é€Ÿé€‚åº”æ–°çš„åˆ†å­ç±»å‹

**æ•°æ®æ‰©å±•:**
1. **æ›´å¤§è§„æ¨¡æ•°æ®**: ç™¾ä¸‡çº§åˆ†å­æ•°æ®é›†
2. **å¤šæ¨¡æ€æ•°æ®**: ç»“åˆ2Då›¾åƒã€3Dç»“æ„ã€é‡å­ç‰¹å¾
3. **ä¸»åŠ¨å­¦ä¹ **: æ™ºèƒ½é‡‡æ ·å›°éš¾æ ·æœ¬
4. **æ•°æ®å¢å¼º**: åˆ†å­å˜æ¢å’Œæ‰°åŠ¨æŠ€æœ¯

**åº”ç”¨æ‹“å±•:**
1. **å®æ—¶é¢„æµ‹ç³»ç»Ÿ**: åœ¨çº¿NMRè°±å›¾è§£æ
2. **ç§»åŠ¨ç«¯éƒ¨ç½²**: è¾¹ç¼˜è®¡ç®—ä¼˜åŒ–
3. **äº‘æœåŠ¡é›†æˆ**: å¤§è§„æ¨¡æ‰¹é‡é¢„æµ‹æœåŠ¡
4. **å¯è§†åŒ–å·¥å…·**: äº¤äº’å¼åˆ†å­å»ºæ¨¡ç•Œé¢

### ğŸ¤ å¼€æºè´¡çŒ®æœºä¼š

**ä»£ç è´¡çŒ®:**
- æ–°çš„GNNæ¶æ„å®ç°
- æ›´é«˜æ•ˆçš„ç‰¹å¾æå–ç®—æ³•
- æ¨¡å‹å‹ç¼©å’ŒåŠ é€ŸæŠ€æœ¯
- å¯è§†åŒ–å’Œè§£é‡Šå·¥å…·

**æ•°æ®è´¡çŒ®:**
- æ›´ä¸°å¯Œçš„åˆ†å­æ•°æ®é›†
- æ•°æ®è´¨é‡æ”¹è¿›å’Œæ ‡æ³¨
- åŸºå‡†æµ‹è¯•æ•°æ®é›†æ„å»º
- è·¨é¢†åŸŸæ•°æ®é›†æ•´åˆ

**æ–‡æ¡£æ”¹è¿›:**
- æ›´è¯¦ç»†çš„åŒ–å­¦èƒŒæ™¯ä»‹ç»
- ä»£ç ç¤ºä¾‹å’Œæ•™ç¨‹æ‰©å±•
- æœ€ä½³å®è·µæŒ‡å—å®Œå–„
- å¤šè¯­è¨€æ–‡æ¡£ç¿»è¯‘

### ğŸ“š æ¨èè¿›ä¸€æ­¥å­¦ä¹ èµ„æº

**å›¾ç¥ç»ç½‘ç»œ:**
- "Graph Neural Networks: A Review of Methods and Applications"
- PyTorch Geometricå®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹
- Stanford CS224W: Machine Learning with Graphs

**åŒ–å­¦ä¿¡æ¯å­¦:**
- "Introduction to Cheminformatics" by A.R. Leach
- RDKitå®˜æ–¹æ•™ç¨‹å’Œæ–‡æ¡£
- "Deep Learning for the Life Sciences" by Ramsundar et al.

**æœºå™¨å­¦ä¹ å®è·µ:**
- "Hands-On Machine Learning" by AurÃ©lien GÃ©ron
- "Deep Learning" by Ian Goodfellow
- "Pattern Recognition and Machine Learning" by Christopher Bishop

**NMRå…‰è°±å­¦:**
- "Introduction to NMR Spectroscopy" by R.J. Abraham
- "NMR Spectroscopy Explained" by Neil E. Jacobsen
- åœ¨çº¿NMRæ•°æ®åº“å’Œå·¥å…·

---

**ğŸ‰ æ­å–œä½ å®Œæˆäº†æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹çš„å®Œæ•´å­¦ä¹ æ—…ç¨‹ï¼**

è¿™ä¸ªç»Ÿä¸€æ¡†æ¶ä¸ºä½ æä¾›äº†ä»ç†è®ºåˆ°å®è·µçš„å…¨é¢æŠ€èƒ½ï¼Œæ— è®ºæ˜¯å­¦æœ¯ç ”ç©¶è¿˜æ˜¯å·¥ä¸šåº”ç”¨ï¼Œéƒ½å°†æˆä¸ºä½ çš„æœ‰åŠ›å·¥å…·ã€‚è®°ä½ï¼Œæœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªä¸æ–­å‘å±•çš„é¢†åŸŸï¼Œä¿æŒå­¦ä¹ å’Œå®è·µæ˜¯æˆé•¿çš„å…³é”®ï¼

**Happy Learning and Happy Coding! ğŸš€**
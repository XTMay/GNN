# åŸºäºå›¾ç¥ç»ç½‘ç»œ(GNN)çš„QM9åˆ†å­å±æ€§é¢„æµ‹æ•™ç¨‹

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [QM9æ•°æ®é›†ä»‹ç»](#qm9æ•°æ®é›†ä»‹ç»)
3. [ä»»åŠ¡å®šä¹‰](#ä»»åŠ¡å®šä¹‰)
4. [å›¾ç¥ç»ç½‘ç»œåŸºç¡€](#å›¾ç¥ç»ç½‘ç»œåŸºç¡€)
5. [æ•°æ®é¢„å¤„ç†æµç¨‹](#æ•°æ®é¢„å¤„ç†æµç¨‹)
6. [æ¨¡å‹æ¶æ„è¯¦è§£](#æ¨¡å‹æ¶æ„è¯¦è§£)
7. [è®­ç»ƒæµç¨‹åˆ†æ](#è®­ç»ƒæµç¨‹åˆ†æ)
8. [ä»£ç å®ç°è¯¦è§£](#ä»£ç å®ç°è¯¦è§£)
9. [ç»“æœåˆ†æä¸å¯è§†åŒ–](#ç»“æœåˆ†æä¸å¯è§†åŒ–)
10. [æ‰©å±•ä¸ä¼˜åŒ–](#æ‰©å±•ä¸ä¼˜åŒ–)

---

## ğŸ“š æ¦‚è¿°

æœ¬æ•™ç¨‹è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œ(Graph Neural Networks, GNN)é¢„æµ‹åˆ†å­å±æ€§ã€‚æˆ‘ä»¬ä»¥QM9æ•°æ®é›†ä¸ºä¾‹ï¼Œæ„å»ºä¸€ä¸ªç«¯åˆ°ç«¯çš„åˆ†å­å±æ€§é¢„æµ‹ç³»ç»Ÿï¼Œä¸“é—¨é¢„æµ‹åˆ†å­çš„HOMO-LUMOèƒ½éš™ã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£åˆ†å­æ•°æ®çš„å›¾è¡¨ç¤ºæ–¹æ³•
- æŒæ¡GNNåœ¨åˆ†å­å±æ€§é¢„æµ‹ä¸­çš„åº”ç”¨
- å­¦ä¼šå¤„ç†åŒ–å­¦æ•°æ®é›†çš„é¢„å¤„ç†æŠ€å·§
- ç†Ÿç»ƒä½¿ç”¨PyTorch Geometricè¿›è¡Œå›¾ç¥ç»ç½‘ç»œå»ºæ¨¡

---

## ğŸ§ª QM9æ•°æ®é›†ä»‹ç»

### æ•°æ®é›†æ¦‚è¿°

QM9æ˜¯ä¸€ä¸ªåŒ…å«çº¦134,000ä¸ªå°åˆ†å­çš„å¤§è§„æ¨¡é‡å­åŒ–å­¦æ•°æ®é›†ï¼Œæ¯ä¸ªåˆ†å­åŒ…å«æœ€å¤š9ä¸ªé‡åŸå­(Cã€Nã€Oã€Fï¼Œé™¤äº†æ°¢åŸå­)ã€‚

### æ•°æ®é›†ç‰¹å¾

```python
# æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
- æ ·æœ¬æ•°é‡: ~134,000ä¸ªå°åˆ†å­
- åŸå­ç±»å‹: H(æ°¢)ã€C(ç¢³)ã€N(æ°®)ã€O(æ°§)ã€F(æ°Ÿ)
- æœ€å¤§åŸå­æ•°: 29ä¸ªåŸå­
- ç›®æ ‡å±æ€§: 19ç§é‡å­åŒ–å­¦å±æ€§
```

### 19ç§ç›®æ ‡å±æ€§

| ç´¢å¼• | å±æ€§åç§° | å•ä½ | æè¿° |
|------|----------|------|------|
| 0 | dipole_moment | Debye | å¶æçŸ© |
| 1 | isotropic_polarizability | BohrÂ³ | å„å‘åŒæ€§æåŒ–ç‡ |
| 2 | homo | Hartree | HOMOè½¨é“èƒ½çº§ |
| 3 | lumo | Hartree | LUMOè½¨é“èƒ½çº§ |
| **4** | **gap** | **Hartree** | **HOMO-LUMOèƒ½éš™(æœ¬æ•™ç¨‹ç›®æ ‡)** |
| 5 | electronic_spatial_extent | BohrÂ² | ç”µå­ç©ºé—´æ‰©å±• |
| 6 | zero_point_vibrational_energy | Hartree | é›¶ç‚¹æŒ¯åŠ¨èƒ½ |
| 7 | internal_energy_0K | Hartree | 0Kå†…èƒ½ |
| 8 | internal_energy_298K | Hartree | 298Kå†…èƒ½ |
| 9 | enthalpy_298K | Hartree | 298Kç„“ |
| 10 | free_energy_298K | Hartree | 298Kè‡ªç”±èƒ½ |
| 11 | heat_capacity | cal/(molÂ·K) | çƒ­å®¹ |
| 12 | atomization_energy | Hartree | åŸå­åŒ–èƒ½ |
| 13 | atomization_enthalpy | Hartree | åŸå­åŒ–ç„“ |
| 14 | atomization_free_energy | Hartree | åŸå­åŒ–è‡ªç”±èƒ½ |
| 15 | rotational_constant_A | GHz | è½¬åŠ¨å¸¸æ•°A |
| 16 | rotational_constant_B | GHz | è½¬åŠ¨å¸¸æ•°B |
| 17 | rotational_constant_C | GHz | è½¬åŠ¨å¸¸æ•°C |
| 18 | vibrational_frequencies | cmâ»Â¹ | æŒ¯åŠ¨é¢‘ç‡ |

### HOMO-LUMOèƒ½éš™çš„é‡è¦æ€§

**HOMO-LUMOèƒ½éš™**æ˜¯åˆ†å­ä¸­æœ€é«˜å ç”¨åˆ†å­è½¨é“(HOMO)å’Œæœ€ä½æœªå ç”¨åˆ†å­è½¨é“(LUMO)ä¹‹é—´çš„èƒ½é‡å·®ï¼Œå®ƒå†³å®šäº†ï¼š

- ğŸ”¬ **åˆ†å­ç¨³å®šæ€§**: èƒ½éš™è¶Šå¤§ï¼Œåˆ†å­è¶Šç¨³å®š
- âš¡ **å¯¼ç”µæ€§**: èƒ½éš™å°çš„åˆ†å­æ›´å®¹æ˜“å¯¼ç”µ
- ğŸŒˆ **å…‰å­¦æ€§è´¨**: èƒ½éš™å†³å®šåˆ†å­çš„å¸æ”¶å…‰è°±
- ğŸ’Š **è¯ç‰©è®¾è®¡**: å½±å“åˆ†å­çš„ååº”æ´»æ€§

---

## ğŸ¯ ä»»åŠ¡å®šä¹‰

### é—®é¢˜æè¿°

ç»™å®šä¸€ä¸ªåˆ†å­çš„ç»“æ„ä¿¡æ¯ï¼Œé¢„æµ‹å…¶HOMO-LUMOèƒ½éš™å€¼ã€‚è¿™æ˜¯ä¸€ä¸ª**å›å½’ä»»åŠ¡**ã€‚

```python
# è¾“å…¥: åˆ†å­å›¾G = (V, E)
# V: åŸå­é›†åˆ (èŠ‚ç‚¹)
# E: åŒ–å­¦é”®é›†åˆ (è¾¹)
#
# è¾“å‡º: HOMO-LUMOèƒ½éš™ (æ ‡é‡å€¼)
```

### æŒ‘æˆ˜

1. **ç»“æ„å¤šæ ·æ€§**: åˆ†å­å¤§å°å’Œå½¢çŠ¶å·®å¼‚å¾ˆå¤§
2. **ç‰¹å¾å¤æ‚æ€§**: éœ€è¦æ•è·åŸå­çº§å’Œåˆ†å­çº§ç‰¹å¾
3. **é•¿ç¨‹ä¾èµ–**: åŸå­é—´çš„ç›¸äº’ä½œç”¨å¯èƒ½è·¨è¶Šå¤šä¸ªé”®
4. **æ•°æ®è§„æ¨¡**: éœ€è¦é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®

---

## ğŸ•¸ï¸ å›¾ç¥ç»ç½‘ç»œåŸºç¡€

### ä¸ºä»€ä¹ˆä½¿ç”¨å›¾è¡¨ç¤ºåˆ†å­ï¼Ÿ

åˆ†å­å¤©ç„¶å…·æœ‰å›¾ç»“æ„ï¼š

```
    H         H
    |         |
Hâ€”Câ€”Câ€”Câ€”Câ€”H  (ä¸çƒ·åˆ†å­)
|   |   |   |
H   H   H   H

èŠ‚ç‚¹(åŸå­): C, H
è¾¹(åŒ–å­¦é”®): C-C, C-H
```

### å›¾ç¥ç»ç½‘ç»œçš„ä¼˜åŠ¿

1. **æ’åˆ—ä¸å˜æ€§**: åŸå­é¡ºåºä¸å½±å“é¢„æµ‹ç»“æœ
2. **å±€éƒ¨æ€§**: åŒ–å­¦é”®çš„å±€éƒ¨ç›¸äº’ä½œç”¨
3. **å¯æ‰©å±•æ€§**: å¤„ç†ä¸åŒå¤§å°çš„åˆ†å­
4. **å¯è§£é‡Šæ€§**: å¯ä»¥åˆ†æé‡è¦çš„åŸå­å’Œé”®

### GNNçš„æ ¸å¿ƒæ€æƒ³

```python
# æ¶ˆæ¯ä¼ é€’æ¡†æ¶
for layer in range(num_layers):
    # 1. æ¶ˆæ¯è®¡ç®—
    messages = compute_messages(node_features, edge_features, edge_index)

    # 2. æ¶ˆæ¯èšåˆ
    aggregated = aggregate_messages(messages, edge_index)

    # 3. èŠ‚ç‚¹æ›´æ–°
    node_features = update_nodes(node_features, aggregated)

# 4. å›¾çº§é¢„æµ‹
graph_representation = global_pooling(node_features)
prediction = mlp(graph_representation)
```

---

## ğŸ”„ æ•°æ®é¢„å¤„ç†æµç¨‹

### 1. æ•°æ®ä¸‹è½½ä¸åŠ è½½

```python
# è‡ªåŠ¨ä¸‹è½½QM9æ•°æ®é›†
dataset = QM9(root='/path/to/dataset', transform=transform)
```

QM9æ•°æ®é›†çš„æ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
- `x`: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ [num_atoms, 11]
- `edge_index`: è¾¹ç´¢å¼• [2, num_edges]
- `edge_attr`: è¾¹ç‰¹å¾çŸ©é˜µ [num_edges, 4]
- `y`: ç›®æ ‡å±æ€§å‘é‡ [1, 19]
- `pos`: åŸå­3Dåæ ‡ [num_atoms, 3]

### 2. èŠ‚ç‚¹ç‰¹å¾è¯¦è§£

æ¯ä¸ªåŸå­(èŠ‚ç‚¹)æœ‰11ç»´ç‰¹å¾ï¼š

| ç»´åº¦ | ç‰¹å¾åç§° | æè¿° |
|------|----------|------|
| 0-4 | åŸå­ç±»å‹ | H, C, N, O, Fçš„one-hotç¼–ç  |
| 5 | åº¦æ•° | åŸå­çš„è¿æ¥æ•° |
| 6 | å½¢å¼ç”µè· | åŸå­çš„å½¢å¼ç”µè· |
| 7 | æ‰‹æ€§ | æ‰‹æ€§æ ‡ç­¾ |
| 8 | æ‚åŒ–ç±»å‹ | sp, sp2, sp3ç­‰ |
| 9 | èŠ³é¦™æ€§ | æ˜¯å¦ä¸ºèŠ³é¦™åŸå­ |
| 10 | æ°¢åŸå­æ•° | è¿æ¥çš„æ°¢åŸå­æ•°é‡ |

### 3. è¾¹ç‰¹å¾è¯¦è§£

æ¯æ¡è¾¹(åŒ–å­¦é”®)æœ‰4ç»´ç‰¹å¾ï¼š

| ç»´åº¦ | ç‰¹å¾åç§° | æè¿° |
|------|----------|------|
| 0-3 | é”®ç±»å‹ | å•é”®ã€åŒé”®ã€ä¸‰é”®ã€èŠ³é¦™é”®çš„one-hotç¼–ç  |

### 4. æ•°æ®æ ‡å‡†åŒ–

```python
def create_single_target_dataset(original_dataset, target_index):
    # æå–ç›®æ ‡å€¼
    targets = [data.y[0, target_index].item() for data in original_dataset]

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    target_mean = np.mean(targets)
    target_std = np.std(targets)

    # æ ‡å‡†åŒ–: z = (x - Î¼) / Ïƒ
    for data in original_dataset:
        target_value = (data.y[0, target_index].item() - target_mean) / target_std
        data.y = torch.tensor([target_value], dtype=torch.float)
```

**æ ‡å‡†åŒ–çš„é‡è¦æ€§**:
- åŠ é€Ÿæ”¶æ•›
- æ•°å€¼ç¨³å®šæ€§
- ä¾¿äºè®¾ç½®å­¦ä¹ ç‡

### 5. æ•°æ®é›†åˆ’åˆ†

```python
# 80% è®­ç»ƒ / 10% éªŒè¯ / 10% æµ‹è¯•
train_data = processed_data[:train_size]
val_data = processed_data[train_size:train_size + val_size]
test_data = processed_data[train_size + val_size:]
```

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„è¯¦è§£

### æ•´ä½“æ¶æ„

```
è¾“å…¥åˆ†å­å›¾
    â†“
åŸå­ç‰¹å¾åµŒå…¥
    â†“
å¤šå±‚GCNå·ç§¯
    â†“
å›¾çº§æ± åŒ–
    â†“
å…¨è¿æ¥å±‚
    â†“
HOMO-LUMOèƒ½éš™é¢„æµ‹
```

### 1. åŸå­ç‰¹å¾åµŒå…¥å±‚

```python
self.atom_embedding = nn.Sequential(
    nn.Linear(num_features, hidden_dim),     # 11 â†’ 128
    nn.BatchNorm1d(hidden_dim),              # æ‰¹å½’ä¸€åŒ–
    nn.ReLU(),                               # ReLUæ¿€æ´»
    nn.Dropout(dropout)                      # Dropoutæ­£åˆ™åŒ–
)
```

**ä½œç”¨**: å°†åŸå§‹åŸå­ç‰¹å¾æ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼Œä¾¿äºGNNå¤„ç†ã€‚

### 2. å¤šå±‚å›¾å·ç§¯ç½‘ç»œ

```python
# 4å±‚GCNå·ç§¯
self.convs = nn.ModuleList()
self.batch_norms = nn.ModuleList()

for i in range(num_layers):
    self.convs.append(GCNConv(hidden_dim, hidden_dim))
    self.batch_norms.append(nn.BatchNorm1d(hidden_dim))
```

**GCNå·ç§¯å…¬å¼**:
```
h_i^(l+1) = Ïƒ(W^(l) Â· âˆ‘_{jâˆˆN(i)âˆª{i}} h_j^(l) / âˆš(d_i Â· d_j))
```

å…¶ä¸­ï¼š
- `h_i^(l)`: ç¬¬lå±‚ä¸­èŠ‚ç‚¹içš„ç‰¹å¾
- `N(i)`: èŠ‚ç‚¹içš„é‚»å±…
- `W^(l)`: ç¬¬lå±‚çš„æƒé‡çŸ©é˜µ
- `d_i`: èŠ‚ç‚¹içš„åº¦æ•°

### 3. å›¾çº§æ± åŒ–å±‚

```python
# ä¸‰ç§æ± åŒ–æ–¹å¼çš„ç»„åˆ
x1 = global_mean_pool(x, batch)    # å¹³å‡æ± åŒ–
x2 = global_max_pool(x, batch)     # æœ€å¤§æ± åŒ–
x3 = global_add_pool(x, batch)     # æ±‚å’Œæ± åŒ–

# æ‹¼æ¥ä¸åŒæ± åŒ–ç»“æœ
x = torch.cat([x1, x2, x3], dim=1)  # [batch_size, hidden_dim*3]
```

**æ± åŒ–çš„ä½œç”¨**:
- **å¹³å‡æ± åŒ–**: æ•è·åˆ†å­çš„æ•´ä½“å¹³å‡ç‰¹æ€§
- **æœ€å¤§æ± åŒ–**: å…³æ³¨æœ€æ˜¾è‘—çš„ç‰¹å¾
- **æ±‚å’Œæ± åŒ–**: ä¿æŒç‰¹å¾çš„æ€»é‡ä¿¡æ¯

### 4. å…¨è¿æ¥é¢„æµ‹å±‚

```python
self.fc_layers = nn.Sequential(
    nn.Linear(hidden_dim * 3, hidden_dim),      # 384 â†’ 128
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(hidden_dim, hidden_dim // 2),     # 128 â†’ 64
    nn.ReLU(),
    nn.Dropout(dropout),
    nn.Linear(hidden_dim // 2, num_targets)     # 64 â†’ 1
)
```

**è®¾è®¡åŸåˆ™**:
- é€å±‚é™ç»´
- ReLUæ¿€æ´»ä¿æŒéçº¿æ€§
- Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ

---

## ğŸ“ è®­ç»ƒæµç¨‹åˆ†æ

### 1. æŸå¤±å‡½æ•°

```python
criterion = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±
loss = criterion(predictions, targets)
```

**MSEé€‚ç”¨äºå›å½’ä»»åŠ¡**:
```
MSE = (1/n) Ã— âˆ‘(y_pred - y_true)Â²
```

### 2. ä¼˜åŒ–å™¨é…ç½®

```python
optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
```

**Adamä¼˜åŒ–å™¨ä¼˜åŠ¿**:
- è‡ªé€‚åº”å­¦ä¹ ç‡
- å¤„ç†ç¨€ç–æ¢¯åº¦
- å¿«é€Ÿæ”¶æ•›

### 3. å­¦ä¹ ç‡è°ƒåº¦

```python
scheduler = ReduceLROnPlateau(optimizer, mode='min',
                             factor=0.8, patience=10)
```

**ä½œç”¨**: éªŒè¯æŸå¤±åœæ­¢ä¸‹é™æ—¶ï¼Œè‡ªåŠ¨é™ä½å­¦ä¹ ç‡ã€‚

### 4. æ—©åœæœºåˆ¶

```python
if val_loss < best_val_loss:
    best_val_loss = val_loss
    patience_counter = 0
    torch.save(model.state_dict(), 'best_model.pth')
else:
    patience_counter += 1

if patience_counter >= patience:
    print("æ—©åœè§¦å‘ï¼")
    break
```

**é˜²æ­¢è¿‡æ‹Ÿåˆ**: éªŒè¯æŸå¤±ä¸å†æ”¹å–„æ—¶åœæ­¢è®­ç»ƒã€‚

### 5. è®­ç»ƒå¾ªç¯è¯¦è§£

```python
for epoch in range(num_epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    for batch in train_loader:
        optimizer.zero_grad()
        predictions = model(batch.x, batch.edge_index, batch.batch)
        loss = criterion(predictions.squeeze(), batch.y.squeeze())
        loss.backward()
        optimizer.step()

    # éªŒè¯é˜¶æ®µ
    model.eval()
    with torch.no_grad():
        val_loss = validate(model, val_loader, criterion, device)

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)
```

---

## ğŸ’» ä»£ç å®ç°è¯¦è§£

### æ ¸å¿ƒç±»ç»“æ„

#### 1. GNNModelç±»

```python
class GNNModel(nn.Module):
    def __init__(self, num_features, hidden_dim=128, num_layers=3,
                 num_targets=1, dropout=0.2):
        super(GNNModel, self).__init__()

        # å±‚å®šä¹‰
        self.atom_embedding = ...
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        self.fc_layers = ...

    def forward(self, x, edge_index, batch):
        # å‰å‘ä¼ æ’­é€»è¾‘
        pass
```

#### 2. create_single_target_datasetå‡½æ•°

```python
def create_single_target_dataset(original_dataset, target_index):
    """
    å°†å¤šç›®æ ‡æ•°æ®é›†è½¬æ¢ä¸ºå•ç›®æ ‡æ•°æ®é›†

    Args:
        original_dataset: åŸå§‹19ç»´ç›®æ ‡æ•°æ®é›†
        target_index: ç›®æ ‡å±æ€§ç´¢å¼•(0-18)

    Returns:
        new_data_list: å•ç›®æ ‡æ•°æ®åˆ—è¡¨
        target_mean: ç›®æ ‡å‡å€¼(ç”¨äºåæ ‡å‡†åŒ–)
        target_std: ç›®æ ‡æ ‡å‡†å·®(ç”¨äºåæ ‡å‡†åŒ–)
    """
```

**å…³é”®åˆ›æ–°ç‚¹**:
- é¿å…ç»´åº¦ä¸åŒ¹é…é—®é¢˜
- æ­£ç¡®å¤„ç†æ ‡å‡†åŒ–
- ä¿æŒæ•°æ®ç»“æ„å®Œæ•´æ€§

### å‰å‘ä¼ æ’­æµç¨‹

```python
def forward(self, x, edge_index, batch):
    # æ­¥éª¤1: åŸå­ç‰¹å¾åµŒå…¥
    x = self.atom_embedding(x)  # [num_atoms, hidden_dim]

    # æ­¥éª¤2: å¤šå±‚å›¾å·ç§¯
    for i in range(self.num_layers):
        x = self.convs[i](x, edge_index)
        x = self.batch_norms[i](x)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)

    # æ­¥éª¤3: å›¾çº§æ± åŒ–
    x1 = global_mean_pool(x, batch)
    x2 = global_max_pool(x, batch)
    x3 = global_add_pool(x, batch)
    x = torch.cat([x1, x2, x3], dim=1)  # [batch_size, hidden_dim*3]

    # æ­¥éª¤4: æœ€ç»ˆé¢„æµ‹
    x = self.fc_layers(x)  # [batch_size, 1]

    return x
```

### æ‰¹å¤„ç†æœºåˆ¶

```python
# batchå‚æ•°çš„ä½œç”¨
batch = [0, 0, 0, 1, 1, 1, 1, 2, 2]
#        |--mol0--| |--mol1--| |-mol2-|

# global_mean_poolä¼šæ ¹æ®batchè‡ªåŠ¨åˆ†ç»„è®¡ç®—
# åˆ†å­0: mean(node_features[0:3])
# åˆ†å­1: mean(node_features[3:7])
# åˆ†å­2: mean(node_features[7:9])
```

---

## ğŸ“Š ç»“æœåˆ†æä¸å¯è§†åŒ–

### 1. è®­ç»ƒå†å²å¯è§†åŒ–

```python
def plot_training_history(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
    plt.plot(val_losses, label='éªŒè¯æŸå¤±', color='red')
    plt.xlabel('Epoch')
    plt.ylabel('æŸå¤±')
    plt.title('è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å˜åŒ–')
    plt.legend()
    plt.grid(True)
```

**åˆ†æè¦ç‚¹**:
- è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ âœ…
- éªŒè¯æŸå¤±ä¸‹é™åè¶‹äºç¨³å®š âœ…
- ä¸¤è€…å·®è·ä¸å¤§(æ— è¿‡æ‹Ÿåˆ) âœ…

### 2. é¢„æµ‹ç»“æœåˆ†æ

```python
def plot_predictions(true_values, predictions, target_name):
    plt.figure(figsize=(8, 8))
    plt.scatter(true_values, predictions, alpha=0.5, s=10)

    # ç†æƒ³é¢„æµ‹çº¿ (y=x)
    min_val = min(np.min(true_values), np.min(predictions))
    max_val = max(np.max(true_values), np.max(predictions))
    plt.plot([min_val, max_val], [min_val, max_val], 'r--',
             linewidth=2, label='ç†æƒ³é¢„æµ‹')

    plt.xlabel(f'çœŸå®å€¼ ({target_name})')
    plt.ylabel(f'é¢„æµ‹å€¼ ({target_name})')
    plt.title(f'{target_name} - é¢„æµ‹å€¼ vs çœŸå®å€¼')
```

### 3. è¯„ä¼°æŒ‡æ ‡

```python
# å¹³å‡ç»å¯¹è¯¯å·®
mae = mean_absolute_error(true_values, predictions)

# å‡æ–¹è¯¯å·®
mse = mean_squared_error(true_values, predictions)

# å‡æ–¹æ ¹è¯¯å·®
rmse = np.sqrt(mse)

# RÂ²å†³å®šç³»æ•°
from sklearn.metrics import r2_score
r2 = r2_score(true_values, predictions)
```

**æŒ‡æ ‡è§£é‡Š**:
- **MAE**: å¹³å‡é¢„æµ‹åå·®ï¼Œå•ä½ä¸ç›®æ ‡ç›¸åŒ
- **RMSE**: å¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿï¼Œå•ä½ä¸ç›®æ ‡ç›¸åŒ
- **RÂ²**: æ¨¡å‹è§£é‡Šæ–¹å·®çš„æ¯”ä¾‹ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½

### 4. å…¸å‹ç»“æœç¤ºä¾‹

```
æµ‹è¯•ç»“æœ (gap):
  å¹³å‡ç»å¯¹è¯¯å·® (MAE): 0.1234
  å‡æ–¹è¯¯å·® (MSE): 0.0456
  å‡æ–¹æ ¹è¯¯å·® (RMSE): 0.2136
  RÂ² å†³å®šç³»æ•°: 0.8956

é¢„æµ‹æ ·ä¾‹:
  æ ·æœ¬ 1: çœŸå®å€¼=5.6789, é¢„æµ‹å€¼=5.7123, è¯¯å·®=0.0334
  æ ·æœ¬ 2: çœŸå®å€¼=7.8901, é¢„æµ‹å€¼=7.8456, è¯¯å·®=0.0445
  ...
```

---

## ğŸš€ æ‰©å±•ä¸ä¼˜åŒ–

### 1. æ¨¡å‹æ¶æ„ä¼˜åŒ–

#### ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶

```python
from torch_geometric.nn import GATConv

# å›¾æ³¨æ„åŠ›ç½‘ç»œ
self.convs = nn.ModuleList([
    GATConv(hidden_dim, hidden_dim, heads=4, dropout=0.2)
    for _ in range(num_layers)
])
```

#### æ®‹å·®è¿æ¥

```python
def forward(self, x, edge_index, batch):
    x = self.atom_embedding(x)

    for conv, bn in zip(self.convs, self.batch_norms):
        # æ®‹å·®è¿æ¥
        residual = x
        x = conv(x, edge_index)
        x = bn(x)
        x = F.relu(x + residual)  # åŠ ä¸Šæ®‹å·®
```

#### å¤šå°ºåº¦ç‰¹å¾èåˆ

```python
# æ”¶é›†ä¸åŒå±‚çš„ç‰¹å¾
layer_outputs = [x]
for conv in self.convs:
    x = conv(x, edge_index)
    layer_outputs.append(x)

# æ‹¼æ¥å¤šå±‚ç‰¹å¾
x = torch.cat(layer_outputs, dim=1)
```

### 2. æ•°æ®å¢å¼ºç­–ç•¥

#### åˆ†å­æ„è±¡å¢å¼º

```python
# æ·»åŠ éšæœºå™ªå£°åˆ°åŸå­åæ ‡
def augment_molecule(data, noise_level=0.1):
    if hasattr(data, 'pos') and data.pos is not None:
        noise = torch.randn_like(data.pos) * noise_level
        data.pos = data.pos + noise
    return data
```

#### åŒ–å­¦ç­‰æ•ˆæ€§å¢å¼º

```python
# åˆ©ç”¨åˆ†å­å¯¹ç§°æ€§ç”Ÿæˆç­‰æ•ˆç»“æ„
def symmetric_augmentation(mol_data):
    # åº”ç”¨åˆ†å­å¯¹ç§°æ“ä½œ
    # æ—‹è½¬ã€é•œåƒç­‰å˜æ¢
    pass
```

### 3. å¤šä»»åŠ¡å­¦ä¹ 

```python
class MultiTaskGNN(nn.Module):
    def __init__(self, num_tasks=19):
        super().__init__()
        # å…±äº«çš„GNNéª¨å¹²ç½‘ç»œ
        self.backbone = GNNModel(...)

        # ä»»åŠ¡ç‰¹å®šçš„å¤´éƒ¨
        self.task_heads = nn.ModuleList([
            nn.Linear(hidden_dim, 1) for _ in range(num_tasks)
        ])

    def forward(self, x, edge_index, batch):
        # å…±äº«ç‰¹å¾æå–
        features = self.backbone.get_features(x, edge_index, batch)

        # å¤šä»»åŠ¡é¢„æµ‹
        predictions = [head(features) for head in self.task_heads]
        return torch.cat(predictions, dim=1)
```

### 4. æ¨¡å‹é›†æˆ

```python
class EnsembleGNN(nn.Module):
    def __init__(self, models):
        super().__init__()
        self.models = nn.ModuleList(models)

    def forward(self, x, edge_index, batch):
        predictions = []
        for model in self.models:
            pred = model(x, edge_index, batch)
            predictions.append(pred)

        # å¹³å‡é›†æˆ
        return torch.mean(torch.stack(predictions), dim=0)
```

### 5. è¶…å‚æ•°ä¼˜åŒ–

```python
import optuna

def objective(trial):
    # æœç´¢è¶…å‚æ•°
    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256])
    num_layers = trial.suggest_int('num_layers', 2, 6)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)

    # è®­ç»ƒæ¨¡å‹å¹¶è¿”å›éªŒè¯è¯¯å·®
    model = GNNModel(hidden_dim=hidden_dim, num_layers=num_layers,
                     dropout=dropout)
    val_error = train_and_validate(model, learning_rate)
    return val_error

# è¿è¡Œä¼˜åŒ–
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
```

---

## ğŸ”¬ é«˜çº§æŠ€å·§

### 1. æ¢¯åº¦è£å‰ª

```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 2. å­¦ä¹ ç‡é¢„çƒ­

```python
class WarmupScheduler:
    def __init__(self, optimizer, warmup_steps, base_lr):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.base_lr = base_lr
        self.step_count = 0

    def step(self):
        if self.step_count < self.warmup_steps:
            lr = self.base_lr * (self.step_count / self.warmup_steps)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
        self.step_count += 1
```

### 3. æ¨¡å‹è§£é‡Šæ€§

```python
# ä½¿ç”¨GNNExplaineråˆ†æé‡è¦ç‰¹å¾
from torch_geometric.explain import GNNExplainer

explainer = GNNExplainer(model, epochs=100)
explanation = explainer(x, edge_index, batch_index=0)

# å¯è§†åŒ–é‡è¦çš„åŸå­å’Œé”®
important_atoms = explanation.node_mask
important_bonds = explanation.edge_mask
```

### 4. ä¸ç¡®å®šæ€§é‡åŒ–

```python
# Monte Carlo Dropoutä¼°è®¡é¢„æµ‹ä¸ç¡®å®šæ€§
def predict_with_uncertainty(model, data, n_samples=100):
    model.train()  # å¯ç”¨dropout
    predictions = []

    with torch.no_grad():
        for _ in range(n_samples):
            pred = model(data.x, data.edge_index, data.batch)
            predictions.append(pred.cpu().numpy())

    predictions = np.array(predictions)
    mean_pred = np.mean(predictions, axis=0)
    uncertainty = np.std(predictions, axis=0)

    return mean_pred, uncertainty
```

---

## ğŸ“ æ€»ç»“

### å…³é”®å­¦ä¹ è¦ç‚¹

1. **å›¾è¡¨ç¤ºçš„é‡è¦æ€§**: åˆ†å­çš„å›¾ç»“æ„å¤©ç„¶é€‚åˆGNNå¤„ç†
2. **æ•°æ®é¢„å¤„ç†**: æ ‡å‡†åŒ–ã€å•ç›®æ ‡è½¬æ¢æ˜¯æˆåŠŸçš„å…³é”®
3. **æ¨¡å‹è®¾è®¡**: åµŒå…¥â†’å·ç§¯â†’æ± åŒ–â†’é¢„æµ‹çš„ç»å…¸æµç¨‹
4. **è®­ç»ƒæŠ€å·§**: æ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ã€æ­£åˆ™åŒ–é˜²è¿‡æ‹Ÿåˆ
5. **è¯„ä¼°æ–¹æ³•**: å¤šç§æŒ‡æ ‡ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½

### å®é™…åº”ç”¨åœºæ™¯

- ğŸ¥ **è¯ç‰©å‘ç°**: é¢„æµ‹åŒ–åˆç‰©çš„ADMETå±æ€§
- ğŸ”‹ **ææ–™ç§‘å­¦**: è®¾è®¡æ–°å‹ç”µæ± ææ–™
- ğŸ§ª **å‚¬åŒ–å‰‚è®¾è®¡**: é¢„æµ‹å‚¬åŒ–æ´»æ€§
- ğŸŒ± **å†œè¯å¼€å‘**: é¢„æµ‹ç”Ÿç‰©æ´»æ€§å’Œæ¯’æ€§

### è¿›ä¸€æ­¥å­¦ä¹ æ–¹å‘

1. **é«˜çº§GNNæ¶æ„**: Transformerã€Graph Transformer
2. **åŒ–å­¦ä¿¡æ¯å­¦**: RDKitã€åˆ†å­æè¿°ç¬¦
3. **é‡å­åŒ–å­¦**: DFTè®¡ç®—ã€ç”µå­ç»“æ„ç†è®º
4. **æ·±åº¦å­¦ä¹ **: æ³¨æ„åŠ›æœºåˆ¶ã€è‡ªç›‘ç£å­¦ä¹ 

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡æ¨è

1. **Quantum chemistry structures and properties of 134 kilo molecules** - QM9æ•°æ®é›†åŸå§‹è®ºæ–‡
2. **Neural Message Passing for Quantum Chemistry** - MPNNåœ¨åˆ†å­é¢„æµ‹ä¸­çš„åº”ç”¨
3. **Graph Attention Networks** - æ³¨æ„åŠ›æœºåˆ¶åœ¨å›¾ç¥ç»ç½‘ç»œä¸­çš„åº”ç”¨

### å¼€æºé¡¹ç›®

- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/): å›¾ç¥ç»ç½‘ç»œåº“
- [RDKit](https://www.rdkit.org/): åŒ–å­¦ä¿¡æ¯å­¦å·¥å…·åŒ…
- [DeepChem](https://deepchem.io/): æ·±åº¦å­¦ä¹ åŒ–å­¦å·¥å…·åŒ…

### åœ¨çº¿èµ„æº

- [Graph Neural Networks Course](https://web.stanford.edu/class/cs224w/)
- [Molecular Machine Learning](https://dmol.pub/)
- [Chemical Space Blog](https://www.chemicalspace.com/blog)

---

*æœ¬æ•™ç¨‹æ—¨åœ¨æä¾›GNNåˆ†å­å±æ€§é¢„æµ‹çš„å…¨é¢æŒ‡å—ã€‚å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿äº¤æµè®¨è®ºï¼* ğŸš€
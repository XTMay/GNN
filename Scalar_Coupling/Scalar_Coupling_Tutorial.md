# åˆ†å­æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹æ•™ç¨‹

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ ‡é‡è€¦åˆå¸¸æ•°èƒŒæ™¯](#æ ‡é‡è€¦åˆå¸¸æ•°èƒŒæ™¯)
3. [æ•°æ®é›†è¯¦ç»†ä»‹ç»](#æ•°æ®é›†è¯¦ç»†ä»‹ç»)
4. [ä»»åŠ¡å®šä¹‰ä¸æŒ‘æˆ˜](#ä»»åŠ¡å®šä¹‰ä¸æŒ‘æˆ˜)
5. [æ•°æ®é¢„å¤„ç†ç­–ç•¥](#æ•°æ®é¢„å¤„ç†ç­–ç•¥)
6. [æ¨¡å‹æ¶æ„è®¾è®¡](#æ¨¡å‹æ¶æ„è®¾è®¡)
7. [è®­ç»ƒæµç¨‹è¯¦è§£](#è®­ç»ƒæµç¨‹è¯¦è§£)
8. [ä»£ç å®ç°åˆ†æ](#ä»£ç å®ç°åˆ†æ)
9. [ç»“æœåˆ†æä¸ä¼˜åŒ–](#ç»“æœåˆ†æä¸ä¼˜åŒ–)
10. [é«˜çº§æ‰©å±•æ–¹å‘](#é«˜çº§æ‰©å±•æ–¹å‘)

---

## ğŸ“š æ¦‚è¿°

æœ¬æ•™ç¨‹è¯¦ç»†ä»‹ç»å¦‚ä½•é¢„æµ‹åˆ†å­ä¸­åŸå­å¯¹ä¹‹é—´çš„**æ ‡é‡è€¦åˆå¸¸æ•°**(Scalar Coupling Constants)ã€‚ä¸ä¼ ç»Ÿçš„åˆ†å­çº§å±æ€§é¢„æµ‹ä¸åŒï¼Œè¿™æ˜¯ä¸€ä¸ª**åŸå­å¯¹çº§åˆ«**çš„é¢„æµ‹ä»»åŠ¡ï¼Œéœ€è¦é’ˆå¯¹åˆ†å­å†…çš„æ¯ä¸€å¯¹åŸå­é¢„æµ‹å®ƒä»¬ä¹‹é—´çš„è€¦åˆå¼ºåº¦ã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£æ ‡é‡è€¦åˆå¸¸æ•°çš„ç‰©ç†æ„ä¹‰å’Œåº”ç”¨ä»·å€¼
- æŒæ¡åŸå­å¯¹ç‰¹å¾å·¥ç¨‹æŠ€æœ¯
- å­¦ä¼šå¤„ç†å¤æ‚çš„åŒ–å­¦æ•°æ®é›†ç»“æ„
- å®ç°ä»å›¾ç¥ç»ç½‘ç»œåˆ°ç®€åŒ–MLPçš„å»ºæ¨¡ç­–ç•¥
- ç†è§£åŒ–å­¦æ•°æ®ä¸­çš„ä¸åŒé¢„æµ‹ç²’åº¦

### ğŸ”¬ æ ¸å¿ƒæ¦‚å¿µ

```python
# ä¼ ç»Ÿåˆ†å­å±æ€§é¢„æµ‹ vs åŸå­å¯¹å±æ€§é¢„æµ‹
ä¼ ç»Ÿä»»åŠ¡: åˆ†å­ â†’ å•ä¸ªå±æ€§å€¼
         Hâ‚‚O â†’ æ²¸ç‚¹ = 100Â°C

æœ¬ä»»åŠ¡:   åˆ†å­ â†’ å¤šä¸ªåŸå­å¯¹å±æ€§å€¼
         Hâ‚‚O â†’ Hâ‚-Hâ‚‚ è€¦åˆå¸¸æ•° = -12.5 Hz
              Hâ‚-O è€¦åˆå¸¸æ•° = +85.2 Hz
              Hâ‚‚-O è€¦åˆå¸¸æ•° = +85.2 Hz
```

---

## ğŸ§² æ ‡é‡è€¦åˆå¸¸æ•°èƒŒæ™¯

### ä»€ä¹ˆæ˜¯æ ‡é‡è€¦åˆå¸¸æ•°ï¼Ÿ

æ ‡é‡è€¦åˆå¸¸æ•°(J-coupling)æ˜¯æ ¸ç£å…±æŒ¯(NMR)å…‰è°±å­¦ä¸­çš„é‡è¦æ¦‚å¿µï¼Œæè¿°äº†åˆ†å­ä¸­ä¸¤ä¸ªåŸå­æ ¸ä¹‹é—´é€šè¿‡åŒ–å­¦é”®ä¼ é€’çš„ç£æ€§ç›¸äº’ä½œç”¨å¼ºåº¦ã€‚

### ç‰©ç†æœºåˆ¶

```
åŸå­Aæ ¸è‡ªæ—‹ â†â†’ ç”µå­äº‘ â†â†’ åŒ–å­¦é”® â†â†’ ç”µå­äº‘ â†â†’ åŸå­Bæ ¸è‡ªæ—‹
    â†“                                            â†“
  ç£åœºå½±å“  â†--------è€¦åˆä¼ é€’--------â†’  ç£åœºæ„Ÿåº”
```

### NMRå…‰è°±ä¸­çš„è¡¨ç°

```python
# NMRä¿¡å·åˆ†è£‚æ¨¡å¼
æ— è€¦åˆ:     å•å³°    â€”â€”â€”â€”
J = 7 Hz:  åŒå³°    â€”  â€”
J = 12 Hz: åŒå³°    â€”    â€”  (åˆ†è£‚æ›´å¤§)
```

### è€¦åˆå¸¸æ•°çš„åˆ†ç±»

| è€¦åˆç±»å‹ | ç¬¦å· | æè¿° | å…¸å‹å€¼èŒƒå›´ |
|---------|------|------|-----------|
| **1JCH** | Â¹J(C,H) | C-Hç›´æ¥è€¦åˆ | 100-250 Hz |
| **2JHH** | Â²J(H,H) | è·¨è¶Š2ä¸ªé”®çš„H-Hè€¦åˆ | -20 - +5 Hz |
| **3JHH** | Â³J(H,H) | è·¨è¶Š3ä¸ªé”®çš„H-Hè€¦åˆ | 0-20 Hz |
| **1JCC** | Â¹J(C,C) | C-Cç›´æ¥è€¦åˆ | 30-70 Hz |
| **2JCH** | Â²J(C,H) | è·¨è¶Š2ä¸ªé”®çš„C-Hè€¦åˆ | -10 - +10 Hz |

### åŒ–å­¦æ„ä¹‰ä¸åº”ç”¨

1. **ç»“æ„é‰´å®š**: ç¡®å®šåˆ†å­çš„ç«‹ä½“ç»“æ„
2. **æ„è±¡åˆ†æ**: ç ”ç©¶åˆ†å­çš„ç©ºé—´æ’åˆ—
3. **åŒ–å­¦é”®åˆ†æ**: äº†è§£é”®çš„æ€§è´¨å’Œå¼ºåº¦
4. **è¯ç‰©è®¾è®¡**: ä¼˜åŒ–è¯ç‰©åˆ†å­çš„ç»“æ„
5. **ææ–™ç§‘å­¦**: è®¾è®¡å…·æœ‰ç‰¹å®šNMRæ€§è´¨çš„ææ–™

---

## ğŸ“Š æ•°æ®é›†è¯¦ç»†ä»‹ç»

æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯**åˆ†å­æ ‡é‡è€¦åˆå¸¸æ•°æ•°æ®é›†**ï¼ŒåŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

### æ•°æ®æ–‡ä»¶ç»“æ„

```
Dataset/scalar_coupling_constant/
â”œâ”€â”€ train.csv                    # è®­ç»ƒæ•°æ®ï¼šåŸå­å¯¹ + è€¦åˆå¸¸æ•°
â”œâ”€â”€ test.csv                     # æµ‹è¯•æ•°æ®ï¼šåŸå­å¯¹(æ— æ ‡ç­¾)
â”œâ”€â”€ structures.csv               # åˆ†å­ç»“æ„ï¼šåŸå­åæ ‡
â””â”€â”€ scalar_coupling_contributions.csv  # è€¦åˆè´¡çŒ®åˆ†è§£
```

### 1. train.csv - è®­ç»ƒæ•°æ®

```csv
id,molecule_name,atom_index_0,atom_index_1,type,scalar_coupling_constant
0,dsgdb9nsd_000001,1,0,1JHC,84.8076
1,dsgdb9nsd_000001,1,2,2JHH,-11.257
2,dsgdb9nsd_000001,1,3,2JHH,-11.2548
```

| å­—æ®µ | æè¿° | ç¤ºä¾‹ |
|------|------|------|
| `id` | è®°å½•å”¯ä¸€æ ‡è¯†ç¬¦ | 0, 1, 2, ... |
| `molecule_name` | åˆ†å­åç§° | dsgdb9nsd_000001 |
| `atom_index_0` | ç¬¬ä¸€ä¸ªåŸå­ç´¢å¼• | 1 |
| `atom_index_1` | ç¬¬äºŒä¸ªåŸå­ç´¢å¼• | 0 |
| `type` | è€¦åˆç±»å‹ | 1JHC, 2JHH, 3JHH |
| `scalar_coupling_constant` | **ç›®æ ‡å€¼**ï¼šè€¦åˆå¸¸æ•°(Hz) | 84.8076 |

**æ•°æ®è§„æ¨¡**: çº¦465ä¸‡ä¸ªåŸå­å¯¹

### 2. structures.csv - åˆ†å­ç»“æ„

```csv
molecule_name,atom_index,atom,x,y,z
dsgdb9nsd_000001,0,C,-0.0127,1.0858,0.0080
dsgdb9nsd_000001,1,H,0.0022,-0.0060,0.0020
dsgdb9nsd_000001,2,H,1.0117,1.4638,0.0003
```

| å­—æ®µ | æè¿° | å•ä½ |
|------|------|------|
| `molecule_name` | åˆ†å­åç§° | - |
| `atom_index` | åŸå­åœ¨åˆ†å­ä¸­çš„ç´¢å¼• | - |
| `atom` | åŸå­ç±»å‹ | H, C, N, O, F |
| `x, y, z` | 3Dåæ ‡ | Angstrom (Ã…) |

**æ•°æ®è§„æ¨¡**: çº¦236ä¸‡åŸå­åæ ‡

### 3. test.csv - æµ‹è¯•æ•°æ®

```csv
id,molecule_name,atom_index_0,atom_index_1,type
4659076,dsgdb9nsd_000004,2,0,2JHC
4659077,dsgdb9nsd_000004,2,1,1JHC
```

ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„ç»“æ„ï¼Œä½†**æ²¡æœ‰**`scalar_coupling_constant`åˆ—ã€‚

**æ•°æ®è§„æ¨¡**: çº¦251ä¸‡ä¸ªå¾…é¢„æµ‹çš„åŸå­å¯¹

### 4. scalar_coupling_contributions.csv - è€¦åˆè´¡çŒ®åˆ†è§£

```csv
molecule_name,atom_index_0,atom_index_1,type,fc,sd,pso,dso
dsgdb9nsd_000001,1,0,1JHC,83.0224,0.2546,1.2586,0.2720
```

| å­—æ®µ | æè¿° | ç‰©ç†å«ä¹‰ |
|------|------|----------|
| `fc` | Fermi Contact | è´¹ç±³æ¥è§¦ç›¸äº’ä½œç”¨ |
| `sd` | Spin-Dipolar | è‡ªæ—‹å¶æç›¸äº’ä½œç”¨ |
| `pso` | Paramagnetic Spin-Orbit | é¡ºç£æ€§è‡ªæ—‹è½¨é“è€¦åˆ |
| `dso` | Diamagnetic Spin-Orbit | é€†ç£æ€§è‡ªæ—‹è½¨é“è€¦åˆ |

**å…³ç³»**: `scalar_coupling_constant = fc + sd + pso + dso`

### æ•°æ®ç»Ÿè®¡ä¿¡æ¯

```python
# è®­ç»ƒæ•°æ®ç»Ÿè®¡
æ€»åŸå­å¯¹æ•°: 4,659,076
ç‹¬ç‰¹åˆ†å­æ•°: ~85,000
å¹³å‡æ¯åˆ†å­åŸå­å¯¹æ•°: ~55

# è€¦åˆç±»å‹åˆ†å¸ƒ
1JHC: ~40%  (C-Hç›´æ¥è€¦åˆ)
2JHH: ~25%  (H-Hé—´æ¥è€¦åˆ)
3JHH: ~20%  (H-Hé•¿ç¨‹è€¦åˆ)
1JCC: ~10%  (C-Cç›´æ¥è€¦åˆ)
2JHC: ~5%   (C-Hé—´æ¥è€¦åˆ)

# è€¦åˆå¸¸æ•°å€¼åˆ†å¸ƒ
èŒƒå›´: -100 Hz åˆ° +250 Hz
å‡å€¼: ~20 Hz
æ ‡å‡†å·®: ~45 Hz
```

---

## ğŸ¯ ä»»åŠ¡å®šä¹‰ä¸æŒ‘æˆ˜

### æ ¸å¿ƒä»»åŠ¡

ç»™å®šï¼š
- åˆ†å­ç»“æ„ä¿¡æ¯ï¼ˆåŸå­ç±»å‹å’Œ3Dåæ ‡ï¼‰
- ä¸¤ä¸ªåŸå­çš„ç´¢å¼•
- è€¦åˆç±»å‹

é¢„æµ‹ï¼šè¯¥åŸå­å¯¹çš„æ ‡é‡è€¦åˆå¸¸æ•°å€¼ï¼ˆè¿ç»­æ•°å€¼ï¼‰

```python
# æ•°å­¦è¡¨è¿°
f: (åˆ†å­ç»“æ„, åŸå­i, åŸå­j, è€¦åˆç±»å‹) â†’ è€¦åˆå¸¸æ•°å€¼
```

### ä¸»è¦æŒ‘æˆ˜

#### 1. æ•°æ®å¤æ‚æ€§

```python
# å¤šå±‚æ¬¡çš„æ•°æ®ç»“æ„
åˆ†å­çº§åˆ«:    ~85,000 ä¸ªä¸åŒåˆ†å­
åŸå­çº§åˆ«:    ~236 ä¸‡ä¸ªåŸå­
åŸå­å¯¹çº§åˆ«:  ~466 ä¸‡ä¸ªåŸå­å¯¹ â† é¢„æµ‹ç›®æ ‡
```

#### 2. ç‰¹å¾å·¥ç¨‹éš¾é¢˜

- **å‡ ä½•ç‰¹å¾**: åŸå­é—´è·ç¦»ã€è§’åº¦ã€ç«‹ä½“åŒ–å­¦
- **åŒ–å­¦ç‰¹å¾**: åŸå­ç±»å‹ã€é”®ç±»å‹ã€åˆ†å­ç¯å¢ƒ
- **ç‰©ç†ç‰¹å¾**: ç”µå­å¯†åº¦ã€è½¨é“ç›¸äº’ä½œç”¨

#### 3. æ ‡ç­¾ä¸å¹³è¡¡

```python
# è€¦åˆå¸¸æ•°å€¼åˆ†å¸ƒä¸å‡
1JHC:  å¤§å¤šæ•°åœ¨ 80-200 Hz èŒƒå›´
2JHH:  å¤§å¤šæ•°åœ¨ -20-0 Hz èŒƒå›´
3JHH:  å¤§å¤šæ•°åœ¨ 0-15 Hz èŒƒå›´
```

#### 4. é•¿ç¨‹ä¾èµ–é—®é¢˜

æŸäº›è€¦åˆå¸¸æ•°å—åˆ°**é•¿ç¨‹æ•ˆåº”**å½±å“ï¼š
- åˆ†å­æ•´ä½“æ„è±¡
- ç¯å¢ƒåŸå­çš„å½±å“
- å…±è½­æ•ˆåº”

---

## ğŸ”„ æ•°æ®é¢„å¤„ç†ç­–ç•¥

### æ•´ä½“é¢„å¤„ç†æµç¨‹

```
åŸå§‹æ•°æ® â†’ æ•°æ®æ¸…ç† â†’ ç‰¹å¾èåˆ â†’ ç‰¹å¾å·¥ç¨‹ â†’ æ•°æ®æ ‡å‡†åŒ– â†’ è®­ç»ƒå‡†å¤‡
```

### 1. æ•°æ®åŠ è½½ä¸æ¸…ç†

```python
class SimpleCouplingDataset(Dataset):
    def __init__(self, data_path, max_samples=5000):
        # åŠ è½½å¤šä¸ªCSVæ–‡ä»¶
        self.train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))
        self.structures_df = pd.read_csv(os.path.join(data_path, 'structures.csv'))

        # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        self.train_df = self.train_df.head(max_samples)
```

**è®¾è®¡è€ƒè™‘**:
- å†…å­˜ç®¡ç†: å¤§æ•°æ®é›†éœ€è¦åˆ†æ‰¹åŠ è½½
- å¿«é€ŸåŸå‹: ä½¿ç”¨å°æ ·æœ¬å¿«é€ŸéªŒè¯æ–¹æ³•

### 2. æ•°æ®èåˆç­–ç•¥

```python
# å°†ç»“æ„ä¿¡æ¯ä¸è€¦åˆæ•°æ®åˆå¹¶
# æ­¥éª¤1: è·å–åŸå­0çš„ä¿¡æ¯
atom0_info = structures_df.rename(columns={
    'atom': 'atom_0', 'x': 'x_0', 'y': 'y_0', 'z': 'z_0'
})

merged_df = train_df.merge(
    atom0_info[['molecule_name', 'atom_index', 'atom_0', 'x_0', 'y_0', 'z_0']],
    left_on=['molecule_name', 'atom_index_0'],
    right_on=['molecule_name', 'atom_index']
)

# æ­¥éª¤2: è·å–åŸå­1çš„ä¿¡æ¯
atom1_info = structures_df.rename(columns={
    'atom': 'atom_1', 'x': 'x_1', 'y': 'y_1', 'z': 'z_1'
})

merged_df = merged_df.merge(
    atom1_info[['molecule_name', 'atom_index', 'atom_1', 'x_1', 'y_1', 'z_1']],
    left_on=['molecule_name', 'atom_index_1'],
    right_on=['molecule_name', 'atom_index']
)
```

**èåˆåçš„æ•°æ®ç»“æ„**:
```csv
molecule_name,atom_index_0,atom_index_1,type,scalar_coupling_constant,
atom_0,x_0,y_0,z_0,atom_1,x_1,y_1,z_1
dsgdb9nsd_000001,1,0,1JHC,84.8076,H,0.002,-0.006,0.002,C,-0.013,1.086,0.008
```

### 3. ç‰¹å¾å·¥ç¨‹è¯¦è§£

#### 3.1 å‡ ä½•ç‰¹å¾è®¡ç®—

```python
# åŸå­é—´è·ç¦» - æœ€é‡è¦çš„å‡ ä½•ç‰¹å¾
merged_df['distance'] = np.sqrt(
    (merged_df['x_1'] - merged_df['x_0'])**2 +
    (merged_df['y_1'] - merged_df['y_0'])**2 +
    (merged_df['z_1'] - merged_df['z_0'])**2
)
```

**è·ç¦»ç‰¹å¾çš„é‡è¦æ€§**:
- ğŸ”¬ **ç‰©ç†ä¾æ®**: è€¦åˆå¼ºåº¦ä¸è·ç¦»å¯†åˆ‡ç›¸å…³
- ğŸ“Š **ç»éªŒè§„å¾‹**: J âˆ 1/rÂ³ (è·ç¦»ä¸‰æ¬¡æ–¹åæ¯”)
- ğŸ¯ **é¢„æµ‹èƒ½åŠ›**: å•ç‹¬è·ç¦»ç‰¹å¾å°±æœ‰è¾ƒå¥½çš„é¢„æµ‹æ•ˆæœ

#### 3.2 åŸå­ç±»å‹ç¼–ç 

```python
# ä½¿ç”¨LabelEncoderç¼–ç åŸå­ç±»å‹
self.type_encoder = LabelEncoder()
all_atoms = list(merged_df['atom_0']) + list(merged_df['atom_1'])
self.type_encoder.fit(all_atoms)

merged_df['atom_0_encoded'] = self.type_encoder.transform(merged_df['atom_0'])
merged_df['atom_1_encoded'] = self.type_encoder.transform(merged_df['atom_1'])
```

**ç¼–ç æ˜ å°„ç¤ºä¾‹**:
```python
H â†’ 0    # æ°¢åŸå­
C â†’ 1    # ç¢³åŸå­
N â†’ 2    # æ°®åŸå­
O â†’ 3    # æ°§åŸå­
F â†’ 4    # æ°ŸåŸå­
```

#### 3.3 æœ€ç»ˆç‰¹å¾å‘é‡

```python
feature_cols = [
    'atom_index_0',       # åŸå­0ç´¢å¼•
    'atom_index_1',       # åŸå­1ç´¢å¼•
    'atom_0_encoded',     # åŸå­0ç±»å‹ç¼–ç 
    'atom_1_encoded',     # åŸå­1ç±»å‹ç¼–ç 
    'distance',           # åŸå­é—´è·ç¦»
    'x_0', 'y_0', 'z_0', # åŸå­0åæ ‡
    'x_1', 'y_1'          # åŸå­1éƒ¨åˆ†åæ ‡
]
```

**ç‰¹å¾ç»´åº¦**: 10ç»´ç‰¹å¾å‘é‡

### 4. æ•°æ®æ ‡å‡†åŒ–

```python
# ä½¿ç”¨StandardScaleræ ‡å‡†åŒ–ç‰¹å¾
self.scaler = StandardScaler()
self.features = self.scaler.fit_transform(self.features)
```

**æ ‡å‡†åŒ–çš„é‡è¦æ€§**:
- âš–ï¸ **ç‰¹å¾å‡è¡¡**: ä¸åŒé‡çº²çš„ç‰¹å¾(è·ç¦»vsç´¢å¼•)
- ğŸƒ **åŠ é€Ÿæ”¶æ•›**: ä¼˜åŒ–ç®—æ³•æ›´ç¨³å®š
- ğŸ¯ **æ•°å€¼ç¨³å®š**: é¿å…æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±

### 5. æ•°æ®é›†åˆ’åˆ†

```python
# 80% è®­ç»ƒ / 10% éªŒè¯ / 10% æµ‹è¯•
total_size = len(dataset)
train_size = int(0.8 * total_size)
val_size = int(0.1 * total_size)
test_size = total_size - train_size - val_size
```

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„è®¾è®¡

### è®¾è®¡ç†å¿µï¼šä»å¤æ‚åˆ°ç®€å•

ä¼ ç»ŸGNNæ–¹æ³•çš„é—®é¢˜ï¼š
- ğŸ”´ **æ‰¹å¤„ç†å›°éš¾**: ä¸åŒåˆ†å­æœ‰ä¸åŒæ•°é‡çš„åŸå­å¯¹
- ğŸ”´ **å†…å­˜æ¶ˆè€—å¤§**: éœ€è¦æ„å»ºå®Œæ•´çš„åˆ†å­å›¾
- ğŸ”´ **è®­ç»ƒç¼“æ…¢**: å›¾å·ç§¯è®¡ç®—å¤æ‚

æˆ‘ä»¬çš„ç®€åŒ–æ–¹æ¡ˆï¼š
- âœ… **ç›´æ¥ç‰¹å¾æ–¹æ³•**: åŸå­å¯¹ç‰¹å¾ â†’ MLPé¢„æµ‹
- âœ… **é«˜æ•ˆæ‰¹å¤„ç†**: å›ºå®šé•¿åº¦ç‰¹å¾å‘é‡
- âœ… **å¿«é€Ÿè®­ç»ƒ**: ç®€åŒ–çš„ç½‘ç»œç»“æ„

### æ¨¡å‹æ¶æ„è¯¦è§£

```python
class SimpleCouplingGNN(nn.Module):
    def __init__(self, num_features=10, hidden_dim=128):
        super(SimpleCouplingGNN, self).__init__()

        self.feature_mlp = nn.Sequential(
            nn.Linear(num_features, hidden_dim),     # 10 â†’ 128
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim, hidden_dim // 2), # 128 â†’ 64
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim // 2, 1)           # 64 â†’ 1
        )
```

### ç½‘ç»œç»“æ„åˆ†æ

```
è¾“å…¥ç‰¹å¾ (10ç»´)
    â†“
ç¬¬ä¸€å±‚å…¨è¿æ¥ (10 â†’ 128)
    â†“
ReLUæ¿€æ´» + Dropout(0.2)
    â†“
ç¬¬äºŒå±‚å…¨è¿æ¥ (128 â†’ 64)
    â†“
ReLUæ¿€æ´» + Dropout(0.2)
    â†“
è¾“å‡ºå±‚ (64 â†’ 1)
    â†“
æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹
```

### è®¾è®¡åŸåˆ™

#### 1. æ¸è¿›é™ç»´

```python
# ç‰¹å¾ç»´åº¦å˜åŒ–: 10 â†’ 128 â†’ 64 â†’ 1
# å…ˆå‡ç»´å†é™ç»´çš„è®¾è®¡å…è®¸ç½‘ç»œå­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾è¡¨ç¤º
```

#### 2. æ¿€æ´»å‡½æ•°é€‰æ‹©

```python
# ReLUæ¿€æ´»å‡½æ•°çš„ä¼˜åŠ¿
- è®¡ç®—ç®€å•ï¼Œè®­ç»ƒå¿«é€Ÿ
- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- ç¨€ç–æ¿€æ´»ï¼Œæé«˜æ•ˆç‡
```

#### 3. æ­£åˆ™åŒ–ç­–ç•¥

```python
# Dropout(0.2) çš„ä½œç”¨
- é˜²æ­¢è¿‡æ‹Ÿåˆ
- æé«˜æ³›åŒ–èƒ½åŠ›
- æ¨¡æ‹Ÿé›†æˆå­¦ä¹ æ•ˆæœ
```

### å‰å‘ä¼ æ’­è¿‡ç¨‹

```python
def forward(self, pair_features):
    """
    Args:
        pair_features: [batch_size, 10] åŸå­å¯¹ç‰¹å¾

    Returns:
        predictions: [batch_size, 1] è€¦åˆå¸¸æ•°é¢„æµ‹
    """
    return self.feature_mlp(pair_features)
```

**è®¡ç®—å¤æ‚åº¦**:
- å‚æ•°æ•°é‡: ~11,000ä¸ªå‚æ•°
- è®¡ç®—å¤æ‚åº¦: O(batch_size Ã— feature_dim)
- å†…å­˜éœ€æ±‚: ç›¸å¯¹è¾ƒä½

---

## ğŸ“ è®­ç»ƒæµç¨‹è¯¦è§£

### 1. æŸå¤±å‡½æ•°è®¾è®¡

```python
criterion = nn.MSELoss()  # å‡æ–¹è¯¯å·®æŸå¤±
```

**MSEé€‚ç”¨æ€§åˆ†æ**:
- âœ… **å›å½’ä»»åŠ¡æ ‡å‡†**: è¿ç»­æ•°å€¼é¢„æµ‹
- âœ… **ç‰©ç†æ„ä¹‰**: å¹³æ–¹è¯¯å·®æƒ©ç½šå¤§çš„é¢„æµ‹åå·®
- âœ… **æ•°å­¦æ€§è´¨**: å‡¸å‡½æ•°ï¼Œæ˜“äºä¼˜åŒ–

### 2. ä¼˜åŒ–å™¨é…ç½®

```python
optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
```

**Adamä¼˜åŒ–å™¨ä¼˜åŠ¿**:
- ğŸš€ **è‡ªé€‚åº”å­¦ä¹ ç‡**: æ¯ä¸ªå‚æ•°ç‹¬ç«‹è°ƒæ•´
- ğŸ¯ **åŠ¨é‡æœºåˆ¶**: åŠ é€Ÿæ”¶æ•›ï¼Œè·³å‡ºå±€éƒ¨æœ€ä¼˜
- ğŸ’ª **é²æ£’æ€§å¼º**: å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ

**æƒé‡è¡°å‡ (L2æ­£åˆ™åŒ–)**:
```python
# L2æ­£åˆ™åŒ–é¡¹: weight_decay Ã— ||Î¸||Â²
# ä½œç”¨: é˜²æ­¢æƒé‡è¿‡å¤§ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›
```

### 3. å­¦ä¹ ç‡è°ƒåº¦

```python
scheduler = ReduceLROnPlateau(optimizer, mode='min',
                             factor=0.8, patience=5)
```

**è‡ªé€‚åº”å­¦ä¹ ç‡ç­–ç•¥**:
```python
# è°ƒåº¦é€»è¾‘
if val_lossæ²¡æœ‰æ”¹å–„ for 5ä¸ªepochs:
    learning_rate *= 0.8

# ä¾‹å­ï¼š
åˆå§‹å­¦ä¹ ç‡: 0.001
ç¬¬1æ¬¡è°ƒæ•´: 0.0008  (æ”¹å–„åœæ»5è½®å)
ç¬¬2æ¬¡è°ƒæ•´: 0.00064 (å†æ¬¡åœæ»5è½®å)
```

### 4. è®­ç»ƒå¾ªç¯æ ¸å¿ƒé€»è¾‘

```python
def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()  # å¯ç”¨è®­ç»ƒæ¨¡å¼(Dropoutç”Ÿæ•ˆ)
    total_loss = 0

    for features, labels in train_loader:
        # 1. æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        features = features.to(device)
        labels = labels.to(device)

        # 2. æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()

        # 3. å‰å‘ä¼ æ’­
        predictions = model(features)

        # 4. è®¡ç®—æŸå¤±
        loss = criterion(predictions, labels)

        # 5. åå‘ä¼ æ’­
        loss.backward()

        # 6. æ›´æ–°å‚æ•°
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(train_loader)
```

### 5. éªŒè¯å¾ªç¯

```python
def validate(model, val_loader, criterion, device):
    model.eval()  # å¯ç”¨è¯„ä¼°æ¨¡å¼(Dropoutå…³é—­)
    total_loss = 0

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
        for features, labels in val_loader:
            features = features.to(device)
            labels = labels.to(device)

            predictions = model(features)
            loss = criterion(predictions, labels)

            total_loss += loss.item()

    return total_loss / len(val_loader)
```

### 6. å®Œæ•´è®­ç»ƒå¾ªç¯

```python
for epoch in range(num_epochs):
    # è®­ç»ƒé˜¶æ®µ
    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)

    # éªŒè¯é˜¶æ®µ
    val_loss = validate(model, val_loader, criterion, device)

    # è®°å½•å†å²
    train_losses.append(train_loss)
    val_losses.append(val_loss)

    # å­¦ä¹ ç‡è°ƒæ•´
    scheduler.step(val_loss)

    # æ¨¡å‹ä¿å­˜
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth')

    # è¿›åº¦æ‰“å°
    if (epoch + 1) % 5 == 0:
        print(f'Epoch {epoch+1}: Train={train_loss:.6f}, Val={val_loss:.6f}')
```

### è®­ç»ƒç›‘æ§æŒ‡æ ‡

```python
# ç›‘æ§çš„å…³é”®æŒ‡æ ‡
1. è®­ç»ƒæŸå¤± (Training Loss)
2. éªŒè¯æŸå¤± (Validation Loss)
3. å­¦ä¹ ç‡å˜åŒ– (Learning Rate)
4. è®­ç»ƒæ—¶é—´ (Training Time)
```

---

## ğŸ’» ä»£ç å®ç°åˆ†æ

### 1. æ•°æ®é›†ç±»è®¾è®¡

```python
class SimpleCouplingDataset(Dataset):
    """
    è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ªPyTorchçš„Dataset

    èŒè´£ï¼š
    1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    2. ç‰¹å¾å·¥ç¨‹
    3. æä¾›æ ‡å‡†çš„__len__å’Œ__getitem__æ¥å£
    """

    def __init__(self, data_path, max_samples=5000):
        # åˆå§‹åŒ–ï¼šåŠ è½½æ•°æ®ï¼Œè¿›è¡Œé¢„å¤„ç†

    def _preprocess_data(self):
        # æ ¸å¿ƒé¢„å¤„ç†é€»è¾‘

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.features[idx]), \
               torch.FloatTensor([self.labels[idx]])
```

### 2. æ•°æ®é¢„å¤„ç†å‡½æ•°è¯¦è§£

```python
def _preprocess_data(self):
    """æ•°æ®é¢„å¤„ç†çš„å®Œæ•´æµç¨‹"""

    # æ­¥éª¤1: æ•°æ®åˆå¹¶
    # å°†åŸå­åæ ‡ä¿¡æ¯ä¸è€¦åˆæ•°æ®åˆå¹¶
    merged_df = self._merge_structure_data()

    # æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹
    # è®¡ç®—åŸå­é—´è·ç¦»
    merged_df['distance'] = self._calculate_distance(merged_df)

    # æ­¥éª¤3: ç±»åˆ«ç¼–ç 
    # åŸå­ç±»å‹è½¬æ¢ä¸ºæ•°å€¼
    merged_df = self._encode_atom_types(merged_df)

    # æ­¥éª¤4: ç‰¹å¾é€‰æ‹©
    feature_cols = ['atom_index_0', 'atom_index_1', 'atom_0_encoded',
                   'atom_1_encoded', 'distance', 'x_0', 'y_0', 'z_0',
                   'x_1', 'y_1']

    # æ­¥éª¤5: æ•°æ®è½¬æ¢
    self.features = merged_df[feature_cols].values.astype(np.float32)
    self.labels = merged_df['scalar_coupling_constant'].values.astype(np.float32)

    # æ­¥éª¤6: æ ‡å‡†åŒ–
    self.features = self.scaler.fit_transform(self.features)
```

### 3. è·ç¦»è®¡ç®—å‡½æ•°

```python
def _calculate_distance(self, df):
    """è®¡ç®—åŸå­é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»"""
    return np.sqrt(
        (df['x_1'] - df['x_0'])**2 +
        (df['y_1'] - df['y_0'])**2 +
        (df['z_1'] - df['z_0'])**2
    )
```

**å‡ ä½•æ„ä¹‰**:
```
è·ç¦» = âˆš[(xâ‚-xâ‚€)Â² + (yâ‚-yâ‚€)Â² + (zâ‚-zâ‚€)Â²]

ç‰©ç†æ„ä¹‰ï¼š
- çŸ­è·ç¦»(1-2 Ã…): åŒ–å­¦é”®è¿æ¥ï¼Œå¼ºè€¦åˆ
- ä¸­è·ç¦»(2-4 Ã…): é—´æ¥ç›¸äº’ä½œç”¨ï¼Œä¸­ç­‰è€¦åˆ
- é•¿è·ç¦»(>4 Ã…): å¼±ç›¸äº’ä½œç”¨ï¼Œå°è€¦åˆ
```

### 4. åŸå­ç±»å‹ç¼–ç 

```python
def _encode_atom_types(self, df):
    """ç¼–ç åŸå­ç±»å‹ä¸ºæ•°å€¼"""

    # æ”¶é›†æ‰€æœ‰åŸå­ç±»å‹
    all_atoms = list(df['atom_0']) + list(df['atom_1'])

    # è®­ç»ƒç¼–ç å™¨
    self.type_encoder.fit(all_atoms)

    # åº”ç”¨ç¼–ç 
    df['atom_0_encoded'] = self.type_encoder.transform(df['atom_0'])
    df['atom_1_encoded'] = self.type_encoder.transform(df['atom_1'])

    return df
```

**ç¼–ç ç­–ç•¥**:
```python
# LabelEncoderçš„ä¼˜åŠ¿
1. è‡ªåŠ¨å¤„ç†æ–°çš„åŸå­ç±»å‹
2. æ•´æ•°ç¼–ç ï¼Œå†…å­˜æ•ˆç‡é«˜
3. å¯é€†è½¬æ¢ï¼Œä¾¿äºç»“æœè§£é‡Š

# ç¼–ç ç¤ºä¾‹
åŸå§‹: ['H', 'C', 'N', 'O', 'H', 'C']
ç¼–ç : [ 0,   1,   2,   3,   0,   1 ]
```

### 5. è®­ç»ƒå‡½æ•°è®¾è®¡

```python
def train_epoch(model, train_loader, optimizer, criterion, device):
    """å•ä¸ªè®­ç»ƒå‘¨æœŸçš„å®ç°"""

    model.train()  # é‡è¦ï¼šè®¾ç½®è®­ç»ƒæ¨¡å¼
    total_loss = 0

    for batch_idx, (features, labels) in enumerate(train_loader):
        # æ•°æ®å‡†å¤‡
        features, labels = features.to(device), labels.to(device)

        # å‰å‘+åå‘ä¼ æ’­
        optimizer.zero_grad()
        predictions = model(features)
        loss = criterion(predictions, labels)
        loss.backward()
        optimizer.step()

        # æŸå¤±ç´¯è®¡
        total_loss += loss.item()

        # å¯é€‰ï¼šæ‰¹æ¬¡çº§åˆ«çš„æ—¥å¿—
        if batch_idx % 100 == 0:
            print(f'Batch {batch_idx}: Loss = {loss.item():.6f}')

    return total_loss / len(train_loader)
```

### 6. æ¨¡å‹è¯„ä¼°å‡½æ•°

```python
def test_model(model, test_loader, device):
    """æ¨¡å‹æµ‹è¯•å’Œæ€§èƒ½è¯„ä¼°"""

    model.eval()  # è¯„ä¼°æ¨¡å¼
    all_predictions = []
    all_labels = []

    with torch.no_grad():
        for features, labels in test_loader:
            features = features.to(device)
            labels = labels.to(device)

            # é¢„æµ‹
            predictions = model(features)

            # æ”¶é›†ç»“æœ
            all_predictions.extend(predictions.cpu().numpy().flatten())
            all_labels.extend(labels.cpu().numpy().flatten())

    return np.array(all_predictions), np.array(all_labels)
```

---

## ğŸ“Š ç»“æœåˆ†æä¸ä¼˜åŒ–

### 1. è¯„ä¼°æŒ‡æ ‡ä½“ç³»

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def comprehensive_evaluation(true_values, predictions):
    """å…¨é¢çš„æ¨¡å‹è¯„ä¼°"""

    # åŸºç¡€å›å½’æŒ‡æ ‡
    mae = mean_absolute_error(true_values, predictions)
    mse = mean_squared_error(true_values, predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(true_values, predictions)

    # åŒ–å­¦ç‰¹å®šæŒ‡æ ‡
    relative_error = np.mean(np.abs((predictions - true_values) / true_values))

    return {
        'MAE': mae,      # å¹³å‡ç»å¯¹è¯¯å·®
        'MSE': mse,      # å‡æ–¹è¯¯å·®
        'RMSE': rmse,    # å‡æ–¹æ ¹è¯¯å·®
        'RÂ²': r2,        # å†³å®šç³»æ•°
        'Relative Error': relative_error  # ç›¸å¯¹è¯¯å·®
    }
```

### 2. æŒ‡æ ‡è§£é‡Šä¸åŒ–å­¦æ„ä¹‰

| æŒ‡æ ‡ | æ•°å­¦å®šä¹‰ | åŒ–å­¦è§£é‡Š | ç›®æ ‡å€¼ |
|------|----------|----------|--------|
| **MAE** | `Î£|y_pred - y_true|/n` | å¹³å‡é¢„æµ‹åå·®(Hz) | < 5 Hz |
| **RMSE** | `âˆš(Î£(y_pred - y_true)Â²/n)` | å¯¹å¤§è¯¯å·®æ›´æ•æ„Ÿ(Hz) | < 8 Hz |
| **RÂ²** | `1 - SS_res/SS_tot` | æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ | > 0.9 |

### 3. å…¸å‹ç»“æœç¤ºä¾‹

```python
# åœ¨2000æ ·æœ¬çš„æµ‹è¯•ä¸­ï¼Œå…¸å‹ç»“æœï¼š
æµ‹è¯•ç»“æœ:
  å¹³å‡ç»å¯¹è¯¯å·® (MAE): 12.34 Hz
  å‡æ–¹è¯¯å·® (MSE): 456.78
  å‡æ–¹æ ¹è¯¯å·® (RMSE): 21.37 Hz
  RÂ² å†³å®šç³»æ•°: 0.7856
  ç›¸å¯¹è¯¯å·®: 15.2%

é¢„æµ‹æ ·ä¾‹:
  æ ·æœ¬ 1: çœŸå®å€¼=84.81 Hz, é¢„æµ‹å€¼=82.45 Hz, è¯¯å·®=2.36 Hz
  æ ·æœ¬ 2: çœŸå®å€¼=-11.26 Hz, é¢„æµ‹å€¼=-13.45 Hz, è¯¯å·®=2.19 Hz
  æ ·æœ¬ 3: çœŸå®å€¼=156.78 Hz, é¢„æµ‹å€¼=162.34 Hz, è¯¯å·®=5.56 Hz
```

### 4. å¯è§†åŒ–åˆ†æ

#### 4.1 è®­ç»ƒå†å²æ›²çº¿

```python
def plot_training_curves(train_losses, val_losses):
    """ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿"""

    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='è®­ç»ƒæŸå¤±', linewidth=2)
    plt.plot(val_losses, label='éªŒè¯æŸå¤±', linewidth=2)

    plt.xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)')
    plt.ylabel('æŸå¤±å€¼ (MSE)')
    plt.title('æ¨¡å‹è®­ç»ƒè¿‡ç¨‹')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # æ·»åŠ æœ€ä¼˜ç‚¹æ ‡è®°
    best_epoch = np.argmin(val_losses)
    plt.axvline(x=best_epoch, color='red', linestyle='--', alpha=0.7,
                label=f'æœ€ä½³æ¨¡å‹ (Epoch {best_epoch})')
```

**ç†æƒ³çš„è®­ç»ƒæ›²çº¿ç‰¹å¾**:
- âœ… è®­ç»ƒæŸå¤±å•è°ƒä¸‹é™
- âœ… éªŒè¯æŸå¤±å…ˆé™åç¨³å®š
- âœ… ä¸¤æ›²çº¿æ¥è¿‘(æ— è¿‡æ‹Ÿåˆ)
- âŒ éªŒè¯æŸå¤±ä¸Šå‡(è¿‡æ‹Ÿåˆè­¦å‘Š)

#### 4.2 é¢„æµ‹æ•£ç‚¹å›¾

```python
def plot_prediction_scatter(true_values, predictions):
    """é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾"""

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # æ•£ç‚¹å›¾
    ax1.scatter(true_values, predictions, alpha=0.6, s=30)

    # ç†æƒ³é¢„æµ‹çº¿ y=x
    min_val = min(np.min(true_values), np.min(predictions))
    max_val = max(np.max(true_values), np.max(predictions))
    ax1.plot([min_val, max_val], [min_val, max_val], 'r--',
             linewidth=2, label='ç†æƒ³é¢„æµ‹')

    ax1.set_xlabel('çœŸå®è€¦åˆå¸¸æ•° (Hz)')
    ax1.set_ylabel('é¢„æµ‹è€¦åˆå¸¸æ•° (Hz)')
    ax1.set_title('é¢„æµ‹å‡†ç¡®æ€§åˆ†æ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
    errors = predictions - true_values
    ax2.hist(errors, bins=50, alpha=0.7, edgecolor='black')
    ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)
    ax2.set_xlabel('é¢„æµ‹è¯¯å·® (Hz)')
    ax2.set_ylabel('é¢‘æ¬¡')
    ax2.set_title('è¯¯å·®åˆ†å¸ƒ')
    ax2.grid(True, alpha=0.3)
```

### 5. è¯¯å·®åˆ†æ

#### 5.1 æŒ‰è€¦åˆç±»å‹åˆ†æ

```python
def analyze_by_coupling_type(results_df):
    """æŒ‰ç…§ä¸åŒè€¦åˆç±»å‹åˆ†ææ€§èƒ½"""

    type_analysis = {}
    for coupling_type in ['1JHC', '2JHH', '3JHH', '1JCC', '2JHC']:
        mask = results_df['type'] == coupling_type
        if mask.sum() > 0:
            subset = results_df[mask]
            type_analysis[coupling_type] = {
                'count': len(subset),
                'mae': mean_absolute_error(subset['true'], subset['pred']),
                'mean_true': subset['true'].mean(),
                'std_true': subset['true'].std()
            }

    return type_analysis
```

**å…¸å‹åˆ†æç»“æœ**:
```python
1JHC (C-Hç›´æ¥è€¦åˆ):
  æ ·æœ¬æ•°: 800, MAE: 8.5 Hz, å‡å€¼: 156.2 Hz
  åˆ†æ: é¢„æµ‹ç›¸å¯¹å‡†ç¡®ï¼Œè¯¯å·®ä¸»è¦æ¥è‡ªæå€¼æ ·æœ¬

2JHH (H-Hé—´æ¥è€¦åˆ):
  æ ·æœ¬æ•°: 500, MAE: 3.2 Hz, å‡å€¼: -11.4 Hz
  åˆ†æ: é¢„æµ‹ç²¾åº¦æœ€é«˜ï¼Œæ•°æ®åˆ†å¸ƒé›†ä¸­

3JHH (H-Hé•¿ç¨‹è€¦åˆ):
  æ ·æœ¬æ•°: 400, MAE: 2.8 Hz, å‡å€¼: 8.7 Hz
  åˆ†æ: å°æ•°å€¼é¢„æµ‹ï¼Œç»å¯¹è¯¯å·®å°ä½†ç›¸å¯¹è¯¯å·®è¾ƒå¤§
```

#### 5.2 æŒ‰åˆ†å­å¤§å°åˆ†æ

```python
def analyze_by_molecule_size(results_df, structures_df):
    """æŒ‰åˆ†å­å¤§å°åˆ†æé¢„æµ‹æ€§èƒ½"""

    # è®¡ç®—æ¯ä¸ªåˆ†å­çš„åŸå­æ•°
    molecule_sizes = structures_df.groupby('molecule_name').size()

    # åˆ†ç±»ï¼šå°åˆ†å­(<10åŸå­)ï¼Œä¸­åˆ†å­(10-20)ï¼Œå¤§åˆ†å­(>20)
    size_bins = [0, 10, 20, 50]
    size_labels = ['å°åˆ†å­', 'ä¸­åˆ†å­', 'å¤§åˆ†å­']

    for i, label in enumerate(size_labels):
        mask = (molecule_sizes >= size_bins[i]) & (molecule_sizes < size_bins[i+1])
        relevant_molecules = molecule_sizes[mask].index
        subset = results_df[results_df['molecule_name'].isin(relevant_molecules)]

        if len(subset) > 0:
            print(f"{label}: MAE={subset_mae:.2f} Hz, æ ·æœ¬æ•°={len(subset)}")
```

---

## ğŸš€ é«˜çº§æ‰©å±•æ–¹å‘

### 1. ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–

#### 1.1 é«˜çº§å‡ ä½•ç‰¹å¾

```python
def advanced_geometric_features(structures_df, pair_data):
    """è®¡ç®—é«˜çº§å‡ ä½•ç‰¹å¾"""

    features = {}

    # é”®è§’ç‰¹å¾
    features['bond_angle'] = calculate_bond_angles(structures_df, pair_data)

    # äºŒé¢è§’ç‰¹å¾
    features['dihedral_angle'] = calculate_dihedral_angles(structures_df, pair_data)

    # åˆ†å­ä½“ç§¯
    features['molecular_volume'] = calculate_molecular_volume(structures_df)

    # åŸå­æ¥è§¦è¡¨é¢ç§¯
    features['contact_area'] = calculate_contact_surface_area(structures_df, pair_data)

    return features

def calculate_bond_angles(structures, pairs):
    """è®¡ç®—æ¶‰åŠåŸå­å¯¹çš„é”®è§’"""
    # å¯¹äºæ¯ä¸ªåŸå­å¯¹(A,B)ï¼Œæ‰¾åˆ°è¿æ¥åŸå­C
    # è®¡ç®—è§’åº¦ âˆ CAB å’Œ âˆ CBA
    pass

def calculate_dihedral_angles(structures, pairs):
    """è®¡ç®—äºŒé¢è§’ - é‡è¦çš„ç«‹ä½“åŒ–å­¦ä¿¡æ¯"""
    # å¯¹äºåŸå­å¯¹(A,B)å’Œå®ƒä»¬çš„é‚»å±…(C,D)
    # è®¡ç®—äºŒé¢è§’ C-A-B-D
    pass
```

#### 1.2 åŒ–å­¦ç¯å¢ƒç‰¹å¾

```python
def chemical_environment_features(structures_df, pair_data):
    """æå–åŒ–å­¦ç¯å¢ƒç‰¹å¾"""

    features = {}

    # åŸå­çš„åŒ–å­¦ç¯å¢ƒ
    features['coordination_number'] = get_coordination_numbers(structures_df)

    # èŠ³é¦™æ€§æ£€æµ‹
    features['aromaticity'] = detect_aromatic_rings(structures_df)

    # ç”µè´Ÿæ€§å·®å¼‚
    features['electronegativity_diff'] = calculate_electronegativity_diff(pair_data)

    # å½¢å¼ç”µè·
    features['formal_charges'] = assign_formal_charges(structures_df)

    return features
```

#### 1.3 é‡å­åŒ–å­¦æè¿°ç¬¦

```python
# ä½¿ç”¨RDKitè®¡ç®—åˆ†å­æè¿°ç¬¦
from rdkit import Chem
from rdkit.Chem import Descriptors, rdMolDescriptors

def quantum_chemical_features(molecule_smiles):
    """è®¡ç®—é‡å­åŒ–å­¦æè¿°ç¬¦"""

    mol = Chem.MolFromSmiles(molecule_smiles)

    features = {
        'molecular_weight': Descriptors.MolWt(mol),
        'logp': Descriptors.MolLogP(mol),  # è„‚æ°´åˆ†é…ç³»æ•°
        'tpsa': rdMolDescriptors.CalcTPSA(mol),  # æ‹“æ‰‘ææ€§è¡¨é¢ç§¯
        'num_rotatable_bonds': Descriptors.NumRotatableBonds(mol),
        'num_aromatic_rings': Descriptors.NumAromaticRings(mol),
        'balaban_j': Descriptors.BalabanJ(mol)  # BalabanæŒ‡æ•°
    }

    return features
```

### 2. æ·±åº¦å­¦ä¹ æ¶æ„ä¼˜åŒ–

#### 2.1 æ³¨æ„åŠ›æœºåˆ¶

```python
class AttentionCouplingNet(nn.Module):
    """å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„è€¦åˆå¸¸æ•°é¢„æµ‹æ¨¡å‹"""

    def __init__(self, feature_dim, hidden_dim):
        super().__init__()

        # ç‰¹å¾ç¼–ç å™¨
        self.feature_encoder = nn.Linear(feature_dim, hidden_dim)

        # è‡ªæ³¨æ„åŠ›å±‚
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=8)

        # é¢„æµ‹å¤´
        self.predictor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

    def forward(self, features):
        # ç‰¹å¾ç¼–ç 
        encoded = self.feature_encoder(features)  # [batch, hidden]

        # è‡ªæ³¨æ„åŠ› (éœ€è¦è°ƒæ•´ç»´åº¦)
        encoded = encoded.unsqueeze(0)  # [1, batch, hidden]
        attended, _ = self.attention(encoded, encoded, encoded)
        attended = attended.squeeze(0)  # [batch, hidden]

        # é¢„æµ‹
        prediction = self.predictor(attended)
        return prediction
```

#### 2.2 æ®‹å·®ç½‘ç»œ

```python
class ResidualCouplingNet(nn.Module):
    """å¸¦æ®‹å·®è¿æ¥çš„æ·±åº¦ç½‘ç»œ"""

    def __init__(self, feature_dim, hidden_dim, num_blocks=3):
        super().__init__()

        self.input_layer = nn.Linear(feature_dim, hidden_dim)

        # æ®‹å·®å—
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(hidden_dim) for _ in range(num_blocks)
        ])

        self.output_layer = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.input_layer(x)

        for block in self.residual_blocks:
            x = block(x) + x  # æ®‹å·®è¿æ¥

        return self.output_layer(x)

class ResidualBlock(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, x):
        return self.layers(x)
```

### 3. é›†æˆå­¦ä¹ æ–¹æ³•

#### 3.1 æ¨¡å‹é›†æˆ

```python
class EnsembleCouplingPredictor:
    """é›†æˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹å™¨"""

    def __init__(self, model_configs):
        self.models = []
        for config in model_configs:
            model = SimpleCouplingGNN(**config)
            self.models.append(model)

    def train_ensemble(self, train_data, val_data):
        """è®­ç»ƒé›†æˆä¸­çš„æ¯ä¸ªæ¨¡å‹"""
        for i, model in enumerate(self.models):
            print(f"è®­ç»ƒæ¨¡å‹ {i+1}/{len(self.models)}")

            # å¯ä»¥ä½¿ç”¨ä¸åŒçš„è®­ç»ƒç­–ç•¥
            # 1. ä¸åŒçš„éšæœºç§å­
            # 2. ä¸åŒçš„æ•°æ®å­é›†
            # 3. ä¸åŒçš„è¶…å‚æ•°
            self._train_single_model(model, train_data, val_data)

    def predict(self, features):
        """é›†æˆé¢„æµ‹"""
        predictions = []

        for model in self.models:
            pred = model(features)
            predictions.append(pred)

        # å¹³å‡é›†æˆ
        ensemble_pred = torch.mean(torch.stack(predictions), dim=0)

        # ä¹Ÿå¯ä»¥åŠ æƒé›†æˆ
        # weights = [0.3, 0.4, 0.3]  # åŸºäºéªŒè¯æ€§èƒ½è®¾å®šæƒé‡
        # weighted_pred = sum(w * pred for w, pred in zip(weights, predictions))

        return ensemble_pred
```

#### 3.2 Stackingé›†æˆ

```python
class StackingEnsemble(nn.Module):
    """Stackingé›†æˆå­¦ä¹ """

    def __init__(self, base_models, meta_model):
        super().__init__()
        self.base_models = nn.ModuleList(base_models)
        self.meta_model = meta_model

    def forward(self, features):
        # ç¬¬ä¸€å±‚ï¼šåŸºæ¨¡å‹é¢„æµ‹
        base_predictions = []
        for model in self.base_models:
            pred = model(features)
            base_predictions.append(pred)

        # ç¬¬äºŒå±‚ï¼šå…ƒæ¨¡å‹ç»„åˆåŸºæ¨¡å‹é¢„æµ‹
        stacked_features = torch.cat(base_predictions, dim=1)
        final_prediction = self.meta_model(stacked_features)

        return final_prediction
```

### 4. ä¸ç¡®å®šæ€§é‡åŒ–

#### 4.1 Monte Carlo Dropout

```python
def predict_with_uncertainty(model, features, n_samples=100):
    """ä½¿ç”¨MC Dropoutä¼°è®¡é¢„æµ‹ä¸ç¡®å®šæ€§"""

    model.train()  # ä¿æŒè®­ç»ƒæ¨¡å¼ä»¥å¯ç”¨dropout
    predictions = []

    with torch.no_grad():
        for _ in range(n_samples):
            pred = model(features)
            predictions.append(pred.cpu().numpy())

    predictions = np.array(predictions)

    # è®¡ç®—ç»Ÿè®¡é‡
    mean_pred = np.mean(predictions, axis=0)
    std_pred = np.std(predictions, axis=0)

    # ç½®ä¿¡åŒºé—´
    conf_95_lower = np.percentile(predictions, 2.5, axis=0)
    conf_95_upper = np.percentile(predictions, 97.5, axis=0)

    return {
        'prediction': mean_pred,
        'uncertainty': std_pred,
        'confidence_interval': (conf_95_lower, conf_95_upper)
    }
```

#### 4.2 æ·±åº¦é›†æˆ

```python
class DeepEnsemble:
    """æ·±åº¦é›†æˆ - è®­ç»ƒå¤šä¸ªç‹¬ç«‹çš„ç¥ç»ç½‘ç»œ"""

    def __init__(self, model_class, num_models=5):
        self.models = []
        for i in range(num_models):
            # æ¯ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–
            torch.manual_seed(i * 42)
            model = model_class()
            self.models.append(model)

    def predict_with_uncertainty(self, features):
        predictions = []

        for model in self.models:
            model.eval()
            with torch.no_grad():
                pred = model(features)
                predictions.append(pred.cpu().numpy())

        predictions = np.array(predictions)

        return {
            'mean': np.mean(predictions, axis=0),
            'std': np.std(predictions, axis=0),
            'predictions': predictions  # æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        }
```

### 5. æ¨¡å‹è§£é‡Šæ€§

#### 5.1 SHAPå€¼åˆ†æ

```python
import shap

def explain_predictions(model, features, feature_names):
    """ä½¿ç”¨SHAPè§£é‡Šæ¨¡å‹é¢„æµ‹"""

    # åˆ›å»ºSHAPè§£é‡Šå™¨
    explainer = shap.Explainer(model, features[:100])  # ä½¿ç”¨èƒŒæ™¯æ ·æœ¬

    # è®¡ç®—SHAPå€¼
    shap_values = explainer(features[:20])  # è§£é‡Šå‰20ä¸ªæ ·æœ¬

    # å¯è§†åŒ–
    shap.plots.waterfall(shap_values[0])  # å•ä¸ªæ ·æœ¬çš„è§£é‡Š
    shap.plots.beeswarm(shap_values)      # æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾é‡è¦æ€§

    # ç‰¹å¾é‡è¦æ€§æ’åº
    feature_importance = np.abs(shap_values.values).mean(0)
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)

    return importance_df
```

#### 5.2 æ¢¯åº¦åˆ†æ

```python
def analyze_feature_gradients(model, features, target_idx=0):
    """åˆ†æç‰¹å¾å¯¹é¢„æµ‹çš„æ¢¯åº¦è´¡çŒ®"""

    features.requires_grad_(True)

    # å‰å‘ä¼ æ’­
    predictions = model(features)

    # è®¡ç®—æ¢¯åº¦
    grad = torch.autograd.grad(
        outputs=predictions[target_idx],
        inputs=features,
        create_graph=True
    )[0]

    # ç‰¹å¾é‡è¦æ€§ = æ¢¯åº¦ Ã— è¾“å…¥å€¼
    feature_importance = (grad * features).abs().mean(dim=0)

    return feature_importance.detach().numpy()
```

### 6. è¶…å‚æ•°ä¼˜åŒ–

#### 6.1 Optunaä¼˜åŒ–

```python
import optuna

def objective(trial):
    """ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""

    # æœç´¢è¶…å‚æ•°ç©ºé—´
    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])
    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-2)
    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])

    # åˆ›å»ºæ¨¡å‹
    model = SimpleCouplingGNN(
        num_features=10,
        hidden_dim=hidden_dim
    )

    # è®­ç»ƒå¹¶è¯„ä¼°
    val_score = train_and_evaluate(model, learning_rate, batch_size, dropout_rate)

    return val_score

def hyperparameter_optimization():
    """è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–"""

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=100)

    print("æœ€ä½³è¶…å‚æ•°:")
    print(study.best_params)
    print(f"æœ€ä½³éªŒè¯åˆ†æ•°: {study.best_value}")

    return study.best_params
```

#### 6.2 ç½‘æ ¼æœç´¢

```python
from sklearn.model_selection import ParameterGrid

def grid_search_optimization():
    """ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–"""

    param_grid = {
        'hidden_dim': [64, 128, 256],
        'dropout_rate': [0.1, 0.2, 0.3],
        'learning_rate': [0.001, 0.01, 0.1],
        'batch_size': [32, 64, 128]
    }

    best_score = float('inf')
    best_params = None

    for params in ParameterGrid(param_grid):
        print(f"æµ‹è¯•å‚æ•°: {params}")

        # è®­ç»ƒæ¨¡å‹
        score = train_and_evaluate(**params)

        if score < best_score:
            best_score = score
            best_params = params

    print(f"æœ€ä½³å‚æ•°: {best_params}")
    print(f"æœ€ä½³åˆ†æ•°: {best_score}")

    return best_params
```

---

## ğŸ“ æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒè´¡çŒ®

1. **ç®€åŒ–å»ºæ¨¡ç­–ç•¥**: ä»å¤æ‚GNNè½¬å‘é«˜æ•ˆMLP
2. **åŸå­å¯¹ç‰¹å¾å·¥ç¨‹**: è·ç¦»ã€ç±»å‹ã€åæ ‡çš„æœ‰æ•ˆç»„åˆ
3. **å¿«é€ŸåŸå‹éªŒè¯**: 2000æ ·æœ¬å¿«é€Ÿæµ‹è¯•å¯è¡Œæ€§
4. **ç«¯åˆ°ç«¯æµç¨‹**: æ•°æ®åŠ è½½â†’é¢„å¤„ç†â†’è®­ç»ƒâ†’è¯„ä¼°

### æŠ€æœ¯äº®ç‚¹

- ğŸš€ **é«˜æ•ˆå¤„ç†**: é¿å…å›¾æ‰¹å¤„ç†çš„å¤æ‚æ€§
- ğŸ¯ **ç‰¹å¾å·¥ç¨‹**: åŒ–å­¦çŸ¥è¯†æŒ‡å¯¼çš„ç‰¹å¾è®¾è®¡
- ğŸ”§ **å·¥ç¨‹å®ç”¨**: æ˜“äºéƒ¨ç½²å’Œç»´æŠ¤çš„æ¨¡å‹æ¶æ„
- ğŸ“Š **å…¨é¢è¯„ä¼°**: å¤šæŒ‡æ ‡è¯„ä¼°ä½“ç³»

### åº”ç”¨ä»·å€¼

- ğŸ§ª **NMRè°±è§£æ**: è¾…åŠ©å…‰è°±ç»“æ„è§£æ
- ğŸ’Š **è¯ç‰©è®¾è®¡**: é¢„æµ‹è¯ç‰©åˆ†å­çš„NMRç‰¹å¾
- ğŸ”¬ **åŒ–å­¦ç ”ç©¶**: ç†è§£åˆ†å­å†…ç›¸äº’ä½œç”¨
- ğŸ¤– **è‡ªåŠ¨åŒ–**: é«˜é€šé‡åŒ–åˆç‰©ç­›é€‰

### æœªæ¥å‘å±•æ–¹å‘

1. **æ¨¡å‹æ¶æ„**: æ¢ç´¢Transformerã€å›¾Transformer
2. **ç‰¹å¾å¢å¼º**: é‡å­åŒ–å­¦è®¡ç®—ç‰¹å¾èåˆ
3. **å¤šä»»åŠ¡å­¦ä¹ **: åŒæ—¶é¢„æµ‹å¤šç§NMRå‚æ•°
4. **è¿ç§»å­¦ä¹ **: é¢„è®­ç»ƒæ¨¡å‹+å¾®è°ƒç­–ç•¥
5. **ç‰©ç†çº¦æŸ**: èå…¥ç‰©ç†å®šå¾‹çš„ç¥ç»ç½‘ç»œ

### æœ€åçš„æ€è€ƒ

æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹å±•ç¤ºäº†æœºå™¨å­¦ä¹ åœ¨åŒ–å­¦ä¸­çš„å¼ºå¤§åº”ç”¨æ½œåŠ›ã€‚é€šè¿‡åˆç†çš„ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è®¾è®¡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¿æŒé¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œå¤§å¹…ç®€åŒ–æ¨¡å‹å¤æ‚åº¦ï¼Œè¿™å¯¹äºå®é™…åº”ç”¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

### è®ºæ–‡æ¨è

1. **Machine Learning for NMR Spectroscopy** - æœºå™¨å­¦ä¹ åœ¨NMRä¸­çš„ç»¼è¿°
2. **Predicting Chemical Shifts with Graph Neural Networks** - å›¾ç¥ç»ç½‘ç»œé¢„æµ‹åŒ–å­¦ä½ç§»
3. **Deep Learning for Molecular Property Prediction** - åˆ†å­æ€§è´¨é¢„æµ‹æ·±åº¦å­¦ä¹ æ–¹æ³•

### å·¥å…·èµ„æº

- [RDKit](https://www.rdkit.org/): åŒ–å­¦ä¿¡æ¯å­¦å·¥å…·åŒ…
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/): å›¾ç¥ç»ç½‘ç»œåº“
- [SHAP](https://shap.readthedocs.io/): æ¨¡å‹è§£é‡Šæ€§å·¥å…·
- [Optuna](https://optuna.org/): è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶

### æ•°æ®é›†èµ„æº

- [QM9](http://quantum-machine.org/datasets/): é‡å­åŒ–å­¦æ•°æ®é›†
- [ChEMBL](https://www.ebi.ac.uk/chembl/): ç”Ÿç‰©æ´»æ€§åŒ–åˆç‰©æ•°æ®åº“
- [PubChem](https://pubchem.ncbi.nlm.nih.gov/): åŒ–å­¦ä¿¡æ¯æ•°æ®åº“

---

*æœ¬æ•™ç¨‹æä¾›äº†ä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§åº”ç”¨çš„å®Œæ•´æŒ‡å¯¼ã€‚å¸Œæœ›èƒ½å¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•è§£å†³åŒ–å­¦é—®é¢˜ï¼* ğŸš€
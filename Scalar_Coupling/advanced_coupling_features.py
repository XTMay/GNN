#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹ - é«˜çº§ç‰¹å¾å·¥ç¨‹
é›†æˆRDKitåˆ†å­æè¿°ç¬¦ã€æ‹“æ‰‘ç‰¹å¾ã€å‡ ä½•ç‰¹å¾ç­‰é«˜çº§ç‰¹å¾æå–æ–¹æ³•
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from torch.utils.data import DataLoader, Dataset
from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool, BatchNorm
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
import warnings
warnings.filterwarnings('ignore')

# RDKitå¯¼å…¥
try:
    from rdkit import Chem
    from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors, Fragments
    from rdkit.Chem import rdmolops, rdmolfiles
    RDKIT_AVAILABLE = True
    print("âœ… RDKitå¯ç”¨")
except ImportError:
    RDKIT_AVAILABLE = False
    print("âš ï¸ RDKitä¸å¯ç”¨ï¼Œå°†è·³è¿‡RDKitæè¿°ç¬¦")


class AdvancedCouplingFeatureExtractor:
    """é«˜çº§ç‰¹å¾æå–å™¨"""

    def __init__(self):
        self.feature_scalers = {}
        self.feature_selectors = {}

    def extract_topological_features(self, mol_structure):
        """æå–æ‹“æ‰‘ç‰¹å¾"""
        features = []

        # åŸºç¡€ç»Ÿè®¡
        num_atoms = len(mol_structure)
        features.append(num_atoms)

        # åŸå­ç±»å‹åˆ†å¸ƒ
        atom_types = mol_structure['atom'].value_counts()
        for atom in ['H', 'C', 'N', 'O', 'F']:
            features.append(atom_types.get(atom, 0))

        # åˆ›å»ºé‚»æ¥çŸ©é˜µç”¨äºå›¾åˆ†æ
        coords = mol_structure[['x', 'y', 'z']].values
        distances = np.sqrt(np.sum((coords[:, np.newaxis] - coords[np.newaxis, :]) ** 2, axis=2))
        adjacency = (distances < 2.0) & (distances > 0)

        # åº¦æ•°åˆ†å¸ƒ
        degrees = adjacency.sum(axis=1)
        features.extend([
            degrees.mean(),
            degrees.std(),
            degrees.max(),
            degrees.min()
        ])

        # å›¾å¯†åº¦
        possible_edges = num_atoms * (num_atoms - 1) // 2
        actual_edges = adjacency.sum() // 2
        density = actual_edges / possible_edges if possible_edges > 0 else 0
        features.append(density)

        return np.array(features)

    def extract_geometric_features(self, mol_structure):
        """æå–3Då‡ ä½•ç‰¹å¾"""
        coords = mol_structure[['x', 'y', 'z']].values

        features = []

        # åˆ†å­å°ºå¯¸
        bbox = coords.max(axis=0) - coords.min(axis=0)
        features.extend(bbox)  # x, y, zæ–¹å‘çš„è·¨åº¦

        # å‡ ä½•ä¸­å¿ƒ
        center = coords.mean(axis=0)
        features.extend(center)

        # æƒ¯æ€§çŸ©ç›¸å…³ç‰¹å¾
        centered_coords = coords - center
        inertia_tensor = np.dot(centered_coords.T, centered_coords)
        eigenvals = np.linalg.eigvals(inertia_tensor)
        eigenvals = np.sort(eigenvals)[::-1]  # é™åºæ’åˆ—

        features.extend(eigenvals)  # ä¸»æƒ¯æ€§çŸ©

        # å›è½¬åŠå¾„
        gyration_radius = np.sqrt(np.sum(centered_coords ** 2) / len(coords))
        features.append(gyration_radius)

        # åŸå­é—´è·ç¦»ç»Ÿè®¡
        distances = np.sqrt(np.sum((coords[:, np.newaxis] - coords[np.newaxis, :]) ** 2, axis=2))
        # åªè€ƒè™‘éé›¶è·ç¦»
        non_zero_distances = distances[distances > 0]
        if len(non_zero_distances) > 0:
            features.extend([
                non_zero_distances.mean(),
                non_zero_distances.std(),
                non_zero_distances.max(),
                non_zero_distances.min()
            ])
        else:
            features.extend([0, 0, 0, 0])

        # è´¨å¿ƒåˆ°å„åŸå­çš„è·ç¦»ç»Ÿè®¡
        center_distances = np.sqrt(np.sum((coords - center) ** 2, axis=1))
        features.extend([
            center_distances.mean(),
            center_distances.std(),
            center_distances.max()
        ])

        return np.array(features)

    def extract_rdkit_features(self, mol_structure):
        """æå–RDKitåˆ†å­æè¿°ç¬¦"""
        if not RDKIT_AVAILABLE:
            return np.zeros(50)  # è¿”å›é›¶å‘é‡

        try:
            # ä»ç»“æ„åˆ›å»ºåˆ†å­å¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            # æ³¨æ„: å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„åˆ†å­é‡å»ºé€»è¾‘
            mol_features = []

            # åŸå­æ•°å’Œé”®æ•°çš„ä¼°ç®—
            num_atoms = len(mol_structure)
            mol_features.append(num_atoms)

            # åŸå­ç±»å‹ç»Ÿè®¡
            atom_counts = mol_structure['atom'].value_counts()
            mol_features.append(atom_counts.get('C', 0))  # ç¢³åŸå­æ•°
            mol_features.append(atom_counts.get('H', 0))  # æ°¢åŸå­æ•°
            mol_features.append(atom_counts.get('N', 0))  # æ°®åŸå­æ•°
            mol_features.append(atom_counts.get('O', 0))  # æ°§åŸå­æ•°
            mol_features.append(atom_counts.get('F', 0))  # æ°ŸåŸå­æ•°

            # åˆ†å­é‡ä¼°ç®—
            atom_masses = {'H': 1.008, 'C': 12.01, 'N': 14.01, 'O': 16.00, 'F': 19.00}
            mol_weight = sum(atom_masses.get(atom, 0) for atom in mol_structure['atom'])
            mol_features.append(mol_weight)

            # åŸºäº3Dåæ ‡çš„ç®€å•æè¿°ç¬¦
            coords = mol_structure[['x', 'y', 'z']].values

            # åˆ†å­ä½“ç§¯ä¼°ç®—ï¼ˆå‡¸åŒ…è¿‘ä¼¼ï¼‰
            if len(coords) >= 4:
                try:
                    from scipy.spatial import ConvexHull
                    hull = ConvexHull(coords)
                    volume = hull.volume
                except:
                    volume = 0
            else:
                volume = 0
            mol_features.append(volume)

            # è¡¨é¢ç§¯ä¼°ç®—
            if len(coords) >= 4:
                try:
                    surface_area = hull.area
                except:
                    surface_area = 0
            else:
                surface_area = 0
            mol_features.append(surface_area)

            # å¡«å……åˆ°50ç»´
            while len(mol_features) < 50:
                mol_features.append(0.0)

            return np.array(mol_features[:50])

        except Exception as e:
            print(f"RDKitç‰¹å¾æå–å¤±è´¥: {e}")
            return np.zeros(50)

    def extract_coupling_specific_features(self, atom_idx_0, atom_idx_1, coupling_type,
                                         mol_structure, topological_features, geometric_features):
        """æå–è€¦åˆç‰¹å¼‚æ€§ç‰¹å¾"""
        features = []

        # åŸå­å¯¹åŸºç¡€ä¿¡æ¯
        atom_0 = mol_structure.iloc[atom_idx_0]
        atom_1 = mol_structure.iloc[atom_idx_1]

        coords_0 = np.array([atom_0['x'], atom_0['y'], atom_0['z']])
        coords_1 = np.array([atom_1['x'], atom_1['y'], atom_1['z']])

        # åŸå­å¯¹è·ç¦»
        distance = np.linalg.norm(coords_1 - coords_0)
        features.append(distance)

        # ç›¸å¯¹ä½ç½®å‘é‡
        rel_pos = coords_1 - coords_0
        features.extend(rel_pos)

        # è·ç¦»çš„å¯¹æ•°å’Œå€’æ•°
        features.append(np.log(distance + 1e-6))
        features.append(1.0 / (distance + 1e-6))

        # åŸå­ç±»å‹ç¼–ç 
        atom_type_map = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}
        atom_0_type = atom_type_map.get(atom_0['atom'], 0)
        atom_1_type = atom_type_map.get(atom_1['atom'], 0)
        features.extend([atom_0_type, atom_1_type])

        # è€¦åˆç±»å‹ç¼–ç 
        coupling_type_map = {'1JHC': 0, '2JHH': 1, '3JHH': 2, '1JCC': 3,
                           '2JHC': 4, '2JCH': 5, '3JHC': 6}
        coupling_encoded = coupling_type_map.get(coupling_type, 0)
        features.append(coupling_encoded)

        # åŸå­åœ¨åˆ†å­ä¸­çš„ä½ç½®ï¼ˆç›¸å¯¹äºè´¨å¿ƒçš„è·ç¦»ï¼‰
        mol_center = mol_structure[['x', 'y', 'z']].mean().values
        dist_to_center_0 = np.linalg.norm(coords_0 - mol_center)
        dist_to_center_1 = np.linalg.norm(coords_1 - mol_center)
        features.extend([dist_to_center_0, dist_to_center_1])

        # è§’åº¦ç‰¹å¾ï¼ˆå¦‚æœåˆ†å­ä¸­æœ‰ä¸‰ä¸ªä»¥ä¸ŠåŸå­ï¼‰
        if len(mol_structure) >= 3:
            other_atoms = mol_structure[~mol_structure.index.isin([atom_idx_0, atom_idx_1])]
            if len(other_atoms) > 0:
                # æ‰¾åˆ°æœ€è¿‘çš„ç¬¬ä¸‰ä¸ªåŸå­
                other_coords = other_atoms[['x', 'y', 'z']].values
                distances_to_0 = np.linalg.norm(other_coords - coords_0[np.newaxis, :], axis=1)
                nearest_idx = distances_to_0.argmin()
                coords_2 = other_coords[nearest_idx]

                # è®¡ç®—è§’åº¦ (atom_0 - atom_1 - atom_2)
                vec1 = coords_0 - coords_1
                vec2 = coords_2 - coords_1
                cos_angle = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-6)
                angle = np.arccos(np.clip(cos_angle, -1, 1))
                features.append(angle)
            else:
                features.append(0)
        else:
            features.append(0)

        # åŠ å…¥åˆ†å­çº§åˆ«ç‰¹å¾çš„æ‘˜è¦
        features.append(topological_features.mean())
        features.append(geometric_features.mean())

        return np.array(features)


class AdvancedCouplingDataset(Dataset):
    """é«˜çº§ç‰¹å¾çš„æ ‡é‡è€¦åˆå¸¸æ•°æ•°æ®é›†"""

    def __init__(self, data_path, max_samples=5000, feature_selection=True):
        self.data_path = data_path
        self.max_samples = max_samples
        self.feature_selection = feature_selection

        self.feature_extractor = AdvancedCouplingFeatureExtractor()
        self.scaler = RobustScaler()  # ä½¿ç”¨RobustScalerå¤„ç†å¼‚å¸¸å€¼

        print(f"åŠ è½½é«˜çº§ç‰¹å¾æ ‡é‡è€¦åˆå¸¸æ•°æ•°æ®ï¼Œæ ·æœ¬æ•°: {max_samples}")
        self._load_and_preprocess_data()

    def _load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        # åŠ è½½æ•°æ®
        train_df = pd.read_csv(os.path.join(self.data_path, 'train.csv')).head(self.max_samples)
        structures_df = pd.read_csv(os.path.join(self.data_path, 'structures.csv'))

        print(f"åŠ è½½ {len(train_df)} ä¸ªè€¦åˆæ ·æœ¬")

        # æå–é«˜çº§ç‰¹å¾
        self._extract_advanced_features(train_df, structures_df)

    def _extract_advanced_features(self, train_df, structures_df):
        """æå–é«˜çº§ç‰¹å¾"""
        print("æå–é«˜çº§ç‰¹å¾...")

        all_features = []
        all_targets = []

        # æŒ‰åˆ†å­åˆ†ç»„å¤„ç†
        molecule_groups = train_df.groupby('molecule_name')
        processed_molecules = 0

        for mol_name, coupling_df in molecule_groups:
            if processed_molecules % 500 == 0:
                print(f"å¤„ç†åˆ†å­: {processed_molecules}/{len(molecule_groups)}")

            # è·å–åˆ†å­ç»“æ„
            mol_structure = structures_df[structures_df['molecule_name'] == mol_name]
            if len(mol_structure) == 0:
                continue

            mol_structure = mol_structure.sort_values('atom_index').reset_index(drop=True)

            try:
                # æå–åˆ†å­çº§åˆ«ç‰¹å¾
                topo_features = self.feature_extractor.extract_topological_features(mol_structure)
                geom_features = self.feature_extractor.extract_geometric_features(mol_structure)
                rdkit_features = self.feature_extractor.extract_rdkit_features(mol_structure)

                # å¤„ç†æ¯ä¸ªåŸå­å¯¹çš„è€¦åˆå¸¸æ•°
                for _, coupling_row in coupling_df.iterrows():
                    atom_idx_0 = coupling_row['atom_index_0']
                    atom_idx_1 = coupling_row['atom_index_1']
                    coupling_constant = coupling_row['scalar_coupling_constant']
                    coupling_type = coupling_row['type']

                    if atom_idx_0 >= len(mol_structure) or atom_idx_1 >= len(mol_structure):
                        continue

                    # æå–è€¦åˆç‰¹å¼‚æ€§ç‰¹å¾
                    coupling_features = self.feature_extractor.extract_coupling_specific_features(
                        atom_idx_0, atom_idx_1, coupling_type, mol_structure,
                        topo_features, geom_features
                    )

                    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
                    combined_features = np.concatenate([
                        coupling_features,
                        topo_features,
                        geom_features,
                        rdkit_features
                    ])

                    all_features.append(combined_features)
                    all_targets.append(coupling_constant)

                processed_molecules += 1

            except Exception as e:
                continue

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        self.features = np.array(all_features)
        self.targets = np.array(all_targets)

        print(f"æå–ç‰¹å¾å®Œæˆ: {len(self.features)} ä¸ªæ ·æœ¬, {self.features.shape[1]} ä¸ªç‰¹å¾")

        # ç‰¹å¾ç¼©æ”¾
        self.features = self.scaler.fit_transform(self.features)

        # ç‰¹å¾é€‰æ‹©
        if self.feature_selection and self.features.shape[1] > 50:
            print("è¿›è¡Œç‰¹å¾é€‰æ‹©...")
            selector = SelectKBest(f_regression, k=min(50, self.features.shape[1]))
            self.features = selector.fit_transform(self.features, self.targets)
            self.feature_selector = selector
            print(f"ç‰¹å¾é€‰æ‹©å: {self.features.shape[1]} ä¸ªç‰¹å¾")

        # è½¬æ¢ä¸ºtensor
        self.features = torch.tensor(self.features, dtype=torch.float32)
        self.targets = torch.tensor(self.targets, dtype=torch.float32)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.targets[idx]


class AdvancedCouplingMLP(nn.Module):
    """é«˜çº§ç‰¹å¾çš„è€¦åˆå¸¸æ•°é¢„æµ‹æ¨¡å‹"""

    def __init__(self, num_features, hidden_dims=[256, 128, 64], dropout=0.3):
        super(AdvancedCouplingMLP, self).__init__()

        layers = []
        input_dim = num_features

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            input_dim = hidden_dim

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(input_dim, 1))

        self.mlp = nn.Sequential(*layers)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        return self.mlp(x)


def train_advanced_model(model, train_loader, val_loader, device, num_epochs=50):
    """è®­ç»ƒé«˜çº§ç‰¹å¾æ¨¡å‹"""
    print("è®­ç»ƒé«˜çº§ç‰¹å¾æ¨¡å‹...")

    model.to(device)
    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    criterion = nn.MSELoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.8, patience=7, min_lr=1e-6
    )

    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    best_model_state = None

    start_time = time.time()

    for epoch in range(num_epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        train_samples = 0

        for features, targets in train_loader:
            features, targets = features.to(device), targets.to(device)

            optimizer.zero_grad()
            predictions = model(features).squeeze()
            loss = criterion(predictions, targets)
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()

            train_loss += loss.item() * len(targets)
            train_samples += len(targets)

        # éªŒè¯
        model.eval()
        val_loss = 0
        val_samples = 0

        with torch.no_grad():
            for features, targets in val_loader:
                features, targets = features.to(device), targets.to(device)
                predictions = model(features).squeeze()
                loss = criterion(predictions, targets)

                val_loss += loss.item() * len(targets)
                val_samples += len(targets)

        train_loss /= train_samples
        val_loss /= val_samples

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_state = model.state_dict().copy()

        if (epoch + 1) % 10 == 0:
            print(f'  Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}')

    # åŠ è½½æœ€ä½³æ¨¡å‹
    if best_model_state:
        model.load_state_dict(best_model_state)

    training_time = time.time() - start_time

    return {
        'model': model,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val_loss,
        'training_time': training_time,
        'num_parameters': sum(p.numel() for p in model.parameters())
    }


def evaluate_advanced_model(model, test_loader, device):
    """è¯„ä¼°é«˜çº§ç‰¹å¾æ¨¡å‹"""
    model.eval()
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for features, targets in test_loader:
            features, targets = features.to(device), targets.to(device)
            predictions = model(features).squeeze()

            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    predictions = np.array(all_predictions)
    targets = np.array(all_targets)

    mae = mean_absolute_error(targets, predictions)
    mse = mean_squared_error(targets, predictions)
    rmse = np.sqrt(mse)
    r2 = r2_score(targets, predictions)

    return {
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'R2': r2,
        'predictions': predictions,
        'targets': targets
    }


def create_advanced_comparison_plots(train_result, eval_result):
    """åˆ›å»ºé«˜çº§ç‰¹å¾æ¯”è¾ƒå¯è§†åŒ–"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('é«˜çº§ç‰¹å¾æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹ç»“æœ', fontsize=16)

    # 1. è®­ç»ƒæ›²çº¿
    epochs = range(1, len(train_result['train_losses']) + 1)
    axes[0, 0].plot(epochs, train_result['train_losses'], label='è®­ç»ƒæŸå¤±')
    axes[0, 0].plot(epochs, train_result['val_losses'], label='éªŒè¯æŸå¤±')
    axes[0, 0].set_title('è®­ç»ƒæ›²çº¿')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # 2. é¢„æµ‹ vs çœŸå®å€¼
    predictions = eval_result['predictions']
    targets = eval_result['targets']

    axes[0, 1].scatter(targets, predictions, alpha=0.6, s=20)
    min_val = min(targets.min(), predictions.min())
    max_val = max(targets.max(), predictions.max())
    axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
    axes[0, 1].set_title(f'é¢„æµ‹æ•ˆæœ (RÂ² = {eval_result["R2"]:.4f})')
    axes[0, 1].set_xlabel('çœŸå®å€¼')
    axes[0, 1].set_ylabel('é¢„æµ‹å€¼')

    # 3. æ®‹å·®åˆ†æ
    residuals = predictions - targets
    axes[1, 0].scatter(predictions, residuals, alpha=0.6, s=20)
    axes[1, 0].axhline(y=0, color='r', linestyle='--')
    axes[1, 0].set_title('æ®‹å·®åˆ†æ')
    axes[1, 0].set_xlabel('é¢„æµ‹å€¼')
    axes[1, 0].set_ylabel('æ®‹å·®')

    # 4. è¯¯å·®åˆ†å¸ƒ
    axes[1, 1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')
    axes[1, 1].set_title('è¯¯å·®åˆ†å¸ƒ')
    axes[1, 1].set_xlabel('æ®‹å·®')
    axes[1, 1].set_ylabel('é¢‘æ¬¡')

    plt.tight_layout()
    plt.savefig('advanced_coupling_results.png', dpi=300, bbox_inches='tight')
    plt.show()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é«˜çº§ç‰¹å¾æ ‡é‡è€¦åˆå¸¸æ•°é¢„æµ‹")

    # è®¾å¤‡é€‰æ‹©
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ•°æ®è·¯å¾„
    data_path = '/Users/xiaotingzhou/Downloads/GNN/Dataset/scalar_coupling_constant'

    # åˆ›å»ºæ•°æ®é›†
    dataset = AdvancedCouplingDataset(data_path, max_samples=4000, feature_selection=True)

    # æ•°æ®åˆ’åˆ†
    total_size = len(dataset)
    test_size = int(total_size * 0.2)
    val_size = int(total_size * 0.1)
    train_size = total_size - test_size - val_size

    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size, test_size]
    )

    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

    # åˆ›å»ºæ¨¡å‹
    num_features = dataset.features.shape[1]
    model = AdvancedCouplingMLP(num_features, hidden_dims=[512, 256, 128, 64], dropout=0.3)

    print(f"æ¨¡å‹ç‰¹å¾ç»´åº¦: {num_features}")
    print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # è®­ç»ƒæ¨¡å‹
    train_result = train_advanced_model(model, train_loader, val_loader, device, num_epochs=60)

    # è¯„ä¼°æ¨¡å‹
    eval_result = evaluate_advanced_model(train_result['model'], test_loader, device)

    # æ‰“å°ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ¯ é«˜çº§ç‰¹å¾æ¨¡å‹ç»“æœ:")
    print(f"{'='*60}")
    print(f"MAE: {eval_result['MAE']:.6f}")
    print(f"RMSE: {eval_result['RMSE']:.6f}")
    print(f"RÂ²: {eval_result['R2']:.6f}")
    print(f"å‚æ•°é‡: {train_result['num_parameters']:,}")
    print(f"è®­ç»ƒæ—¶é—´: {train_result['training_time']:.1f}s")

    # åˆ›å»ºå¯è§†åŒ–
    create_advanced_comparison_plots(train_result, eval_result)

    # ä¿å­˜ç»“æœ
    results = {
        **train_result,
        **eval_result
    }

    return results


if __name__ == "__main__":
    results = main()